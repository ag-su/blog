[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "blog",
    "section": "",
    "text": "[cropdoctor] 4. 작물 질병 데이터셋으로 커스텀 YOLO 모델 학습\n\n\n\n\n\n\n\nproject\n\n\nelice\n\n\n\n\n\n\n\n\n\n\n\nMay 7, 2023\n\n\nagsu\n\n\n\n\n\n\n  \n\n\n\n\n[cropdoctor] 3. 작물 질병 진단 모델 학습 및 평가 (mobilenetV2)\n\n\n\n\n\n\n\nproject\n\n\nelice\n\n\n\n\n\n\n\n\n\n\n\nMay 3, 2023\n\n\nagsu\n\n\n\n\n\n\n  \n\n\n\n\n[cropdoctor] 2. tar 확장자 모델 docker image 불러오기\n\n\n\n\n\n\n\nproject\n\n\nelice\n\n\n\n\n\n\n\n\n\n\n\nApr 30, 2023\n\n\nagsu\n\n\n\n\n\n\n  \n\n\n\n\n[cropdoctor] 1. 노지작물 데이터 EDA\n\n\n\n\n\n\n\nproject\n\n\nelice\n\n\n\n\n\n\n\n\n\n\n\nApr 28, 2023\n\n\nagsu\n\n\n\n\n\n\n  \n\n\n\n\nPIL을 사용한 전통적인 이미지 처리 방법\n\n\n\n\n\n\n\nImage\n\n\n\n\n\n\n\n\n\n\n\nApr 4, 2023\n\n\nagsu\n\n\n\n\n\n\n  \n\n\n\n\n서울시 1인가구 시각화 분석\n\n\n\n\n\n\n\nvisualization\n\n\n\n\n\n\n\n\n\n\n\nMar 24, 2023\n\n\nagsu\n\n\n\n\n\n\n  \n\n\n\n\n[stockait] stockait 사용하여 주가 데이터 표준화 실험하기\n\n\n\n\n\n\n\nproject\n\n\nstockait\n\n\n\n\n\n\n\n\n\n\n\nFeb 15, 2023\n\n\nagsu\n\n\n\n\n\n\n  \n\n\n\n\n[stockait] stockait 사용하여 주가 예측 머신러닝 모델 실험하기\n\n\n\n\n\n\n\nproject\n\n\nstockait\n\n\n\n\n\n\n\n\n\n\n\nFeb 11, 2023\n\n\nagsu\n\n\n\n\n\n\n  \n\n\n\n\n[stockait] stockait에서 trader의 개념과 사용법\n\n\n\n\n\n\n\nproject\n\n\nstockait\n\n\n\n\n\n\n\n\n\n\n\nFeb 5, 2023\n\n\nagsu\n\n\n\n\n\n\n  \n\n\n\n\n[stockait] stockait 사용하여 주가 예측 모델 학습하기\n\n\n\n\n\n\n\nproject\n\n\nstockait\n\n\n\n\n\n\n\n\n\n\n\nFeb 2, 2023\n\n\nagsu\n\n\n\n\n\n\n  \n\n\n\n\n[stock prediction] 3.3 클러스터 탐색을 통한 주가상승 패턴 검출\n\n\n\n\n\n\n\nproject\n\n\nstock prediction\n\n\n\n\n\n\n\n\n\n\n\nJul 25, 2022\n\n\nagsu\n\n\n\n\n\n\n  \n\n\n\n\n[stock prediction] 3.2 t-SNE를 사용한 주가데이터 2차원 시각화\n\n\n\n\n\n\n\nproject\n\n\nstock prediction\n\n\n\n\n\n\n\n\n\n\n\nJul 22, 2022\n\n\nagsu\n\n\n\n\n\n\n  \n\n\n\n\n[stock prediction] 3.1. 설명가능 AI (XAI), SHAP value\n\n\n\n\n\n\n\nproject\n\n\nstock prediction\n\n\n\n\n\n\n\n\n\n\n\nJul 18, 2022\n\n\nagsu\n\n\n\n\n\n\n  \n\n\n\n\n[stock prediction] 2.3 CCI를 이용한 주가 데이터 필터링\n\n\n\n\n\n\n\nproject\n\n\nstock prediction\n\n\n\n\n\n\n\n\n\n\n\nJul 15, 2022\n\n\nagsu\n\n\n\n\n\n\n  \n\n\n\n\n[stock prediction] 2.2 주가 데이터 스케일링\n\n\n\n\n\n\n\nproject\n\n\nstock prediction\n\n\n\n\n\n\n\n\n\n\n\nJul 11, 2022\n\n\nagsu\n\n\n\n\n\n\n  \n\n\n\n\n[stock prediction] 2.1 주가 데이터셋 보조지표 추가\n\n\n\n\n\n\n\nproject\n\n\nstock prediction\n\n\n\n\n\n\n\n\n\n\n\nJul 9, 2022\n\n\nagsu\n\n\n\n\n\n\n  \n\n\n\n\n[stock prediction] 1.2 머신러닝 모델 비교\n\n\n\n\n\n\n\nproject\n\n\nstock prediction\n\n\n\n\n\n\n\n\n\n\n\nJul 8, 2022\n\n\nagsu\n\n\n\n\n\n\n  \n\n\n\n\n[stock prediction] 1.1 머신러닝을 위한 주가 데이터셋 생성\n\n\n\n\n\n\n\nproject\n\n\nstock prediction\n\n\n\n\n\n\n\n\n\n\n\nJul 6, 2022\n\n\nagsu\n\n\n\n\n\n\n  \n\n\n\n\n상권정보 시각화 분석\n\n\n\n\n\n\n\nlecture/practice\n\n\nvisualization\n\n\n\n\n\n\n\n\n\n\n\nMay 7, 2022\n\n\nagsu\n\n\n\n\n\n\n  \n\n\n\n\n전국 아파트 분양가격 시각화 분석\n\n\n\n\n\n\n\nlecture/practice\n\n\nvisualization\n\n\n\n\n\n\n\n\n\n\n\nApr 19, 2022\n\n\nagsu\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/01.stock_dataset.html",
    "href": "posts/01.stock_dataset.html",
    "title": "[stock prediction] 1.1 머신러닝을 위한 주가 데이터셋 생성",
    "section": "",
    "text": "본 글에서는 시가, 저가, 고가, 종가, 거래량, 변화율로 구성되어있는 주가 데이터셋을 머신러닝 학습을 위한 형태로 전처리한다. 또한, 이 전처리 과정에서 처음에 시도하였던 방법으로부터 점차 효율적으로 개선하여 처리 시간을 줄이는 방법의 과정을 담았다.\n\n\n\n본 연구에서 머신러닝 학습을 위한 데이터셋은 다음과 같이 정의하였다.\n\n조건: 전체 약 2000개 종목 중 2018년 부터 존속하였던 기업 중 거래대금이 1000억 이상 발생한 날짜 (거래대금은 추후에 수정 예정)\n독립변수 정의: 조건 에서 선별한 특정 날짜를 D0라고 했을 때, D-9, D-8, …, D0, 총 10일 치의 Open, High, Low, Close, 거래대금(trading_value)\n종속변수 정의: 조건에 부합하는 특정 날짜(D0) 대비 다음날(D+1) 날짜의 종가(close)가 2%이상 상승하면 1, 상승하지 않으면 0\n학습&시험 데이터셋 정의\n\ntrain dataset: 2018년 1월 2일 - 2020년 12월 31일 (2년)\ntest dataset: 2021년 1월 2일 ~ 2021년 6월 31일 (6개월)\n\n\n\n\n\n\n데이터셋 생성을 3단계의 과정으로 나누어 진행했다.\n\n과제 I: 종목 리스트 생성-2018년 부터 존속하였던 기업(상장일 2018년 1월 1일 이전 기업) 선별\n과제 II: 과제I 의 기업 중 거래대금이 1000억 이상 발생했던 특정 날짜 선별 (거래대금=거래량X종가 로 계산)\n과제 III: 과제II 의 날짜에 대해 최종 머신러닝 데이터 생성하여 csv 파일로 저장\n\n\n주가 데이터셋 생성 절차는 df2list - dictionary - MultiProcessing - MySQL 순서로 진행한다. 4가지 절차를 통해 데이터셋이 달라지는 것은 아니고, 속도를 개선시키는 방향으로의 효율적인 코드를 짜기 위한 훈련 과정이다. 대규모 데이터셋을 다루기 위해서는 효율적인 코딩을 통해 속도를 개선하는 것도 중요한 일이다. 특히 이번에 사용하는 주가 데이터셋은 양이 많을 뿐만 아니라, fdr 라이브러리를 사용하여 외부에서 불러와야하기 때문에 데이터셋을 구성하는 것만 해도 속도가 상당히 느리다. 이러한 문제점을 해결하고자 본 포스팅에서는 4단계에 걸친 데이터셋 생성 과정을 보여준다.\n\n\n\n\n\n(0) Finance Data Reader를 이용한 주가 데이터셋 - DataFrame\n(1) Finance Data Reader를 이용한 주가 데이터셋 - df2list\n(2) Finance Data Reader를 이용한 주가 데이터셋 - dictionary\n(3) Finance Data Reader를 이용한 주가 데이터셋 - MultiProcessing\n(4) Finance Data Reader를 이용한 주가 데이터셋 - MySQL\n(5) 최종 머신러닝 데이터셋\n\n\n\n\n\n# finance datareader 설치 \n# ! pip install -U finance-datareader\n\n\nimport pandas as pd\nimport os\nfrom tqdm import tqdm\nimport FinanceDataReader as fdr\nimport time\n\n# 경고 메시지 무시 \nimport warnings\nwarnings.filterwarnings(action='ignore')\n\n \n\n\n\n\n속도 비교를 위해 기존 데이터의 타입인 DataFrame을 사용하여 데이터셋을 생성한다.\n과제 I\n\ndf = pd.read_html('http://kind.krx.co.kr/corpgeneral/corpList.do?method=download', header=0)[0]\n\n# 회사명, 종목코드, 상장일 컬럼만 사용 \ndf_code = df[['회사명', '종목코드', '상장일']]\n\n# 종목코드를 6자리로 맞춰 준다. \ndf_code['종목코드'] = df_code['종목코드'].apply(lambda x : str(x).zfill(6))\ndisplay(df_code.head(3))\nprint()\n\n# 상장일이 2018년 1월 1일 이전인 종목코드 선별 \nstart_time = time.time()\nlst_code = df_code.loc[df_code['상장일'] < '2018-01-01', '종목코드'].to_list()\nprint(\"걸린 시간: \", time.time() - start_time)\nprint()\nprint('상장일 2018-01-01 이전 종목코드: ', lst_code[:5])\nprint()\nprint(f'총 {len(df_code)} 개의 종목 중 {len(lst_code)} 개의 종목 선별')\n\n\n\n\n\n  \n    \n      \n      회사명\n      종목코드\n      상장일\n    \n  \n  \n    \n      0\n      DL\n      000210\n      1976-02-02\n    \n    \n      1\n      DRB동일\n      004840\n      1976-05-21\n    \n    \n      2\n      DSR\n      155660\n      2013-05-15\n    \n  \n\n\n\n\n\n걸린 시간:  0.0006058216094970703\n\n상장일 2018-01-01 이전 종목코드:  ['000210', '004840', '155660', '078930', '001390']\n\n총 2536 개의 종목 중 1972 개의 종목 선별\n\n\n과제 II\n\nstock_dict = {}\nfor code in tqdm(lst_code): \n    stock = fdr.DataReader(code, start='20180101', end='20201231')\n    stock['trading'] = stock['Volume'] * stock['Close'] # 거래대금 컬럼 추가\n    \n    if sum(stock['trading'] >= 100000000000) >= 1: # 거래대금이 1000억 이상인 데이터가 하나 이상 존재하면\n        stock_dict[code] = stock[stock['trading'] >= 100000000000].index # index == Date \n\nprint(f'총 {len(lst_code)} 개의 종목 중 {len(stock_dict)} 개의 종목 사용')\n\n100%|██████████████████████████████████████████████| 1977/1977 [02:58<00:00, 11.08it/s]\n\n\n총 1977 개의 종목 중 799 개의 종목 사용\n\n\n\n\n\n먼저, fdrDataReader 패키지를 사용하여 거래대금이 1000억 이상인 데이터의 날짜를 stock_dict에 stock_dict[code] = [날짜,…] 의 형식으로 넣어주었다.\n\n# 선별된 종목과 날짜를 lst_code_date에 넣어준다. \nlst_code_date = []\nfor code in tqdm(stock_dict): \n    for date in (stock_dict[code]):\n        lst_code_date.append([code, date])\n        \nprint(f'선별된 날짜는 총 {len(lst_code_date)}개')\n\n100%|█████████████████████████████████████████████| 799/799 [00:00<00:00, 13125.89it/s]\n\n\n선별된 날짜는 총 14182개\n\n\n\n\n\nlst_code_date에 [코드, 날짜] 형식으로 추가한다.\n과제 III\n\ndata_dict = {'code': [], 'd0': [], 'info': [], 'up': []}\nfor code, date in tqdm(lst_code_date):\n    start_date = '20171201' # 2018년 초반 날짜가 D0라면 2017년 데이터 필요 (D-9~D-1)\n    end_date = '20210130' # 2020년 후반 날짜가 D0라면 2021년 데이터 필요 (D+1) \n    stock = fdr.DataReader(code, start = start_date, end = end_date)\n    stock.reset_index(inplace=True) # 'Date' index -> column \n    \n    D9_index = stock[stock['Date'] == str(date)].index[0] - 9 # D-9 날짜의 인덱스  \n    next_index = stock[stock['Date'] == str(date)].index[0] + 1 # D+1 날짜의 인덱스  \n        \n    # 종목코드 (code)\n    data_dict['code'].append(code) \n    \n    # 기준일 (d0)\n    data_dict['d0'].append(date)\n    \n    # D-9 ~ D+1, 총 11일치의 sub stock DataFrame 생성 \n    sub_stock = stock.iloc[D9_index:next_index+1]\n    sub_stock['trading'] = sub_stock['Close'] * sub_stock['Volume'] # 거래대금 컬럼 추가 \n   \n    \n    # 10일 간의 데이터 (info)\n    info_list = []\n    for i in range(10):        \n        info_list.append(sub_stock.iloc[i, [1, 2, 3, 4, -1]].to_list())\n    remove_list=['[', ']']\n    for i in range(2): \n        info_list = f'{info_list}'.replace(remove_list[i], '')\n    data_dict['info'].append(info_list)    \n        \n        \n    # D+1 종가 2% 상승 여부 (up)\n    up = sub_stock.iloc[-2]['Close'] + 0.02 * sub_stock.iloc[-2]['Close']\n    \n    if sub_stock.iloc[-1]['Close'] >= up: \n        data_dict['up'].append(1)\n    else: \n        data_dict['up'].append(0)\n\n100%|████████████████████████████████████████████| 14182/14182 [17:08<00:00, 13.78it/s]\n\n\n\ndf_result = pd.DataFrame(data_dict)\ndisplay(df_result.head()) \n\n# 최종 결과 데이터셋 txt 파일 저장 \ndf_result.to_csv(\"assignment3.txt\")\nprint(f'생성된 데이터의 개수는 {len(pd.read_csv(\"assignment3.txt\"))} 개')\n\n\n\n\n\n  \n    \n      \n      code\n      d0\n      info\n      up\n    \n  \n  \n    \n      0\n      000210\n      2018-01-26\n      78343, 78614, 76987, 77892, 9590608284, 77801,...\n      0\n    \n    \n      1\n      000210\n      2018-08-08\n      68855, 69397, 67590, 69036, 6067435968, 69487,...\n      0\n    \n    \n      2\n      000210\n      2020-04-02\n      44819, 52951, 44322, 51145, 15615437965, 47439...\n      0\n    \n    \n      3\n      000210\n      2020-09-11\n      80783, 84487, 78524, 78524, 61554885076, 79608...\n      0\n    \n    \n      4\n      000210\n      2020-12-11\n      76174, 76174, 72289, 72289, 76043112348, 72831...\n      0\n    \n  \n\n\n\n\n생성된 데이터의 개수는 14182 개\n\n\n\n최종적으로 머신러닝 학습을 위한 데이터셋을 처리하는 코드는 총 17분이 걸렸다. 연구 초반에 가장 첫 번째로 작성한 코드인데, 지금 보니 확실히 비효율적으로 작성했던 부분이 많은 것 같다.\n\n개선해야할 사항\n\n속도 개선: 과제III 은 약 17분이 걸린 것을 보아 속도 면에서 상당히 비효율적이었다. (거래대금을 10억으로 설정했을 때 1% 진행에 5분이 걸렸습니다. 그럼 총 500분이 걸릴 것으로 예상할 수 있다.)\n# 10일 간의 데이터 부분: info_list에 데이터를 추가하는 과정에서 억지로 포맷을 맞추기 위해 불필요한 for문이 들어갔다.\n# D+1 종가 2% 상승 여부 부분: 전 날 대비 2% 상승율을 직접 계산해 주었는데, 이미 변화율이 계산 되어 있는 change라는 컬럼을 사용하는 것으로 대체한다.\ntxt 파일 저장: DataFrame으로 생성을 한 후에 txt 파일로 저장했는데, 데이터 생성 시 파일 입출력 write()를 사용해서 바로 txt 파일로 저장하는 방식으로 변경한다.\n\n\n \n\n\n\n첫번째 방법은 DataFrame 사용을 지양하고, python의 기본 데이터 타입인 list로 바꾸어 사용하는 방법입니다. 이로써 column 중심의 연산을 row 중심의 연산으로 바꾼다. 이 방법에서는 속도 개선 보다는 (0)번 방법의 코드에서 효율적이지 못했던 부분을 고치고 깔끔한 코드로 보완한다.\n과제 I\n\n# (0) 방법에서 사용했던 df_code 데이터 프레임 사용 \ndisplay(df_code.head(2))\n\n# 🌟 dataframe -> list \nlst_stock = df_code.values.tolist()\nprint(lst_stock[:2])\nprint()\n\n\nlst_code = [] # 선별 된 코드를 담을 리스트 \nstart_time = time.time()\nfor row in lst_stock:\n    code, date = row[1], row[2]\n    if date <= '2018-01-01':\n        lst_code.append(code)\nprint(\"걸린 시간: \", time.time() - start_time)\nprint()\n\n        \nprint('상장일 2018-01-01 이전 종목코드: ', lst_code[:4])\nprint()\nprint(f'총 {len(df_code)} 개의 종목 중 {len(lst_code)} 개의 종목 선별')\n\n\n\n\n\n  \n    \n      \n      회사명\n      종목코드\n      상장일\n    \n  \n  \n    \n      0\n      DL\n      000210\n      1976-02-02\n    \n    \n      1\n      DRB동일\n      004840\n      1976-05-21\n    \n  \n\n\n\n\n[['DL', '000210', '1976-02-02'], ['DRB동일', '004840', '1976-05-21']]\n\n걸린 시간:  0.0004563331604003906\n\n상장일 2018-01-01 이전 종목코드:  ['000210', '004840', '155660', '078930']\n\n총 2536 개의 종목 중 1972 개의 종목 선별\n\n\n과제 II\n\nlst_code_date = []\nfor code in tqdm(lst_code):\n    stock = fdr.DataReader(code, start='20180102', end='20201231')\n    stock.reset_index(inplace=True)\n    \n    # 🌟 dataframe -> list \n    lst_stock = stock.values.tolist()\n    \n    for row in lst_stock: \n        date, trading_value = row[0], row[4]*row[5]\n        if trading_value >= 100000000000:  # 거래대금 1000억 이상\n            lst_code_date.append([code, date.date().strftime(\"%Y%m%d\")])\n            \nprint(f'선별된 날짜는 총 {len(lst_code_date)}개')\n\n100%|██████████████████████████████████████████████| 1977/1977 [02:55<00:00, 11.26it/s]\n\n\n선별된 날짜는 총 14182개\n\n\n\n\n\n과제 III\n\nOF = open('assignment3.txt','w')\n\nfor code, date in tqdm(lst_code_date):\n    start_date = '20180101' \n    end_date = '20201231' \n    stock = fdr.DataReader(code, start = start_date, end = end_date)\n    stock.reset_index(inplace=True) # 'Date' index -> column \n    \n    \n    # 🌟 dataframe -> list \n    lst_stock = stock.values.tolist()\n    \n    \n    for idx, row in enumerate(lst_stock): \n        if (idx < 9) or (idx >= len(lst_stock)-1): # 예외 처리 \n            continue \n        \n        if row[0].date().strftime(\"%Y%m%d\") == date: \n            \n            # D-9 ~ D0 데이터만 담기\n            sub_stock = lst_stock[idx-9:idx+1]\n            \n            # 10일 간의 데이터 \n            lst_info = []\n            for row2 in sub_stock:\n                lst_prices, trading_value = row2[1:5], row[4]*row[5]\n                lst_info += lst_prices + [trading_value]\n                \n            info = ','.join(map(str, lst_info))\n            \n            # D+1 종가 2% 상승 여부 (up)\n            change = lst_stock[idx+1][6]\n            label = int(change >= 0.02)\n            \n            # 저장 \n            OF.write(f'{code}\\t{date}\\t{lst_info}\\t{label}\\n')\n            \nOF.close()\n\nprint(f'생성된 데이터의 개수는 {len(pd.read_csv(\"assignment3.txt\"))} 개')\n\n100%|████████████████████████████████████████████| 14182/14182 [17:43<00:00, 13.33it/s]\n\n\n생성된 데이터의 개수는 13939 개\n\n\n\n\n\n\nOF.write()를 사용함으로써 블필요한 DataFrame 을 생성하지 않는다.\n# 10일간의 데이터 부분은 join() 함수를 사용하여 불필요한 for문 사용을 줄였고, # D+1 종가 2% 상승 여부 (up) 부분은 전날 대비 종가 변화율을 계산하지 않고 change 컬럼을 활용하는 것으로 바꾸었다.\n속도에 대한 문제점 : 전 보다 깔끔한 코드로 보완이 되었지만, 아직 속도에 대한 문제점이 남아있다. 속도를 개선하기 위해서는 fdr 라이브러리를 최소한으로 사용해야한다. 하지만 지금 [코드, 날짜]의 형태로 for문이 돌고있기 때문에, 중복되는 같은 Code를 여러 번 불러오고 있다. 이를 해결하기 위해 lst_code_date 리스트 대신, dictionary를 사용하는 방법으로 넘어간다.\n\n\n \n\n\n\ndictionary를 사용하는 방법은 과제III 의 속도를 개선할 수 있다. fdr 라이브러리를 최소한으로 사용하고, for문을 최대한 줄인다. 현재 문제점은 날짜를 기준으로 for문이 돌아가기 때문에 같은 데이터(같은 종목)가 여러번 불러와지는 경우가 다수 존재한다는 것이다. 따라서 과제II 에서 code별 D0 날짜 리스트를 dictionary 타입으로 생성하고, 과제III 에서 code 당 한번만 fdr 라이브러리를 사용하도록 바꾸어준다.\n과제 I\n과제I 은 앞의 방법과 같으므로 생략한다.\n\nprint('상장일 2018-01-01 이전 종목코드: ', lst_code[:4])\nprint()\nprint(f'총 {len(df_code)} 개의 종목 중 {len(lst_code)} 개의 종목 선별')\n\n상장일 2018-01-01 이전 종목코드:  ['000210', '004840', '155660', '078930']\n\n총 2507 개의 종목 중 1977 개의 종목 선별\n\n\n과제 II\n\ndict_code2date = {}\nfor code in tqdm(lst_code): \n    start_date = '20180102'\n    end_date = '20201231'\n    stock = fdr.DataReader(code, start = start_date, end = end_date)\n    stock.reset_index(inplace=True)    \n    \n    # 🌟 dataframe -> list     \n    lst_stock = stock.values.tolist()\n    \n    for row in lst_stock: \n        date, trading_value = row[0], row[4]*row[5]\n        if trading_value >= 100000000000:\n            if code not in dict_code2date.keys():\n                dict_code2date[code] = [date.date().strftime(\"%Y%m%d\")]\n            else:\n                dict_code2date[code].append(date.date().strftime(\"%Y%m%d\"))\n\nprint(f'총 {len(lst_code)} 개의 종목 중 {len(dict_code2date)} 개의 종목 사용')\n\n100%|██████████████████████████████████████████████| 1977/1977 [02:42<00:00, 12.14it/s]\n\n\n총 1977 개의 종목 중 799 개의 종목 사용\n\n\n\n\n\n과제 III\n\nOF = open('assignment3.txt', 'w')\nfor code in tqdm(dict_code2date): \n    # code의 stock \n    start_date = '20180101' \n    end_date = '20201231' \n    stock = fdr.DataReader(code, start = start_date, end = end_date)\n    stock.reset_index(inplace=True)\n    \n    # 🌟 dataframe -> list     \n    lst_stock = stock.values.tolist()  \n       \n    for idx, row in enumerate(lst_stock):   \n        if (idx < 9) or (idx >= len(lst_stock)-1): # 예외 처리 \n            continue \n        \n        date = row[0].date().strftime(\"%Y%m%d\") \n        if date not in dict_code2date[code]: # 조건에 부합하는 날짜 (D0 날짜)를 발견할 때까지 continue\n            continue \n\n        # D-9 ~ D0 데이터만 담기\n        sub_stock = lst_stock[idx-9:idx+1] \n        \n        # 10일간의 데이터 \n        lst_info = []\n        for row2 in sub_stock:\n            lst_prices, trading_value = row2[1:5], row2[4]*row2[5]\n            lst_info += lst_prices + [trading_value]\n        info = ','.join(map(str, lst_info))\n\n        # D+1 종가 2% 상승 여부 \n        label = int(lst_stock[idx+1][6] >= 0.02)\n\n        # 저장 \n        OF.write(f'{code}\\t{date}\\t{info}\\t{label}\\n')\n                         \nOF.close()   \n\nprint(f'생성된 데이터의 개수는 {len(pd.read_csv(\"assignment3.txt\"))} 개')\n\n100%|████████████████████████████████████████████████| 799/799 [01:07<00:00, 11.84it/s]\n\n\n생성된 데이터의 개수는 13939 개\n\n\n\n\n\n날짜 중심의 for문을 코드 중심으로 변경함으로써 fdr 라이브러리 불러오는 것을 최소화하여, 약 17분이 걸리던 시간이 1분으로 대폭 줄어든 것을 확인하였다.\n\n \n\n\n\n\nmulti processing(다중 처리): 컴퓨터 시스템 한 대에 두개 이상의 중앙 처리 장치 (CPU) 를 이용하여 병렬처리하는 것\npython multi processing 문서: https://docs.python.org/ko/3/library/multiprocessing.html.\n\npython은 multiprocessing 라이브러리를 통해 다중 처리를 지원한다. 여러 개의 코어를 연산에 사용함으로써 많은 작업을 빠른 시간에 처리해줄 수 있다는 장점이 있다.\n [ core=10으로 설정하여 multi processing을 수행할 때 코어 사용 ]\n\n# MultiProcessing을 위한 library import \nimport time, os\nfrom multiprocessing import Pool\n\n과제 I\n과제I 은 앞의 방법과 같으므로 생략한다.\n\nprint('상장일 2018-01-01 이전 종목코드: ', lst_code[:4])\nprint()\nprint(f'총 {len(df_code)} 개의 종목 중 {len(lst_code)} 개의 종목 선별')\n\n상장일 2018-01-01 이전 종목코드:  ['000210', '004840', '155660', '078930']\n\n총 2507 개의 종목 중 1977 개의 종목 선별\n\n\n과제 II\n\nmulti processing을 위한 함수 정의\n\n\ndef make_lst_result(code): \n    start_date = '20180101'\n    end_date = '20201231'\n    \n    lst_date = []\n    \n    stock = fdr.DataReader(code, start = start_date, end = end_date)\n    stock.reset_index(inplace=True)\n  \n    # 🌟 dataframe -> list\n    lst_stock = stock.values.tolist()\n    \n    for row in lst_stock: \n        if row[4] * row[5] >= 100000000000: \n            lst_date.append(row[0].date().strftime(\"%Y%m%d\"))\n        \n    return [code, lst_date]\n\n\nmulti processing 수행\n\n\nstart_time = time.time()\nnum_cores = 10\npool = Pool(num_cores)\nlst_code_date = pool.map(make_lst_result, lst_code)\npool.close()\npool.join()\nprint(time.time() - start_time) \n\n17.97966194152832\n\n\n앞서 약 2분 40초 가 걸렸던 과제II 를 multi processing을 사용하여 17초대로 단축시켰다.\n\ndictionary 생성\n\n\ndict_code2date = {}\n\nfor code, lst_date  in tqdm(lst_code_date):\n    if lst_date == []:\n        continue\n    dict_code2date[code] = lst_date\n            \nprint(f'총 {len(lst_code)} 개의 종목 중 {len(dict_code2date)} 개의 종목 사용')\n\n100%|█████████████████████████████████████████| 1977/1977 [00:00<00:00, 1610124.08it/s]\n\n\n총 1977 개의 종목 중 799 개의 종목 사용\n\n\n\n\n\n과제 III\n\nmulti processing을 위한 함수 정의\n\n\ndef make_lst_result2(code): \n    # code의 stock \n    start_date = '20180101' \n    end_date = '20201231' \n    stock = fdr.DataReader(code, start = start_date, end = end_date)\n    stock.reset_index(inplace=True)\n    \n    # 🌟 dataframe -> list     \n    lst_stock = stock.values.tolist()  \n       \n    lst_result = []\n        \n    for idx, row in enumerate(lst_stock): \n        \n        if (idx < 9) or (idx >= len(lst_stock)-1): # 예외 처리 \n            continue \n            \n        date = row[0].date().strftime(\"%Y%m%d\") \n        if date not in dict_code2date[code]: # 조건에 부합하는 날짜 (D0 날짜)를 발견할 때까지 continue\n            continue \n\n        # D-9 ~ D0 데이터만 담기\n        sub_stock = lst_stock[idx-9:idx+1] \n        \n        # 10일간의 데이터 \n        lst_info = []\n        for row2 in sub_stock:\n            lst_prices, trading_value = row2[1:5], row2[4]*row2[5]\n            lst_info += lst_prices + [trading_value]\n        info = ','.join(map(str, lst_info))\n\n        # D+1 종가 2% 상승 여부 \n        label = int(lst_stock[idx+1][6] >= 0.02)\n        \n        lst_result.append([code, date, info, label])\n        \n    return lst_result\n\n\nmulti processing 수행\n\n\nstart_time = time.time()\nnum_cores = 10\npool = Pool(num_cores)\nlst_data = pool.map(make_lst_result2, dict_code2date.keys())\npool.close()\npool.join()\nprint(time.time() - start_time) \n\n8.186521768569946\n\n\n앞서 진행했던 방법에서 1분이 걸렸던 작업이 multi processing을 사용하여 8초로 줄어들었다.\n\ntxt 파일 생성\n\n\nOF = open(\"assignment3_multi_processing.txt\", 'w')\n\nfor row in lst_data: \n    for num in range(len(row)): \n        OF.write('\\t'.join(map(str, row[num])) + '\\n')\n        \nOF.close()\n\nprint(f'생성된 데이터의 개수는 {len(pd.read_csv(\"assignment3_multi_processing.txt\"))} 개')\n\n생성된 데이터의 개수는 13939 개\n\n\n\n \n\n\n4번째 방법은 MySQL을 사용하는 것이다. 외부 데이터를 불러오지 않아도 되고, 서버 DB에 저장된 데이터를 불러오는 것이므로 multi processing을 사용하지 않고도 빠른 속도로 데이터셋을 생성할 수 있다.\n과제 I\n최종적으로 머신러닝 분석에 사용할 데이터셋은 코스피, 코스닥 시장에 해당하는 종목들만을 사용한다. 현재 db에 저장되어 있는 데이터도 코스피, 코스닥 시장에 해당하는 종목들이 입력되어 있으며, 해당 종목들을 추린 code_list.txt에서 종목들을 불러온 lst_code를 사용한다.\n\nIF = open('../data/code_list.txt')\nlst_code = IF.readlines()\n\nprint(f'총 {len(df_code)} 개의 종목 중 {len(lst_code)} 개의 종목 선별')\n\n총 2536 개의 종목 중 1561 개의 종목 선별\n\n\n\n# pymysql 설치\n# ! pip install pymysql\n\nimport pymysql \nfrom sqlalchemy import create_engine\n\n\nMySQL 데이터 저장 (dataframe -> sql) \n\ncode 별로 다른 테이블에 저장한다\n\ndb_connection_str = 'mysql+pymysql://[db username]:[db password]@[host address]/[db name]' \ndb_connection = create_engine(db_connection_str)\nconn = db_connection.connect()\n\nfor code in tqdm(lst_code): \n    start_date = '20170101'\n    end_date = '20211231'\n    stock = fdr.DataReader(code, start = start_date, end = end_date)\n    stock = stock.reset_index()\n    stock = stock[['Date', 'Open', 'High', 'Low', 'Close', 'Volume', 'Change']]\n    stock.to_sql(name=f'stock_{code}', con=db_connection, if_exists='fail', index=False)\n\n\nMySQL 에서 데이터 불러온 후 데이터셋 생성\n\n\ndb_dsml = pymysql.connect(\n    host = 'localhost', \n    port = 3306, \n    user = '[db username]', \n    passwd = '[db password]', \n    db = '[db name]', \n    charset = 'utf8'\n)\ncursor = db_dsml.cursor()\n\n과제 II\n\ndict_code2date = {}\nfor code in tqdm(lst_code): \n    code = code.strip()\n    sql_query = '''\n                SELECT *\n                FROM stock_{}\n                WHERE Date BETWEEN '2018-01-01' AND '2020-12-31'\n                '''.format(code)\n    stock = pd.read_sql(sql = sql_query, con = db_dsml)   \n    \n    # 🌟 dataframe -> list     \n    lst_stock = stock.values.tolist()\n    \n    for row in lst_stock: \n        date, trading_value = row[0], row[4]*row[5]\n        if trading_value >= 100000000000:\n            if code not in dict_code2date.keys():\n                dict_code2date[code] = [date.date().strftime(\"%Y%m%d\")]\n            else:\n                dict_code2date[code].append(date.date().strftime(\"%Y%m%d\"))\n\nprint(f'총 {len(lst_code)} 개의 종목 중 {len(dict_code2date)} 개의 종목 사용')\n\n100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████| 1561/1561 [00:18<00:00, 82.67it/s]\n\n\n총 1561 개의 종목 중 679 개의 종목 사용\n\n\n\n\n\n과제 III\n\nOF = open('assignment3_sql.txt', 'w')\nfor code in tqdm(dict_code2date): \n    code = code.strip()\n    sql_query = '''\n                SELECT *\n                FROM stock_{}\n                WHERE Date BETWEEN '2018-01-01' AND '2020-12-31'\n                '''.format(code)\n    stock = pd.read_sql(sql = sql_query, con = db_dsml)  \n    \n    # 🌟 dataframe -> list     \n    lst_stock = stock.values.tolist()  \n       \n    for idx, row in enumerate(lst_stock):   \n        if (idx < 9) or (idx >= len(lst_stock)-1): # 예외 처리 \n            continue \n        \n        date = row[0].date().strftime(\"%Y%m%d\") \n        if date not in dict_code2date[code]: # 조건에 부합하는 날짜 (D0 날짜)를 발견할 때까지 continue\n            continue \n\n        # D-9 ~ D0 데이터만 담기\n        sub_stock = lst_stock[idx-9:idx+1] \n        \n        # 10일간의 데이터 \n        lst_info = []\n        for row2 in sub_stock:\n            lst_prices, trading_value = row2[1:5], row2[4]*row2[5]\n            lst_info += lst_prices + [trading_value]\n        info = ','.join(map(str, lst_info))\n\n        # D+1 종가 2% 상승 여부 \n        label = int(lst_stock[idx+1][6] >= 0.02)\n\n        # 저장 \n        OF.write(f'{code}\\t{date}\\t{info}\\t{label}\\n')\n                         \nOF.close()   \n\nprint(f'생성된 데이터의 개수는 {len(pd.read_csv(\"assignment3_sql.txt\"))} 개')\n\n100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 679/679 [00:09<00:00, 72.67it/s]\n\n\n생성된 데이터의 개수는 11934 개\n\n\nMySQL에서 코드별로 저장되어있는 데이터셋을 불러와 9초의 시간이 걸렸다. multiprocessing으로 코어를 여러개 쓴 것과 비슷한 결과가 나온 것을 확인하였다.\n\n \n\n\n\n최종적으로 생성된 머신러닝 데이터셋의 형태를 확인한다.\n\nIF=open(\"assignment3_sql.txt\",'r')\nlst_code_date=[]\ntrainX=[]\ntrainY=[]\nfor line in IF:\n    code, date, x, y = line.strip().split(\"\\t\")\n    lst_code_date.append([code, date])\n    trainX.append(list(map(int, x.split(\",\"))))\n    trainY.append(int(y))\ntrainX=pd.DataFrame(trainX)\ntrainY=pd.DataFrame(trainY)\n\n\nprint(\"===== trainX =====\")\nprint(\"trainX shape:\", trainX.shape)\ndisplay(trainX.head())\nprint()\nprint(\"===== trainY =====\")\nprint(\"trainY shape:\", trainY.shape)\ndisplay(trainY.head())\n\n===== trainX =====\ntrainX shape: (11935, 50)\n\n\n\n\n\n\n  \n    \n      \n      0\n      1\n      2\n      3\n      4\n      5\n      6\n      7\n      8\n      9\n      ...\n      40\n      41\n      42\n      43\n      44\n      45\n      46\n      47\n      48\n      49\n    \n  \n  \n    \n      0\n      10250\n      12050\n      10150\n      11800\n      307823874200\n      11950\n      12450\n      10900\n      11750\n      240410569500\n      ...\n      15300\n      15400\n      12650\n      13700\n      789063638200\n      13700\n      16100\n      13400\n      15400\n      897154258000\n    \n    \n      1\n      11950\n      12450\n      10900\n      11750\n      240410569500\n      11850\n      14150\n      11600\n      12600\n      764364560400\n      ...\n      13700\n      16100\n      13400\n      15400\n      897154258000\n      14700\n      15500\n      14000\n      14350\n      277027065700\n    \n    \n      2\n      11850\n      14150\n      11600\n      12600\n      764364560400\n      12800\n      13200\n      12000\n      12200\n      170010147600\n      ...\n      14700\n      15500\n      14000\n      14350\n      277027065700\n      13050\n      13300\n      11650\n      11650\n      231873876050\n    \n    \n      3\n      12800\n      13200\n      12000\n      12200\n      170010147600\n      12450\n      13400\n      12350\n      12850\n      211661434950\n      ...\n      13050\n      13300\n      11650\n      11650\n      231873876050\n      12200\n      13150\n      11600\n      12200\n      222393934200\n    \n    \n      4\n      12450\n      13400\n      12350\n      12850\n      211661434950\n      12800\n      12950\n      11300\n      11700\n      91801277100\n      ...\n      12200\n      13150\n      11600\n      12200\n      222393934200\n      12200\n      13750\n      12100\n      12350\n      256196958550\n    \n  \n\n5 rows × 50 columns\n\n\n\n\n===== trainY =====\ntrainY shape: (11935, 1)\n\n\n\n\n\n\n  \n    \n      \n      0\n    \n  \n  \n    \n      0\n      0\n    \n    \n      1\n      0\n    \n    \n      2\n      1\n    \n    \n      3\n      0\n    \n    \n      4\n      1\n    \n  \n\n\n\n\n\n \n4단계의 과정을 거쳐 머신러닝 데이터셋 생성을 마쳤다. 다양한 방법을 통해 전처리 코드를 개선시키는 과정을 배울 수 있었고, 시간 효율성을 높이는 것은 중요한 작업임을 깨달았다. 17분이 걸리던 작업을 8~9초로 줄였으며, 데이터가 많아질 수록 시간이 더 늘어나기 때문에 시간 효율성의 중요성이 커질 것이다. 다음 글에서는 생성된 머신러닝 데이터셋을 사용하여 여러 머신러닝 모델을 학습 및 평가하여 성능이 가장 좋은 모델을 선정하는 baseline model selection을 진행하겠다."
  },
  {
    "objectID": "posts/02.model_selection.html",
    "href": "posts/02.model_selection.html",
    "title": "[stock prediction] 1.2 머신러닝 모델 비교",
    "section": "",
    "text": "이전 글 (1.1. 머신러닝을 위한 주가 데이터셋 생성) 에서는 머신러닝을 위한 시계열 구조의 데이터셋을 생성했다. 이번 글에서는 생성한 데이터셋을 사용하여 머신러닝 모델을 학습하고, 평가지표를 통해 여러 모델의 성능을 비교하여 가장 좋은 성능을 보이는 모델을 선택한다. 이를 통해 생성한 기본 시계열 데이터셋을 머신러닝 알고리즘으로 학습시켰을 때, 최소한의 성능과 수익률이 발생하는지 확인할 수 있고, 이를 통해 baseline model을 정의한다.\n\n\n\n\n\n데이터 불러오기\n\n\n주가 예측 머신러닝 모델 학습\n\n\n\n평가지표 시각화를 통한 모델 평가\n\n\n모델 선택\n\n\n\n\n\n\n\nimport pandas as pd\nimport numpy as np\nfrom tqdm import tqdm\nimport FinanceDataReader as fdr\nimport pymysql\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n \n\n\n\n\n\n\n지난 글에서 생성한 데이터셋을 불러온다. train은 2018.01.02 ~ 2020.12.31, test는 2021.01.02 ~ 2021.06.31 설정한 데이터셋이다.\n\ntrain dataset\n\n\n#collapse-hide\nIF=open(\"assignment3_sql.txt\",'r')\nlst_code_date=[]\ntrainX=[]\ntrainY=[]\nfor line in IF:\n    code, date, x, y = line.strip().split(\"\\t\")\n    lst_code_date.append([code, date])\n    trainX.append(list(map(int, x.split(\",\"))))\n    trainY.append(int(y))\ntrainX=pd.DataFrame(trainX)\ntrainY=pd.DataFrame(trainY)\n\n\ntest dataset\n\n\n#collapse-hide\nIF=open(\"assignment3_sql_test.txt\",'r')\nlst_code_date_test=[]\ntestX=[]\ntestY=[]\nfor line in IF:\n    code, date, x, y = line.strip().split(\"\\t\")\n    lst_code_date_test.append([code, date])\n    testX.append(list(map(int, x.split(\",\"))))\n    testY.append(int(y))\ntestX=pd.DataFrame(testX)\ntestY=pd.DataFrame(testY)\n\n\nshape 확인\n\n\nprint(\"train dataset: \", trainX.shape, trainY.shape)\nprint(\"test dataset: \", testX.shape, testY.shape)\n\ntrain dataset:  (11935, 50) (11935, 1)\ntest dataset:  (4431, 50) (4431, 1)\n\n\n \n\n\n\n앞서 불러온 주가 데이터셋으로 여러 개의 머신러닝 모델을 학습한다. 모델은 총 9개의 분류 알고리즘을 사용하여 비교한다.\n\nmodel list - Logistic Regression - Decision tree - Support vector machine - Gaussian naive bayes - K nearest neighbor - Random forest - Gradient boosing - Neural network - XGBoost\n\n\n모델학습\n\n\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import roc_auc_score\nimport time\n\nresults=[]\n\n##### 1. Logistic regression    \nfrom sklearn.linear_model import LogisticRegression\nlr = LogisticRegression(max_iter=1000)\nlr.fit(trainX, trainY)\n\n##### 2. Decision tree\nfrom sklearn.tree import DecisionTreeClassifier\ndt = DecisionTreeClassifier()\ndt.fit(trainX, trainY)\n\n##### 3. Support vector machine\nfrom sklearn.svm import SVC\nsvc = SVC(probability=True)\nsvc.fit(trainX, trainY)\n\n##### 4. Gaussian naive bayes\nfrom sklearn.naive_bayes import GaussianNB\ngnb = GaussianNB()\ngnb.fit(trainX, trainY)\n\n##### 5. K nearest neighbor\nfrom sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier()\nknn.fit(trainX, trainY)\n\n##### 6. Random forest\nfrom sklearn.ensemble import RandomForestClassifier\nrf = RandomForestClassifier()\nrf.fit(trainX, trainY)\n\n##### 7. Gradient boosing\nfrom sklearn.ensemble import GradientBoostingClassifier\ngb = GradientBoostingClassifier()\ngb.fit(trainX, trainY)\n\n##### 8. Neural network\nfrom sklearn.neural_network import MLPClassifier\nmlp = MLPClassifier(max_iter=1000)\nmlp.fit(trainX, trainY)\n\n##### XGBoost \nfrom xgboost import XGBClassifier\nxgb = XGBClassifier(\n                   n_jobs=40,\n                   scale_pos_weight=4,\n                   learning_rate=0.01,\n                   max_depth=3,\n                   n_estimators=500,\n                   ) \n\nxgb.fit(trainX, trainY)\n\n[00:14:01] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n\n\nXGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n              gamma=0, gpu_id=-1, importance_type=None,\n              interaction_constraints='', learning_rate=0.01, max_delta_step=0,\n              max_depth=3, min_child_weight=1, missing=nan,\n              monotone_constraints='()', n_estimators=500, n_jobs=40,\n              num_parallel_tree=1, predictor='auto', random_state=0,\n              reg_alpha=0, reg_lambda=1, scale_pos_weight=4, subsample=1,\n              tree_method='exact', validate_parameters=1, verbosity=None)\n\n\n\n예측\n\n학습시킨 모델 별로 Accuracy와 AUC score를 측정하여, lst_result_acc, lst_result_roc 리스트에 추가한다.\n\nlst_model = [lr, dt, svc, gnb, knn, rf, gb, mlp, xgb]\n\ndic_model2name = {lr:'LogisticRegression', dt:'DecisionTree', svc:'SVM', \n                  gnb:'GaussianNaiveBayes', knn:'KNN', rf:'RandomForest', \n                  gb:'GradientBoosting', mlp:'NeuralNetwork', xgb:'XGBoost'}\n\nlst_result_acc = [] # [['모델명', train 평가지표, test 평가지표], ...]\nlst_result_roc = []\n\nfor model in lst_model: \n    # accuracy \n    predY_train = model.predict(trainX)\n    predY_test = model.predict(testX)\n    \n    accuracy_train = accuracy_score(trainY, predY_train)\n    accuracy_test = accuracy_score(testY, predY_test)\n\n    # auc score \n    probY_train = model.predict_proba(trainX)[:, 1]\n    probY_test = model.predict_proba(testX)[:, 1]\n    \n    roc_score_train = roc_auc_score(trainY, probY_train)\n    roc_score_test = roc_auc_score(testY, probY_test)\n    \n    # 리스트에 추가 \n    lst_result_acc.append([dic_model2name[model], accuracy_train, accuracy_test])\n    lst_result_roc.append([dic_model2name[model], roc_score_train, roc_score_test])\n    \nprint('accuracy list: ', lst_result_acc[:2])\nprint()\nprint('auc score list: ', lst_result_roc[:2])\n\naccuracy list:  [['LogisticRegression', 0.7622957687473817, 0.7734145791017829], ['DecisionTree', 1.0, 0.598059128864816]]\n\nauc score list:  [['LogisticRegression', 0.5318105913341066, 0.5430800870053489], ['DecisionTree', 1.0, 0.4824088530616373]]\n\n\n \n\n\n\n위에서 얻은 9가지 모델의 평가 지표(accuracy, auc score) 결과 리스트를 받아 모델 별로 평가지표를 비교하는 bar graph 시각화 함수를 작성한다.\ntest 데이터셋의 성능이 높은 순서대로 정렬하여 시각화한다.\n\ndef result_plot(lst:list, index:str): \n    '''\n    lst: [['모델명', 'train 평가지표', 'test 평가지표'], ...]\n    index: 'acc' or 'auc'\n    '''\n    import seaborn as sns\n    import matplotlib.pyplot as plt\n    \n    if index == 'acc': \n        plt.figure(figsize=(12, 5))  \n        plt.title(\"ACCURACY\", fontsize=20)\n        \n        df = pd.DataFrame(data=lst_result_acc, \n                     columns=['name', 'train', 'test']).set_index('name')\n        \n        df_results = df.reset_index().melt(id_vars='name')\n      \n    elif index == 'auc': \n        plt.figure(figsize=(12, 5))  \n        plt.title(\"ROCAUC\", fontsize=20)\n        df = pd.DataFrame(data=lst_result_roc, \n                     columns=['name', 'train', 'test']).set_index('name')\n        df_results = df.reset_index().melt(id_vars='name')\n        \n    else:\n        print(\"평가지표를 다시 입력해주세요.\")\n        \n      \n    ax = sns.barplot(x=\"name\", y=\"value\", hue='variable', data=df_results, capsize=.2, \n                     order=df['test'].sort_values(ascending=False).index)\n    ax.set(ylim=(0.00, 1.1))\n    ax.set_xticklabels(ax.get_xticklabels(),rotation=60) \n    \n    for p in ax.patches:\n        height = p.get_height()\n        ax.text(p.get_x() + p.get_width() / 2., height + 0.01, round(height, 2), ha = 'center', size = 10)\n\n    plt.show()\n\nAccuracy\n\nresult_plot(lst_result_acc, 'acc')\n\n\n\n\nAUROC\n\nresult_plot(lst_result_acc, 'auc')\n\n\n\n\nAccuracy는 Logistic Regression, ROCAUC는 XGBoost가 가장 높은 값을 얻은 것을 확인하였다.\n \n\n\n\n\n1) 선택 기준 평가지표 선택 - 종속변수 불균형 문제\n\n모델 선택을 위해 사용할 평가지표를 선택한다.\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfig = plt.figure(figsize=(10, 3))\nax1, ax2 = fig.subplots(1, 2).flatten()\n\n# countplot 시각화\nsns.countplot(data=trainY, x=0, ax=ax1)\nsns.countplot(data=testY, x=0, ax=ax2)\n\n# title 설정\nax1.set_title(\"train\", fontsize=17)\nax2.set_title(\"test\", fontsize=17)\n\n# ylim 설정\nax1.set_ylim(0, 10500)\nax2.set_ylim(0, 3900)\n\n# text 추가\nfor p in ax1.patches:\n    height = p.get_height()\n    ax1.text(p.get_x() + p.get_width() / 2., height + 200, height, ha = 'center', size = 12)\n    \nfor p in ax2.patches:\n    height = p.get_height()\n    ax2.text(p.get_x() + p.get_width() / 2., height + 80, height, ha = 'center', size = 12)\n\n# class 0, 1 비율 계산\nprint(\"===============train===============\")\nprint(\"0의 비율: \", sum(trainY[0] == 0) / (sum(trainY[0]==0) + sum(trainY[0]==1)))\nprint(\"1의 비율: \", sum(trainY[0] == 1) / (sum(trainY[0]==0) + sum(trainY[0]==1)))\nprint()\nprint(\"===============test================\")\nprint(\"0의 비율: \", sum(testY[0] == 0) / (sum(testY[0]==0) + sum(testY[0]==1)))\nprint(\"1의 비율: \", sum(testY[0] == 1) / (sum(testY[0]==0) + sum(testY[0]==1)))\nprint()\n\n===============train===============\n0의 비율:  0.7622957687473817\n1의 비율:  0.23770423125261836\n\n===============test================\n0의 비율:  0.7734145791017829\n1의 비율:  0.2265854208982171\n\n\n\n\n\n\n이진분류 된 종속변수를 countplot으로 시각화하여 분포를 살펴보았을 때, 종속변수가 0인 데이터의 비율이 높은 것을 확인할 수 있습니다. 이는 모델이 모두 0이라고 예측해도 정확도가 76~77%가 나온다는 뜻이다. 따라서 정확도는 해당 주가 데이터를 평가하기 위한 지표로는 신뢰성이 떨어진다고 판단되어, threshold별 score가 결정되는 ROCAUC score를 기준으로 모델을 선택한다.\n\n\n2) 예측 신뢰성\n\ntest dataset의 AUC가 높은 상위 2개 모델(XGBoost, GradientBoosting)의 예측 확률을 살펴본다.\n\nxgb_prob = xgb.predict_proba(testX)[:, 1] \ngb_prob = gb.predict_proba(testX)[:, 1]\n\nprint('XGB:', sum(xgb_prob >= 0.6), '개')\nprint('GB:', sum(gb_prob >= 0.6), '개')\n\nXGB: 1268 개\nGB: 1 개\n\n\nclass를 1로 예측할 확률이 60% 이상인 데이터의 개수를 확인해보면, XGBoost는 1268개, GradientBoosting은 1개가 나온다.\n\n1), 2) 를 근거로 하여 여러 머신러닝 모델 중 AUC 점수 1위인 동시에, 예측 신뢰성이 더 높다고 판단되는 XGBoost 를 최종적으로 baseline 머신러닝 모델로 결정합니다.\n \n\n\n\nbaseline 모델 정리\n\n데이터\n\n종목: kospi + kosdaq 주식 시장 3년 이상 존속하였던 종목들\n기간: 2018.01.01-2020.12.31 (총 3년)\n필터링: 거래대금 1000억 이상\n구조: 10일 간의 데이터들을 feature로 두어 시계열 데이터셋 생성\n\n모델\nXGBClassifier(\nn_jobs=40,\nscale_pos_weight=4,\nlearning_rate=0.01,\nmax_depth=3,\nn_estimators=500,\n)\n\n수익률 측정\nbaseline모델을 선정하였으므로 프로젝트에서 중요한 부분인 수익률 측정을 수행한다.\n\nprint('XGB:', sum(xgb_prob >= 0.65), '개')\n\nXGB: 34 개\n\n\n해당 수익률 계산 시뮬레이션에서는 prob 임계값을 0.65로 설정함으로써 6개월 동안 1~2주에 한번 꼴로 매매가 이루어지도록 하였다.\n\ndef compute_earnings_rate(lst_code_date, probY):\n    ##### DB \n    db_dsml = pymysql.connect(\n        host = 'localhost', \n        port = 3306, \n        user = '[db username]', \n        passwd = '[db password]', \n        db = '[db name]', \n        charset = 'utf8'\n    )\n    \n    cursor = db_dsml.cursor()\n    \n    ##### 주문 일지 작성 #####\n    lst_output=[]\n    for (code, date), y in zip(lst_code_date, probY):\n        if y >= 0.65: # 예측 확률 (probY) 임계값 0.65 \n            lst_output.append([code, date, \"buy\", \"all\"])  \n            lst_output.append([code, date+\"n\", \"sell\", \"all\"])  \n            \n    lst_output.sort(key=lambda x:x[1]) # 날짜로 정렬 \n    \n    \n    ##### 수익률 계산 #####\n    start_money = 10000000 # 초기 현금 1천만원\n    money = start_money\n    dic_code2num ={}  # 보유 종목\n    \n    for i, row in enumerate(tqdm(lst_output)): # 주문 일지를 한 줄 읽어 옴\n        code, date, request, amount = row\n\n        sql_query = '''\n                    SELECT *\n                    FROM stock_{}\n                    WHERE Date BETWEEN '2021-01-01' AND '2021-06-31'\n                    '''.format(code)\n        stock = pd.read_sql(sql = sql_query, con = db_dsml)\n        lst_data = stock.values.tolist()    \n\n        \n        for idx, row in enumerate(lst_data):\n            data_date = row[0].date().strftime('%Y%m%d')\n\n            if 'n' in date: # 매도 날짜\n                date_n = date[:-1]\n                if data_date == date_n: \n                    close = lst_data[idx+1][-3] # 매도 할 종가 (매수 다음날 종가)\n                    break            \n\n            else: # 매수 날짜 \n                if data_date == date:\n                    close = row[-3] # 매수 할 종가 \n                    break\n        \n        if request == 'buy': # 매수 \n            if amount.startswith('r'): # 분할 매수 시 사용 \n                request_money = money * float(amount.lstrip(\"r\")) / 100\n            elif amount == 'all':\n                request_money = money\n            elif amount.isdigit():\n                request_money = int(amount)\n            else:\n                raise Exception('Not permitted option')\n                \n            request_money = min(request_money, money)\n            \n            buy_num = int(request_money / close) # 매수 개수 \n            \n            money -= buy_num * close  # 매수 후 잔고 업데이트 \n            \n            if code not in dic_code2num: # 현재 보유종목이 아니라면 \n                dic_code2num[code] = 0 # dict에 key 생성 \n                \n            dic_code2num[code] += buy_num # 해당 종목의 보유 개수 업데이트\n            \n        if request == 'sell': # 매도 \n            if amount == 'all':   \n                sell_num = dic_code2num[code] # 해당 종목의 보유하고 있는 모든 개수 매도\n            else:\n                raise Exception('Not permitted option')       \n                \n            money += sell_num * close # 매도 후 잔고 업데이트 \n            \n            dic_code2num[code] -= sell_num # 해당 종목 매도 후 보유 개수 업데이트 \n            if dic_code2num[code] == 0: # 해당 종목이 보유하고 있는 개수가 없다면 \n                del dic_code2num[code] # 보유종목에서 삭제 \n\n    if dic_code2num != {}: # 주문일지를 모두 돌고난 후에는 보유종목 dict가 비어있어야 함. \n        raise Exception('Not empty stock') \n\n    print(\"Final earning rate : {} %\".format(str((money-start_money) / start_money * 100)))\n#     return str((money-start_money) / start_money * 100)\n\n\ncompute_earnings_rate(lst_code_date_test, xgb_prob)\n\n100%|█████████████████████████████████████████████| 68/68 [00:00<00:00, 265.33it/s]\n\n\nFinal earning rate : 54.73516000000001 %\n\n\n\n\n\n최종적으로 baseline 모델의 수익률을 계산해봤을 때, 6개월간의 총 수익률은 약 54%가 나왔다. 평가지표인 ROCAUC score는 낮은 편에 속했지만, 높은 수익률을 보였다.\n다음 글 부터는 data preprocessing을 진행하며, 주가 빅데이터의 질을 높여 baseline model 보다 뛰어난 성능을 보이도록 개선한다."
  },
  {
    "objectID": "posts/03.add_index.html",
    "href": "posts/03.add_index.html",
    "title": "[stock prediction] 2.1 주가 데이터셋 보조지표 추가",
    "section": "",
    "text": "이전 글 [Stock Research] 1.2. 머신러닝 모델 비교까지 주가 데이터셋 생성과 머신러닝 모델 비교를 통해 baseline model을 생성했다. 이번 글 부터는 데이터 전처리를 진행한다. 모델을 이전 글에서 선택했던 baseline 모델로 fix하고, 데이터의 질을 높임으로써 성능을 향상시킨다. 그 첫번째로 주가데이터셋에 보조지표를 추가하여 설명변수의 크기를 늘린다.\n\n\n\n\n주식 데이터의 보조지표\n\n\n보조지표 추가\n\n\n\n모델 학습\n\n\n\n\nimport pandas as pd\nimport numpy as np\nfrom tqdm import tqdm\nimport FinanceDataReader as fdr\nimport pymysql\n\nimport warnings\nwarnings.filterwarnings('ignore')\n    \nimport ta\n\nfrom ipywidgets import interact, interact_manual\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.graph_objects as go\nimport plotly.offline as pyo\n%matplotlib inline\n%pylab inline\npylab.rcParams['figure.figsize'] = (12,5)\n\n%pylab is deprecated, use %matplotlib inline and import the required libraries.\nPopulating the interactive namespace from numpy and matplotlib\n\n\n \n\n\n\n\n주식데이터에서 보조지표란 기술적지표라고도 불리며, 다양한 각도와 계산식, 통계 등을 바탕으로 기본적 분석 방법들과 조합하여 보다 폭 넓은 시장 예측을 가능하게 도와주는 차트 분석 도구이다.\n[참고]\n-01 보조지표란?\n그 중에서도, 주식 시장에서 많이 알려져 있고, 주가 예측에 자주 사용되는 보조지표로는 이동평균선을 예로 들어 확인해보겠다. 이동평균선을 캔들차트와 함께 시각화 하고, 각 종목과 날짜에 대해 어떻게 나타나는지 직접 확인한다.\n\n데이터 불러오기\n\n2018-01-01 ~ 2020-12-31 기간 동안의 모든 데이터를 미리 저장해 놓은 주가 데이터셋을 불러온다.\n\ndf_stock = pd.read_csv(\"stock_data_2018_2020.csv\")\ndf_stock['Code'] = df_stock['Code'].apply(lambda x : str(x).zfill(6))\ndf_stock\n\n\n\n\n\n  \n    \n      \n      Code\n      Date\n      Open\n      High\n      Low\n      Close\n      Volume\n      Change\n    \n  \n  \n    \n      0\n      050120\n      2018-01-02\n      10250\n      12050\n      10150\n      11800\n      26086769\n      0.145631\n    \n    \n      1\n      050120\n      2018-01-03\n      11950\n      12450\n      10900\n      11750\n      20460474\n      -0.004237\n    \n    \n      2\n      050120\n      2018-01-04\n      11850\n      14150\n      11600\n      12600\n      60663854\n      0.072340\n    \n    \n      3\n      050120\n      2018-01-05\n      12800\n      13200\n      12000\n      12200\n      13935258\n      -0.031746\n    \n    \n      4\n      050120\n      2018-01-08\n      12450\n      13400\n      12350\n      12850\n      16471707\n      0.053279\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      1152013\n      000540\n      2020-12-23\n      2880\n      2945\n      2835\n      2870\n      87318\n      -0.010345\n    \n    \n      1152014\n      000540\n      2020-12-24\n      2850\n      2875\n      2845\n      2860\n      28350\n      -0.003484\n    \n    \n      1152015\n      000540\n      2020-12-28\n      2860\n      3000\n      2805\n      2820\n      66036\n      -0.013986\n    \n    \n      1152016\n      000540\n      2020-12-29\n      2820\n      2920\n      2705\n      2775\n      83187\n      -0.015957\n    \n    \n      1152017\n      000540\n      2020-12-30\n      2835\n      2835\n      2755\n      2830\n      33270\n      0.019820\n    \n  \n\n1152018 rows × 8 columns\n\n\n\n\n이동평균선 캔들차트 시각화\n\n\nIF = open('../data/code_list.txt')\nlst_code = IF.readlines()\n\nfor idx, code in enumerate(lst_code):\n    lst_code[idx] = code.strip()\n\n\n@interact\ndef show_label_dist(code = lst_code):\n    df = df_stock[df_stock['Code']==code]    \n    ma_ls = [5, 20, 60, 120]\n    for i in range(len(ma_ls)):\n        sr1 = df['Close'].rolling(window=ma_ls[i]).mean()\n        df['MA'+str(ma_ls[i])] = sr1\n    df = df.dropna(axis=0)\n    \n    # 캔들 차트 \n    candle = go.Candlestick(x=df['Date'],\n                open=df['Open'],\n                high=df['High'],\n                low=df['Low'],\n                close=df['Close'], increasing_line_color= 'red', decreasing_line_color= 'blue')\n    \n    # 이동평균선\n    line_ma5 = go.Scatter(x=df['Date'], y=df['MA5'], mode='lines', name='MA5', line=dict(color='magenta', width=0.5))\n    line_ma20 = go.Scatter(x=df['Date'], y=df['MA20'], mode='lines', name='MA20', line=dict(color='blue', width=0.5))\n    line_ma60 = go.Scatter(x=df['Date'], y=df['MA60'], mode='lines', name='MA60', line=dict(color='green', width=0.5))\n    line_ma120 = go.Scatter(x=df['Date'], y=df['MA120'], mode='lines', name='MA120', line=dict(color='red', width=0.5))\n    \n    # 제목 추가\n    layout = go.Layout(title='{} 캔들차트'.format(code), titlefont=dict(size=20, color='black'))\n    \n    fig = go.Figure(data=[candle, line_ma5, line_ma20, line_ma60, line_ma120], layout=layout)\n    \n    fig.show()\n\n\n\n\n위의 캔들차트에서 원하는 코드를 선택하여 4일, 20일, 60일, 120일 이동평균선을 확인할 수 있다.\n \n\n\n\n현재 baseline 모델에서는 10일치의 시가, 고가, 저가, 종가, 거래대금을 독립변수로 하여 학습을 진행하였다.\n출처-네이버 증권\n하지만 10일간의 주가 차트 예시 사진을 보면, 너무 적은 정보만을 담고 있어 주가 예측을 위한 패턴을 찾아내기란 매우 어렵다. 따라서 주식의 여러 보조지표를 사용해서 해당 주식에 대한 압축된 정보를 독립변수에 추가해준다.\n앞서 시각화 해보았던 이동평균선 말고도 다양한 주식 보조지표들이 존재한다. 보조지표들을 모두 수동으로 계산하기에는 어려움이 있으므로 TA 라이브러리를 사용하여 총 49개의 보조지표를 추가한다. (+ trading_value)\n-TA library github\n\n보조지표 추가 코드\n\n\ndf_index = pd.DataFrame()\nfor code, stock_df in tqdm(df_stock.groupby('Code')):\n    \n    # 이평선 생성\n    ma = [5,20,60,120]\n    for days in ma:\n        stock_df['ma_'+str(days)] = stock_df['Close'].rolling(window = days).mean()\n    \n    # 여러 보조 지표 생성\n    H, L, C, V = stock_df['High'], stock_df['Low'], stock_df['Close'], stock_df['Volume']\n    \n    stock_df['trading_value'] = stock_df['Close']*stock_df['Volume']\n    \n    stock_df['MFI'] = ta.volume.money_flow_index(\n        high=H, low=L, close=C, volume=V, fillna=True)\n    \n    stock_df['ADI'] = ta.volume.acc_dist_index(\n        high=H, low=L, close=C, volume=V, fillna=True)\n    \n    stock_df['OBV'] = ta.volume.on_balance_volume(close=C, volume=V, fillna=True)\n    stock_df['CMF'] = ta.volume.chaikin_money_flow(\n        high=H, low=L, close=C, volume=V, fillna=True)\n    \n    stock_df['FI'] = ta.volume.force_index(close=C, volume=V, fillna=True)\n    stock_df['EOMEMV'] = ta.volume.ease_of_movement(\n        high=H, low=L, volume=V, fillna=True)\n    \n    stock_df['VPT'] = ta.volume.volume_price_trend(close=C, volume=V, fillna=True)\n    stock_df['NVI'] = ta.volume.negative_volume_index(close=C, volume=V, fillna=True)\n    stock_df['VMAP'] = ta.volume.volume_weighted_average_price(\n        high=H, low=L, close=C, volume=V, fillna=True)\n    \n    # Volatility\n    stock_df['ATR'] = ta.volatility.average_true_range(\n        high=H, low=L, close=C, fillna=True)\n    stock_df['BHB'] = ta.volatility.bollinger_hband(close=C, fillna=True)\n    stock_df['BLB'] = ta.volatility.bollinger_lband(close=C, fillna=True)\n    stock_df['KCH'] = ta.volatility.keltner_channel_hband(\n        high=H, low=L, close=C, fillna=True)\n    stock_df['KCL'] = ta.volatility.keltner_channel_lband(\n        high=H, low=L, close=C, fillna=True)\n    stock_df['KCM'] = ta.volatility.keltner_channel_mband(\n        high=H, low=L, close=C, fillna=True)\n    stock_df['DCH'] = ta.volatility.donchian_channel_hband(\n        high=H, low=L, close=C, fillna=True)\n    stock_df['DCL'] = ta.volatility.donchian_channel_lband(\n        high=H, low=L, close=C, fillna=True)\n    stock_df['DCM'] = ta.volatility.donchian_channel_mband(\n        high=H, low=L, close=C, fillna=True)\n    stock_df['UI'] = ta.volatility.ulcer_index(close=C, fillna=True)\n    # Trend\n    stock_df['SMA'] = ta.trend.sma_indicator(close=C, fillna=True)\n    stock_df['EMA'] = ta.trend.ema_indicator(close=C, fillna=True)\n    stock_df['WMA'] = ta.trend.wma_indicator(close=C, fillna=True)\n    stock_df['MACD'] = ta.trend.macd(close=C, fillna=True)\n    stock_df['ADX'] = ta.trend.adx(high=H, low=L, close=C, fillna=True)\n    stock_df['VIneg'] = ta.trend.vortex_indicator_neg(\n        high=H, low=L, close=C, fillna=True)\n    stock_df['VIpos'] = ta.trend.vortex_indicator_pos(\n        high=H, low=L, close=C, fillna=True)\n    stock_df['TRIX'] = ta.trend.trix(close=C, fillna=True)\n    stock_df['MI'] = ta.trend.mass_index(high=H, low=L, fillna=True)\n    stock_df['CCI'] = ta.trend.cci(high=H, low=L, close=C, fillna=True)\n    stock_df['DPO'] = ta.trend.dpo(close=C, fillna=True)\n    stock_df['KST'] = ta.trend.kst(close=C, fillna=True)\n    stock_df['Ichimoku'] = ta.trend.ichimoku_a(high=H, low=L, fillna=True)\n    stock_df['ParabolicSAR'] = ta.trend.psar_down(\n        high=H, low=L, close=C, fillna=True)\n    stock_df['STC'] = ta.trend.stc(close=C, fillna=True)\n    # Momentum\n    stock_df['RSI'] = ta.momentum.rsi(close=C, fillna=True)\n    stock_df['SRSI'] = ta.momentum.stochrsi(close=C, fillna=True)\n    stock_df['TSI'] = ta.momentum.tsi(close=C, fillna=True)\n    stock_df['UO'] = ta.momentum.ultimate_oscillator(\n        high=H, low=L, close=C, fillna=True)\n    stock_df['SR'] = ta.momentum.stoch(close=C, high=H, low=L, fillna=True)\n    stock_df['WR'] = ta.momentum.williams_r(high=H, low=L, close=C, fillna=True)\n    stock_df['AO'] = ta.momentum.awesome_oscillator(high=H, low=L, fillna=True)\n    stock_df['KAMA'] = ta.momentum.kama(close=C, fillna=True)\n    stock_df['ROC'] = ta.momentum.roc(close=C, fillna=True)\n    stock_df['PPO'] = ta.momentum.ppo(close=C, fillna=True)\n    stock_df['PVO'] = ta.momentum.pvo(volume=V, fillna=True)\n    \n    df_index = df_index.append(stock_df) \n\n# 저장\ndf_index.to_csv(\"stock_data_2018_2020_add_index.csv\", index=False)\ndf_index\n\n100%|██████████████████████████████████████████| 1561/1561 [07:35<00:00,  3.42it/s]\n\n\n\n\n\n\n  \n    \n      \n      Code\n      Date\n      Open\n      High\n      Low\n      Close\n      Volume\n      Change\n      ma_5\n      ma_20\n      ...\n      SRSI\n      TSI\n      UO\n      SR\n      WR\n      AO\n      KAMA\n      ROC\n      PPO\n      PVO\n    \n  \n  \n    \n      862985\n      000020\n      2018-01-02\n      9750\n      9900\n      9700\n      9870\n      120676\n      0.012308\n      NaN\n      NaN\n      ...\n      0.000000\n      0.000000\n      0.000000\n      85.000000\n      -15.000000\n      0.000000\n      9870.000000\n      0.000000\n      0.000000\n      0.000000\n    \n    \n      862986\n      000020\n      2018-01-03\n      9900\n      10250\n      9820\n      10000\n      268220\n      0.013171\n      NaN\n      NaN\n      ...\n      0.000000\n      100.000000\n      28.571429\n      54.545455\n      -45.454545\n      0.000000\n      9925.471999\n      0.000000\n      0.104967\n      8.943334\n    \n    \n      862987\n      000020\n      2018-01-04\n      10050\n      10050\n      9680\n      9750\n      161342\n      -0.025000\n      NaN\n      NaN\n      ...\n      0.000000\n      95.815900\n      25.000000\n      12.280702\n      -87.719298\n      0.000000\n      9854.744908\n      0.000000\n      -0.015865\n      9.215678\n    \n    \n      862988\n      000020\n      2018-01-05\n      9750\n      9980\n      9750\n      9910\n      116604\n      0.016410\n      NaN\n      NaN\n      ...\n      0.000000\n      92.627651\n      33.333333\n      40.350877\n      -59.649123\n      0.000000\n      9882.663078\n      0.000000\n      0.018877\n      6.837356\n    \n    \n      862989\n      000020\n      2018-01-08\n      10000\n      10150\n      9940\n      9950\n      158326\n      0.004036\n      9896.0\n      NaN\n      ...\n      0.000000\n      90.156386\n      30.612245\n      47.368421\n      -52.631579\n      0.000000\n      9935.876263\n      0.000000\n      0.078152\n      7.233628\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      43418\n      950130\n      2020-12-22\n      23950\n      23950\n      22800\n      22900\n      978011\n      -0.043841\n      23490.0\n      22212.5\n      ...\n      0.601825\n      -1.890990\n      29.641945\n      41.379310\n      -58.620690\n      577.058824\n      22725.390944\n      17.737789\n      -0.505568\n      -12.450728\n    \n    \n      43419\n      950130\n      2020-12-23\n      23000\n      23300\n      21000\n      21450\n      1239228\n      -0.063319\n      23080.0\n      22310.0\n      ...\n      0.353271\n      -3.660151\n      28.338277\n      23.963134\n      -76.036866\n      353.382353\n      22679.784965\n      -15.049505\n      -1.064724\n      -16.358507\n    \n    \n      43420\n      950130\n      2020-12-24\n      21500\n      21800\n      19600\n      21700\n      857865\n      0.011655\n      22700.0\n      22382.5\n      ...\n      0.083197\n      -4.812902\n      38.852203\n      20.792079\n      -79.207921\n      -58.235294\n      22583.248771\n      -12.323232\n      -1.408773\n      -20.829146\n    \n    \n      43421\n      950130\n      2020-12-28\n      21750\n      21800\n      19500\n      19650\n      1523257\n      -0.094470\n      21930.0\n      22322.5\n      ...\n      0.000000\n      -7.972071\n      33.462925\n      1.470588\n      -98.529412\n      -402.647059\n      22055.737842\n      -13.245033\n      -2.394064\n      -22.448964\n    \n    \n      43422\n      950130\n      2020-12-29\n      19950\n      22450\n      19900\n      21500\n      1568505\n      0.094148\n      21440.0\n      22392.5\n      ...\n      0.526558\n      -8.199398\n      40.132825\n      28.169014\n      -71.830986\n      -759.852941\n      22026.178902\n      -9.473684\n      -2.489633\n      -23.522675\n    \n  \n\n1149377 rows × 58 columns\n\n\n\n보조지표를 추가하여 8개였던 컬럼이 58개의 컬럼이 된 것을 확인하였다.\n \n\n\n\n\n데이터셋 생성 함수\n\n위에서 보조지표를 추가 했던 방법과 동일하게 미리 생성해 놓은 보조지표 추가 데이터셋을 서버 DB에서 불러온다. baseline 모델에서 진행했던 데이터셋을 더 늘려 train 2017-2020, test 2021 기간 동안의 데이터를 사용한다.\n\ndef make_dataset(train=True):\n    \n    IF = open('../data/code_list.txt')\n    lst_code = IF.readlines()\n\n    lst_X = []\n    lst_Y = []\n    lst_code_date = []\n    \n    db_dsml = pymysql.connect(\n        host = 'localhost', \n        port = 3306, \n        user = '[db username]', \n        passwd = '[db password]', \n        db = '[db name]', \n        charset = 'utf8'\n    )\n    cursor = db_dsml.cursor()\n    \n    \n    for code in tqdm(lst_code): \n        code = code.strip()\n        \n        if train: \n            sql_query = '''\n                        SELECT *\n                        FROM stock_{}\n                        WHERE Date BETWEEN '2017-01-01' AND '2020-12-31'\n                        '''.format(code)\n        else:\n            sql_query = '''\n                        SELECT *\n                        FROM stock_{}\n                        WHERE Date BETWEEN '2021-01-01' AND '2021-12-31'\n                        '''.format(code)\n        \n        stock = pd.read_sql(sql = sql_query, con = db_dsml)   \n\n        lst_stock = stock.values.tolist()\n\n\n        for idx, row in enumerate(lst_stock): \n            date, trading_value = row[0].date().strftime(\"%Y%m%d\"), row[4]*row[5]\n            if trading_value >= 100000000000:\n                if (idx < 9) or (idx >= len(lst_stock)-1): # 예외 처리 \n                    continue \n                \n                # D-9 ~ D0 데이터만 담기 \n                sub_stock = lst_stock[idx-9:idx+1] \n\n                # 10일간의 데이터 \n                lst_result = []\n                for row2 in sub_stock:\n                    lst_prices, lst_index = row2[1:6], row2[8:]\n                    lst_result += lst_prices + lst_index + [trading_value]\n\n                # D+1 종가 2% 상승 여부 \n                label = int(row[7] >= 0.02)\n                \n                # 종속변수, 독립변수, 종목코드, 날짜 리스트에 추가 \n                lst_X.append(lst_result)\n                lst_Y.append(label)\n                lst_code_date.append([code, date])\n            \n    return np.array(lst_X), np.array(lst_Y), np.array(lst_code_date)\n\n\ntrain dataset 생성\n\n\ntrainX, trainY, lst_code_date = make_dataset(train=True)\n\n100%|██████████████████████████████████████████| 1561/1561 [02:16<00:00, 11.44it/s]\n\n\n\ntest dataset 생성\n\n\ntestX, testY, lst_code_date_test = make_dataset(train=False)\n\n100%|██████████████████████████████████████████| 1561/1561 [00:40<00:00, 38.37it/s]\n\n\n\npickle 저장\n\n\n#collapse-hide\nimport pickle\n\ndic_result = {\n    'train': [trainX, trainY, lst_code_date],\n    'test': [testX, testY, lst_code_date_test]\n}\n\nwith open('dataset_2020_2021.pickle', 'wb') as handle:\n    pickle.dump(dic_result, handle, protocol=pickle.HIGHEST_PROTOCOL)\n\n\npickle 불러오기\n\n\n#collapse-hide\nwith open('dataset_2020_2021.pickle', 'rb') as handle:\n    dataset = pickle.load(handle)\n\n\ntrainX, trainY, lst_code_date = dataset['train'][0], dataset['train'][1], dataset['train'][2]\ntestX, testY, lst_code_date_test = dataset['test'][0], dataset['test'][1], dataset['test'][2]\n\nprint('train dataset: ', trainX.shape, trainY.shape)\nprint('test dataset: ', testX.shape, testY.shape)\n\ntrain dataset:  (13870, 550) (13870,)\ntest dataset:  (8341, 550) (8341,)\n\n\n\nXGBoost 모델 학습\n\n\nfrom xgboost import XGBClassifier\nxgb = XGBClassifier(\n                   n_jobs=40,\n                   scale_pos_weight=4,\n                   learning_rate=0.01,\n                   max_depth=3,\n                   n_estimators=500,\n                   ) \n\nxgb.fit(trainX, trainY)\n\n[17:26:28] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n\n\nXGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n              gamma=0, gpu_id=-1, importance_type=None,\n              interaction_constraints='', learning_rate=0.01, max_delta_step=0,\n              max_depth=3, min_child_weight=1, missing=nan,\n              monotone_constraints='()', n_estimators=500, n_jobs=40,\n              num_parallel_tree=1, predictor='auto', random_state=0,\n              reg_alpha=0, reg_lambda=1, scale_pos_weight=4, subsample=1,\n              tree_method='exact', validate_parameters=1, verbosity=None)\n\n\n\n모델 평가지표 시각화 함수\ntrain, test 데이터셋에 대하여 Accuracy, AUC, f1_score, precision, recall 평가지표를 구하고, 시각화 한다.\n\n\ndef plot_evauate(trainX, trainY, testX, testY, model):\n    from sklearn.metrics import roc_curve, roc_auc_score, f1_score, f1_score, accuracy_score, recall_score, precision_score\n    \n    train_pred = model.predict(trainX)\n    train_prob = model.predict_proba(trainX)[:, 1]\n    \n    test_pred = model.predict(testX) \n    test_prob = model.predict_proba(testX)[:, 1]\n    \n    \n    # ROC Curve 시각화 \n    fpr, tpr, thresholds = roc_curve(testY, test_prob) \n    \n    plt.plot(fpr, tpr, color='red', label='ROC')\n    plt.plot([0, 1], [0, 1], color='green', linestyle='--')\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('test ROC : {}'.format(round(roc_auc_score(testY, test_prob),3)),fontsize=25)\n    plt.legend()\n    plt.show()\n    \n    \n    \n    # 여러 평가지표 시각화 \n    dic_name2func = { 'F1-score': f1_score, \n                     'Recall': recall_score, \n                     'Precision': precision_score, \n                     'Accuracy': accuracy_score, \n                     'ROCAUC': roc_auc_score }\n    \n    lst_result = []\n    for name, func in dic_name2func.items():\n        if name == 'ROCAUC':\n            train = func(trainY, train_prob)\n            test = func(testY, test_prob)\n            \n        else:\n            train = func(trainY, train_pred)\n            test = func(testY, test_pred)\n        \n        lst_result.append([name, train, test])\n    \n    df = pd.DataFrame(data=lst_result,\n                columns=['name', 'train', 'test'])\n    df = df.melt(id_vars='name')    \n        \n    ax = sns.barplot(data=df, x='name', y='value', hue='variable')\n    ax.set_ylim(0, 1)\n    \n    # 텍스트 추가 \n    for p in ax.patches:\n        height = p.get_height()\n        ax.text(p.get_x() + p.get_width() / 2., height + 0.01, round(height, 3), ha = 'center', size = 10)\n\n    plt.show()\n\n\n성능 평가\n\n\nplot_evauate(trainX, trainY, testX, testY, xgb)\n\n\n\n\n\n\n\nrocauc score는 0.595의 성능을 보였다. 데이터의 기간을 늘리고, 보조지표를 추가함으로써 지난 글의 Baseline Model의 0.56 보다 성능이 향상되었다.\n다음 글에서는 현재 사용하고 있는 통합 종목 주가 데이터셋의 종목별 가격이 모두 다른 점을 보완하기 위해 데이터를 표준화하는 시간을 가진다."
  },
  {
    "objectID": "posts/04.scaling.html",
    "href": "posts/04.scaling.html",
    "title": "[stock prediction] 2.2 주가 데이터 스케일링",
    "section": "",
    "text": "이전 글 2.1 주가 데이터셋 보조지표 추가까지 주가 데이터에 보조지표를 추가하여 설명변수를 50개 가량 늘렸다. 그런데 현재 사용하고 있는 통합 종목 주가 데이터는 종목마다 가격이 다르다는 문제점이 존재한다. 이는 성능을 저하시기는 원인이 될 수 있다. 따라서 주가 데이터셋의 전처리 파트에서 스케일링은 매우 중요한 요소라고할 수 있다. min-max scaling을 여러 방법으로 시도하고, 전 날 종가로 나누는 div-close 방법을 사용하여 총 4가지 스케일링 성능을 비교해보도록 하겠다.\n\n\n\n\n스케일러 비교\n\n\n\n스케일러 선택\n\n\n\n\n\n\nimport pandas as pd\nimport numpy as np\nfrom tqdm import tqdm\nimport FinanceDataReader as fdr\nimport pymysql\n\nimport warnings\nwarnings.filterwarnings('ignore')\n    \nimport ta\n\nfrom ipywidgets import interact, interact_manual\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.graph_objects as go\nimport plotly.offline as pyo\n%matplotlib inline\n%pylab inline\npylab.rcParams['figure.figsize'] = (12,5)\n\n\n# 자주쓰는 함수들은 모듈화 함 \nimport StockFunc as sf \n\nPopulating the interactive namespace from numpy and matplotlib\n\n\n \n\n\n\n\n\n\n모든 컬럼을 독립적으로 종목 별 전체데이터에 대해 min-max scaling 한다.\n\ndef make_dataset_minmax(trading):\n    from sklearn.preprocessing import MinMaxScaler\n    # 종목코드 불러오기 \n    IF = open('../data/code_list.txt')\n    lst_code = IF.readlines()\n\n    lst_X = []\n    lst_Y = []\n    lst_code_date = []\n    \n    db_dsml = pymysql.connect(\n        host = 'localhost', \n        port = 3306, \n        user = '[db username]', \n        passwd = '[db password]', \n        db = '[db name]', \n        charset = 'utf8'\n    )\n    cursor = db_dsml.cursor()\n    \n    \n    for code in tqdm(lst_code): \n        code = code.strip()\n        \n        sql_query = '''\n                    SELECT *\n                    FROM stock_{0}\n                    WHERE Date BETWEEN '2017-01-01' AND '2021-12-31'\n                    '''.format(code)\n\n        \n        stock = pd.read_sql(sql = sql_query, con = db_dsml) \n        stock['trading_value'] = stock['Close'] * stock['Volume']\n        \n        lst_stock = stock.values.tolist()\n\n        \n        # 🌟 scaling \n        df_temp = stock.drop(columns=['Date', 'Change', 'Next Change'])\n        scaler = MinMaxScaler()\n        scaled = scaler.fit_transform(df_temp)\n        \n        lst_stock_scaled = scaled.tolist()\n         \n        \n        for idx, row in enumerate(lst_stock): \n            date, trading_value = row[0].date().strftime(\"%Y-%m-%d\"), row[-1]\n            if trading_value >= trading:\n                if (idx < 9) or (idx >= len(lst_stock)-1): # 예외 처리 \n                    continue \n                \n                # D-9 ~ D0 데이터만 담기 \n                sub_stock = lst_stock_scaled[idx-9:idx+1] \n\n                # 10일간의 데이터 \n                lst_result = []\n                for row2 in sub_stock:\n                    lst_result += row2\n\n                # D+1 종가 2% 상승 여부 \n                label = int(row[7] >= 0.02)\n                \n                # 종속변수, 독립변수, 종목코드, 날짜 리스트에 추가 \n                lst_X.append(lst_result)\n                lst_Y.append(label)\n                lst_code_date.append([code, date])\n            \n    return pd.concat([pd.DataFrame(lst_code_date), pd.DataFrame(lst_X), pd.DataFrame(lst_Y)], axis=1)\n\n\ndf_dataset = make_dataset_minmax(trading=1000000000)\n\n100%|██████████████████████████████████████████| 1561/1561 [03:37<00:00,  7.18it/s]\n\n\n\ntrainX_1, trainY_1, testX_1, testY_1, lst_code_date, lst_code_date_test = sf.split(df_dataset)\n\nprint('train dataset: ', trainX_1.shape, trainY_1.shape)\nprint('test dataset: ', testX_1.shape, testY_1.shape)\n\ntrain dataset:  (659095, 550) (659095,)\ntest dataset:  (239818, 550) (239818,)\n\n\n\nfrom xgboost import XGBClassifier\nxgb_1 = XGBClassifier(\n                   n_jobs=40,\n                   scale_pos_weight=4,\n                   learning_rate=0.01,\n                   max_depth=3,\n                   n_estimators=500,\n                   ) \n\nxgb_1.fit(trainX_1, trainY_1)\n\n[15:05:37] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n\n\nXGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n              gamma=0, gpu_id=-1, importance_type=None,\n              interaction_constraints='', learning_rate=0.01, max_delta_step=0,\n              max_depth=3, min_child_weight=1, missing=nan,\n              monotone_constraints='()', n_estimators=500, n_jobs=40,\n              num_parallel_tree=1, predictor='auto', random_state=0,\n              reg_alpha=0, reg_lambda=1, scale_pos_weight=4, subsample=1,\n              tree_method='exact', validate_parameters=1, verbosity=None)\n\n\n\nsf.plot_evauate(trainX_1, trainY_1, testX_1, testY_1, xgb_1)\n\n\n\n\n\n\n\nROCAUC 점수가 지난 0.595의 성능 보다는 향상된 0.613의 성능을 보여주었다.\n \n\n\n\n모든 컬럼을 독립적으로 종목 별 10일치에 대해 min-max scaling 한다.\n\ndef make_dataset_minmax_window(trading):\n    from sklearn.preprocessing import MinMaxScaler\n    # 종목코드 불러오기 \n    IF = open('../data/code_list.txt')\n    lst_code = IF.readlines()\n\n    lst_X = []\n    lst_Y = []\n    lst_code_date = []\n    \n    db_dsml = pymysql.connect(\n        host = 'localhost', \n        port = 3306, \n        user = '[db username]', \n        passwd = '[db password]', \n        db = '[db name]', \n        charset = 'utf8'\n    )\n    cursor = db_dsml.cursor()\n    \n    \n    for code in tqdm(lst_code): \n        code = code.strip()\n        \n        sql_query = '''\n                    SELECT *\n                    FROM stock_{0}\n                    WHERE Date BETWEEN '2017-01-01' AND '2021-12-31'\n                    '''.format(code)\n\n        \n        stock = pd.read_sql(sql = sql_query, con = db_dsml) \n        stock['trading_value'] = stock['Close'] * stock['Volume']\n        \n        lst_stock = stock.values.tolist()\n\n        \n        for idx, row in enumerate(lst_stock): \n            date, trading_value = row[0].date().strftime(\"%Y-%m-%d\"), row[-1]\n            if trading_value >= trading:\n                if (idx < 9) or (idx >= len(lst_stock)-1): # 예외 처리 \n                    continue \n                \n                # D-9 ~ D0 데이터만 담기 \n                arr_sub_stock = np.array(lst_stock[idx-9:idx+1])\n\n                # 🌟 scaling \n                arr_temp = np.concatenate((arr_sub_stock[:, 1:6], arr_sub_stock[:, 8:]), axis=1) \n                scaler = MinMaxScaler()\n                scaled = scaler.fit_transform(np.array(arr_temp))\n\n                lst_sub_stock_scaled = scaled.tolist()\n                \n                # 10일간의 데이터 \n                lst_result = []\n                for row2 in lst_sub_stock_scaled:\n                    lst_result += row2\n\n                # D+1 종가 2% 상승 여부 \n                label = int(row[7] >= 0.02)\n                \n                # 종속변수, 독립변수, 종목코드, 날짜 리스트에 추가 \n                lst_X.append(lst_result)\n                lst_Y.append(label)\n                lst_code_date.append([code, date])\n            \n    return pd.concat([pd.DataFrame(lst_code_date), pd.DataFrame(lst_X), pd.DataFrame(lst_Y)], axis=1)\n\n\ndf_dataset = make_dataset_minmax_window(trading=1000000000)\n\n100%|██████████████████████████████████████████| 1561/1561 [05:53<00:00,  4.41it/s]\n\n\n\ntrainX_2, trainY_2, testX_2, testY_2, lst_code_date, lst_code_date_test = sf.split(df_dataset)\n\nprint('train dataset: ', trainX_2.shape, trainY_2.shape)\nprint('test dataset: ', testX_2.shape, testY_2.shape)\n\ntrain dataset:  (659095, 550) (659095,)\ntest dataset:  (239818, 550) (239818,)\n\n\n\nfrom xgboost import XGBClassifier\nxgb_2 = XGBClassifier(\n                   n_jobs=40,\n                   scale_pos_weight=4,\n                   learning_rate=0.01,\n                   max_depth=3,\n                   n_estimators=500,\n                   ) \n\nxgb_2.fit(trainX_2, trainY_2)\n\n[16:05:07] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n\n\nXGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n              gamma=0, gpu_id=-1, importance_type=None,\n              interaction_constraints='', learning_rate=0.01, max_delta_step=0,\n              max_depth=3, min_child_weight=1, missing=nan,\n              monotone_constraints='()', n_estimators=500, n_jobs=40,\n              num_parallel_tree=1, predictor='auto', random_state=0,\n              reg_alpha=0, reg_lambda=1, scale_pos_weight=4, subsample=1,\n              tree_method='exact', validate_parameters=1, verbosity=None)\n\n\n\nsf.plot_evauate(trainX_2, trainY_2, testX_2, testY_2, xgb_2)\n\n\n\n\n\n\n\n지난 글에서 0.595 였던 ROCAUC 성능이 0.575로 하락한 것을 보였다.\n \n\n\n\n\n가격 관련 컬럼: 캔들차트의 최솟값, 최댓값으로 min-max scaling\n나머지 컬럼: scaling 하지 않는다.\n\n\ndef make_dataset_minmax_price(trading):\n    from sklearn.preprocessing import MinMaxScaler\n    \n    col_price = ['Open', 'High', 'Low', 'Close', 'MA5', 'MA20', 'MA60', 'MA120', \n               'VMAP', 'BHB', 'BLB', 'KCH', 'KCL', 'KCM', 'DCH', 'DCL', 'DCM',\n               'SMA', 'EMA', 'WMA', 'Ichimoku', 'Parabolic SAR', 'KAMA','MACD']   \n\n    col_etc = ['Volume', 'MFI', 'ADI', 'OBV',\n           'CMF', 'FI', 'EOM, EMV', 'VPT', 'NVI', 'ATR', 'UI',\n           'ADX', '-VI', '+VI', 'TRIX', 'MI', 'CCI', 'DPO', 'KST',\n           'STC', 'RSI', 'SRSI', 'TSI', 'UO', 'SR',\n           'WR', 'AO', 'ROC', 'PPO', 'PVO', 'trading_value']\n    \n    # 종목코드 불러오기 \n    IF = open('../data/code_list.txt')\n    lst_code = IF.readlines()\n\n    lst_X = []\n    lst_Y = []\n    lst_code_date = []\n    \n    db_dsml = pymysql.connect(\n        host = 'localhost', \n        port = 3306, \n        user = '[db username]', \n        passwd = '[db password]', \n        db = '[db name]', \n        charset = 'utf8'\n    )\n    cursor = db_dsml.cursor()\n    \n    \n    for code in tqdm(lst_code): \n        code = code.strip()\n        \n        sql_query = '''\n                    SELECT *\n                    FROM stock_{0}\n                    WHERE Date BETWEEN '2017-01-01' AND '2021-12-31'\n                    '''.format(code)\n\n        \n        stock = pd.read_sql(sql = sql_query, con = db_dsml) \n        stock['trading_value'] = stock['Close'] * stock['Volume']\n        \n        lst_stock = stock.values.tolist()\n\n        \n        # 🌟 scaling\n        # 1) 가격 관련 컬럼 \n        df_price = stock[col_price]\n        minimum = df_price['Low'].min()\n        maximum = df_price['High'].max()\n        df_price_scaled = df_price.apply(lambda x: (x-minimum) / (maximum-minimum))\n        \n        # 2) 나머지 컬럼 \n        df_etc = stock[col_etc]\n        \n        df_scaled = pd.concat([df_price_scaled, df_etc_scaled], axis=1)\n        \n        lst_stock_scaled = df_scaled.values.tolist()\n         \n            \n        for idx, row in enumerate(lst_stock): \n            date, trading_value = row[0].date().strftime(\"%Y-%m-%d\"), row[-1]\n            if trading_value >= trading:\n                if (idx < 9) or (idx >= len(lst_stock)-1): # 예외 처리 \n                    continue \n                \n                # D-9 ~ D0 데이터만 담기 \n                sub_stock = lst_stock_scaled[idx-9:idx+1] \n\n                # 10일간의 데이터 \n                lst_result = []\n                for row2 in sub_stock:\n                    lst_result += row2\n\n                # D+1 종가 2% 상승 여부 \n                label = int(row[7] >= 0.02)\n                \n                # 종속변수, 독립변수, 종목코드, 날짜 리스트에 추가 \n                lst_X.append(lst_result)\n                lst_Y.append(label)\n                lst_code_date.append([code, date])\n            \n    return pd.concat([pd.DataFrame(lst_code_date), pd.DataFrame(lst_X), pd.DataFrame(lst_Y)], axis=1)\n\n\ndf_dataset = make_dataset_minmax_price(trading=1000000000)\n\n100%|██████████████████████████████████████████| 1561/1561 [03:53<00:00,  6.68it/s]\n\n\n\ntrainX_3, trainY_3, testX_3, testY_3, lst_code_date, lst_code_date_test = sf.split(df_dataset)\n\nprint('train dataset: ', trainX_3.shape, trainY_3.shape)\nprint('test dataset: ', testX_3.shape, testY_3.shape)\n\ntrain dataset:  (659095, 550) (659095,)\ntest dataset:  (239818, 550) (239818,)\n\n\n\nfrom xgboost import XGBClassifier\nxgb_3 = XGBClassifier(\n                   n_jobs=40,\n                   scale_pos_weight=4,\n                   learning_rate=0.01,\n                   max_depth=3,\n                   n_estimators=500,\n                   ) \n\nxgb_3.fit(trainX_3, trainY_3)\n\n[16:15:14] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n\n\nXGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n              gamma=0, gpu_id=-1, importance_type=None,\n              interaction_constraints='', learning_rate=0.01, max_delta_step=0,\n              max_depth=3, min_child_weight=1, missing=nan,\n              monotone_constraints='()', n_estimators=500, n_jobs=40,\n              num_parallel_tree=1, predictor='auto', random_state=0,\n              reg_alpha=0, reg_lambda=1, scale_pos_weight=4, subsample=1,\n              tree_method='exact', validate_parameters=1, verbosity=None)\n\n\n\nsf.plot_evauate(trainX_3, trainY_3, testX_3, testY_3, xgb_3)\n\n\n\n\n\n\n\n지난 글의 0.595 보다 향상된 성능을 보여주었다.\n \n\n\n\n\n가격 관련 컬럼: 전날 종가로 나누어 스케일링\n나머지 컬럼: 스케일링 하지 않는다.\n\n\ndef make_dataset_div_previous_close(trading):\n    from sklearn.preprocessing import MinMaxScaler\n    \n    col_price = ['Open', 'High', 'Low', 'Close', 'MA5', 'MA20', 'MA60', 'MA120', \n               'VMAP', 'BHB', 'BLB', 'KCH', 'KCL', 'KCM', 'DCH', 'DCL', 'DCM',\n               'SMA', 'EMA', 'WMA', 'Ichimoku', 'Parabolic SAR', 'KAMA','MACD']   \n\n    col_etc = ['Volume', 'MFI', 'ADI', 'OBV',\n           'CMF', 'FI', 'EOM, EMV', 'VPT', 'NVI', 'ATR', 'UI',\n           'ADX', '-VI', '+VI', 'TRIX', 'MI', 'CCI', 'DPO', 'KST',\n           'STC', 'RSI', 'SRSI', 'TSI', 'UO', 'SR',\n           'WR', 'AO', 'ROC', 'PPO', 'PVO', 'trading_value']\n    \n    # 종목코드 불러오기 \n    IF = open('../data/code_list.txt')\n    lst_code = IF.readlines()\n\n    lst_X = []\n    lst_Y = []\n    lst_code_date = []\n    \n    db_dsml = pymysql.connect(\n        host = 'localhost', \n        port = 3306, \n        user = '[db username]', \n        passwd = '[db password]', \n        db = '[db name]', \n        charset = 'utf8'\n    )\n    cursor = db_dsml.cursor()\n    \n    \n    for code in tqdm(lst_code): \n        code = code.strip()\n        \n        sql_query = '''\n                    SELECT *\n                    FROM stock_{0}\n                    WHERE Date BETWEEN '2017-01-01' AND '2021-12-31'\n                    '''.format(code)\n\n        \n        stock = pd.read_sql(sql = sql_query, con = db_dsml) \n        stock['PrevClose'] = stock['Close'].shift(1) # 전 날 종가 컬럼 추가\n        stock.dropna(inplace=True)\n        stock = stock.reset_index(drop=True)\n        stock['trading_value'] = stock['Close'] * stock['Volume']\n        lst_stock = stock.values.tolist()\n\n        \n        # 🌟 scaling\n        # 1) 가격 관련 컬럼 \n        df_price = stock[col_price]\n        df_price_scaled = df_price.apply(lambda x: x / stock['PrevClose'])\n        \n        # 2) 나머지 컬럼 \n        df_etc = stock[col_etc]\n        \n        df_scaled = pd.concat([df_price_scaled, df_etc], axis=1)\n        lst_stock_scaled = df_scaled.values.tolist()\n\n        \n        for idx, row in enumerate(lst_stock): \n            date, trading_value = row[0].date().strftime(\"%Y-%m-%d\"), row[-1]\n            if trading_value >= trading:\n                if (idx < 9) or (idx >= len(lst_stock)-1): # 예외 처리 \n                    continue \n                \n                # D-9 ~ D0 데이터만 담기 \n                sub_stock = lst_stock_scaled[idx-9:idx+1] \n\n                # 10일간의 데이터 \n                lst_result = []\n                for row2 in sub_stock:               \n                    lst_result += row2\n\n                # D+1 종가 2% 상승 여부 \n                label = int(row[7] >= 0.02)\n                \n                # 종속변수, 독립변수, 종목코드, 날짜 리스트에 추가 \n                lst_X.append(lst_result)\n                lst_Y.append(label)\n                lst_code_date.append([code, date])\n            \n    return pd.concat([pd.DataFrame(lst_code_date), pd.DataFrame(lst_X), pd.DataFrame(lst_Y)], axis=1)\n\n\ndf_dataset = make_dataset_div_previous_close(trading=1000000000)\n\n100%|██████████████████████████████████████████| 1561/1561 [03:53<00:00,  6.70it/s]\n\n\n\ntrainX_4, trainY_4, testX_4, testY_4, lst_code_date, lst_code_date_test = sf.split(df_dataset)\n\nprint('train dataset: ', trainX_4.shape, trainY_4.shape)\nprint('test dataset: ', testX_4.shape, testY_4.shape)\n\ntrain dataset:  (658364, 550) (658364,)\ntest dataset:  (239817, 550) (239817,)\n\n\n\nfrom xgboost import XGBClassifier\nxgb_4 = XGBClassifier(\n                   n_jobs=40,\n                   scale_pos_weight=4,\n                   learning_rate=0.01,\n                   max_depth=3,\n                   n_estimators=500,\n                   ) \n\nxgb_4.fit(trainX_4, trainY_4)\n\n[16:26:12] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n\n\nXGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n              gamma=0, gpu_id=-1, importance_type=None,\n              interaction_constraints='', learning_rate=0.01, max_delta_step=0,\n              max_depth=3, min_child_weight=1, missing=nan,\n              monotone_constraints='()', n_estimators=500, n_jobs=40,\n              num_parallel_tree=1, predictor='auto', random_state=0,\n              reg_alpha=0, reg_lambda=1, scale_pos_weight=4, subsample=1,\n              tree_method='exact', validate_parameters=1, verbosity=None)\n\n\n\nsf.plot_evauate(trainX_4, trainY_4, testX_4, testY_4, xgb_4)\n\n\n\n\n\n\n\n지난 글 0.595 보다 성능 향상이 된 것을 확인하였다.\n \n\n\n\n\n스케일링을 통해 이전보다 크게 성능 향상이 되었다. 서로 다른 종목 별 가격의 범위를 조정해 주는 작업은 중요하다는 것을 확인하였다. 수익률 시뮬레이션까지 해 본 후, 네 가지 방법 중 지속적으로 사용할 스케일러를 선택하도록 한다.\n\n\n\nsf.compute_earnings_rate(lst_code_date_test, xgb_1, testX_1, threshold=0.75)\n\n100%|████████████████████████████████████████████| 202/202 [00:04<00:00, 42.12it/s]\n\n\nFinal earning rate : 462.37913 %\n\n\n\n\n\n\nsf.compute_earnings_rate(lst_code_date_test, xgb_2, testX_2, threshold=0.75)\n\n0it [00:00, ?it/s]\n\n\nFinal earning rate : 0.0 %\n\n\n\n\n\n\nsf.compute_earnings_rate(lst_code_date_test, xgb_3, testX_3, threshold=0.75)\n\n100%|████████████████████████████████████████████| 222/222 [00:05<00:00, 43.23it/s]\n\n\nFinal earning rate : 124.50311 %\n\n\n\n\n\n\nsf.compute_earnings_rate(lst_code_date_test, xgb_4, testX_4, threshold=0.75)\n\n100%|████████████████████████████████████████████| 260/260 [00:06<00:00, 42.78it/s]\n\n\nFinal earning rate : 1487.42557 %\n\n\n\n\n\n\n예측확률 임계값을 0.75로 설정하였는데, 4) 전 날 종가로 나누기 방법의 스케일링이 가장 매매 개수가 많았다. 1년 동안 130번의 매수가 일어났음을 알 수 있다. 그와 동시에 수익률 시뮬레이션에서 수익률이 가장 높게 나온 전 날 종가로 나누는 스케일링 방법을 최종적으로 선택한다."
  },
  {
    "objectID": "posts/05.filtering.html",
    "href": "posts/05.filtering.html",
    "title": "[stock prediction] 2.3 CCI를 이용한 주가 데이터 필터링",
    "section": "",
    "text": "이전글 2.2. 주가 데이터 스케일링 에서는 통합 종목 주가 데이터셋의 문제점인 종목마다 다른 가격을 표준화해주는 작업을 했다. 마지막 데이터 전처리 과정은 CCI를 이용한 주가 데이터 필터링이다. 매일 쏟아져 나오는 방대한 주가 데이터셋으로부터 특정 패턴을 검출해내는 것은 어려운 과제다. 본 연구에서는 그러한 문제점을 최소화하고자, CCI 구간별로 데이터를 필터링 하는 방법을 사용한다. 이번 글에서는 CCI가 무엇인지 알아보고, CCI를 3가지 구간으로 나누어 필터링 한 데이터로 XGBoost 모델학습을 진행한다.\n\n\n\n\n\nCCI란?\n\n\nCCI를 활용한 주가데이터 필터링\n\n\n\n\nimport pandas as pd\nimport numpy as np\nfrom tqdm import tqdm\nimport pymysql\n\nimport warnings\nwarnings.filterwarnings('ignore')\n    \nimport ta\n\nfrom ipywidgets import interact, interact_manual\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.graph_objects as go\nimport plotly.offline as pyo\n%matplotlib inline\n%pylab inline\npylab.rcParams['figure.figsize'] = (12,5)\n\nimport StockFunc as sf\n\n%pylab is deprecated, use %matplotlib inline and import the required libraries.\nPopulating the interactive namespace from numpy and matplotlib\n\n\n \n\n\n\n\nCCI (상품 채널 지수: Commodity Chnnel Index) 는 일정 기간 동안 주가의 평균값에서 얼마나 떨어졌는지를 보여주는 추세 지표이며, 추세의 방향과 강도를 알 수 있다. 추세의 강도만을 알려주는 ADX에 비해 추세의 방향까지 동시에 알려주기 때문에 추세 추종형 거래자들에게 유용한 지표로 사용되고 있고, 추세지표지만 변동성 지표처럼 사용되기도 한다.\n\n계산식\n\n\n해석 방법\n\nCCI 값이 0이면 현재 주가가 이동 평균선과 일치한다는 뜻\nCCI 값이 +이면 상승추세, -이면 하락추세이다.\nCCI 값이 클수록 추세의 강도가 강하고, 작을수록 추세의 강도가 약하다.\nCCI의 절댓값이 100 이상으로 크다는 것은 가격이 급변하였다는 것을 의미한다. (과매수/과매도 구간)\n\n\n[참고]\n-[주식투자] Chart 보조 지표 CCI 란?\n-주식용어 독파 [25] CCI의 의미와 실전 활용법\n \n\n\n\n본 연구에서는 앞서 살펴본 보조지표 CCI의 범위를 3가지 구간으로 나누어 데이터를 필터링 합니다. 이 때, 이동평균선으로부터 근접한 1) 중립구간, 극단적으로 떨어진 2) 과열구간/과매수구간 과 3) 침체 구간/과매도구간으로 나눕니다.\n나누는 CCI의 구체적인 구간은 다음과 같다.\n1) 중립구간 - CCI : (-20, 20)\n2) 과열구간 - CCI : (100, \\(\\infty\\))\n3) 침체 구간 - CCI : (-\\(\\infty\\),-100)\n\n스케일링 (div-prev_close) + CCI 구간 필터링 데이터셋 생성 함수\n\n\ndef make_dataset_cci(trading, cci_d='_', cci_u='_', train=True):\n    from sklearn.preprocessing import MinMaxScaler\n    \n    col_price = ['Open', 'High', 'Low', 'Close', 'trading_value', 'MA5', 'MA20', 'MA60', 'MA120', \n               'VMAP', 'BHB', 'BLB', 'KCH', 'KCL', 'KCM', 'DCH', 'DCL', 'DCM',\n               'SMA', 'EMA', 'WMA', 'Ichimoku', 'Parabolic SAR', 'KAMA','MACD']   \n\n    col_etc = ['Change', 'Volume', 'MFI', 'ADI', 'OBV',\n           'CMF', 'FI', 'EOM, EMV', 'VPT', 'NVI', 'ATR', 'UI',\n           'ADX', '-VI', '+VI', 'TRIX', 'MI', 'CCI', 'DPO', 'KST',\n           'STC', 'RSI', 'SRSI', 'TSI', 'UO', 'SR',\n           'WR', 'AO', 'ROC', 'PPO', 'PVO']\n    \n    # 종목코드 불러오기 \n    IF = open('../data/code_list.txt')\n    lst_code = IF.readlines()\n\n    lst_X = []\n    lst_Y = []\n    lst_code_date = []\n        \n    db_dsml = pymysql.connect(\n        host = 'localhost', \n        port = 3306, \n        user = '[db username]', \n        passwd = '[db password]', \n        db = '[db name]', \n        charset = 'utf8'\n    )\n    cursor = db_dsml.cursor()\n    \n    temp_cci_d, temp_cci_u = cci_d, cci_u # 🌟 초기 cci 범위 저장 \n    \n    for code in tqdm(lst_code): \n        code = code.strip()\n        \n        if train:\n            sql_query = '''\n                        SELECT *\n                        FROM stock_{}\n                        WHERE Date BETWEEN '2017-01-01' AND '2020-12-31'\n                        '''.format(code)\n        else: \n            sql_query = '''\n                        SELECT *\n                        FROM stock_{}\n                        WHERE Date BETWEEN '2021-01-01' AND '2021-12-31'\n                        '''.format(code)\n            \n        \n        stock = pd.read_sql(sql = sql_query, con = db_dsml) \n        stock['PrevClose'] = stock['Close'].shift(1) # 전 날 종가 컬럼 추가\n        stock.dropna(inplace=True)\n        stock = stock.reset_index(drop=True)\n        stock['trading_value'] = stock['Close'] * stock['Volume']\n        lst_stock = stock.values.tolist()\n\n        if temp_cci_d == '_': \n            cci_d = stock['CCI'].min()\n        if temp_cci_u == '_':\n            cci_u = stock['CCI'].max()    \n        \n        # scaling\n        # 1) 가격 관련 컬럼 \n        df_price = stock[col_price]\n        df_price_scaled = df_price.apply(lambda x: x / stock['PrevClose'])\n        \n        # 2) 나머지 컬럼 \n        df_etc = stock[col_etc]\n        \n        df_scaled = pd.concat([df_price_scaled, df_etc], axis=1)\n        lst_stock_scaled = df_scaled.values.tolist()\n\n        \n        for idx, row in enumerate(lst_stock): \n            date, trading_value, cci_ = row[0].date().strftime(\"%Y-%m-%d\"), row[-1], row[40]\n            # 🌟 cci 조건 설정 \n            if (trading_value >= trading) & ((cci_ >= cci_d) & (cci_ <= cci_u)):\n                if (idx < 9): # 예외 처리 \n                    continue \n                \n                # D-9 ~ D0 데이터만 담기 \n                sub_stock = lst_stock_scaled[idx-9:idx+1] \n\n                # 10일간의 데이터 \n                lst_result = []\n                for row2 in sub_stock:               \n                    lst_result += row2\n\n                # D+1 종가 2% 상승 여부 \n                label = int(row[7] >= 0.02)\n                \n                # 종속변수, 독립변수, 종목코드, 날짜 리스트에 추가 \n                lst_X.append(lst_result)\n                lst_Y.append(label)\n                lst_code_date.append([code, date])\n            \n    return np.array(lst_X), np.array(lst_Y), np.array(lst_code_date) \n\n\n\n\n중립구간에서 데이터셋의 개수는 train: 123321, test:36885 개로, 세 구간 중 가장 적은 데이터셋의 개수가 나왔다. 정확도는 가장 높지만, auc score는 미미한 결과를 보였다.\n\ntrainX_1, trainY_1, lst_code_date_1 = make_dataset_cci(trading=100000000, cci_d=-20, cci_u=20, train=True)\ntestX_1, testY_1, lst_code_date_test_1 = make_dataset_cci(trading=100000000, cci_d=-20, cci_u=20, train=False)\n\n100%|███████████████████████████████████████████████████████████████████████| 1561/1561 [02:12<00:00, 11.77it/s]\n100%|███████████████████████████████████████████████████████████████████████| 1561/1561 [00:42<00:00, 36.60it/s]\n\n\n\nprint('train dataset: ', trainX_1.shape, trainY_1.shape)\nprint('test dataset: ', testX_1.shape, testY_1.shape)\n\ntrain dataset:  (123321, 560) (123321,)\ntest dataset:  (36885, 560) (36885,)\n\n\n\nfrom xgboost import XGBClassifier\nxgb_1 = XGBClassifier(\n                   n_jobs=40,\n                   scale_pos_weight=4,\n                   learning_rate=0.01,\n                   max_depth=3,\n                   n_estimators=800,\n                   ) \n\nxgb_1.fit(trainX_1, trainY_1)\n\nXGBClassifier(base_score=0.5, booster='gbtree', callbacks=None,\n              colsample_bylevel=1, colsample_bynode=1, colsample_bytree=1,\n              early_stopping_rounds=None, enable_categorical=False,\n              eval_metric=None, gamma=0, gpu_id=-1, grow_policy='depthwise',\n              importance_type=None, interaction_constraints='',\n              learning_rate=0.01, max_bin=256, max_cat_to_onehot=4,\n              max_delta_step=0, max_depth=3, max_leaves=0, min_child_weight=1,\n              missing=nan, monotone_constraints='()', n_estimators=800,\n              n_jobs=40, num_parallel_tree=1, predictor='auto', random_state=0,\n              reg_alpha=0, reg_lambda=1, ...)\n\n\n\nsf.plot_evauate(trainX_1, trainY_1, testX_1, testY_1, xgb_1)\n\n\n\n\n\n\n\n\n\n\n\n침체구간에서 데이터셋의 개수는 train: 256399, test:64768 개로, 꽤 많은 데이터셋의 개수가 나왔다. 세 구간 중 가장 낮은 분류 성능을 보였다.\n\ntrainX_2, trainY_2, lst_code_date_2 = make_dataset_cci(trading=100000000, cci_d=100, train=True)\ntestX_2, testY_2, lst_code_date_test_2 = make_dataset_cci(trading=100000000, cci_d=100, train=False)\n\n100%|██████████████████████████████████████████████████████████████████████████████| 1561/1561 [02:16<00:00, 11.44it/s]\n100%|██████████████████████████████████████████████████████████████████████████████| 1561/1561 [00:42<00:00, 36.67it/s]\n\n\n\nprint('train dataset: ', trainX_2.shape, trainY_2.shape)\nprint('test dataset: ', testX_2.shape, testY_2.shape)\n\ntrain dataset:  (256399, 560) (256399,)\ntest dataset:  (64768, 560) (64768,)\n\n\n\nfrom xgboost import XGBClassifier\nxgb_2 = XGBClassifier(\n                   n_jobs=40,\n                   scale_pos_weight=4,\n                   learning_rate=0.01,\n                   max_depth=3,\n                   n_estimators=800,\n                   ) \n\nxgb_2.fit(trainX_2, trainY_2)\n\nXGBClassifier(base_score=0.5, booster='gbtree', callbacks=None,\n              colsample_bylevel=1, colsample_bynode=1, colsample_bytree=1,\n              early_stopping_rounds=None, enable_categorical=False,\n              eval_metric=None, gamma=0, gpu_id=-1, grow_policy='depthwise',\n              importance_type=None, interaction_constraints='',\n              learning_rate=0.01, max_bin=256, max_cat_to_onehot=4,\n              max_delta_step=0, max_depth=3, max_leaves=0, min_child_weight=1,\n              missing=nan, monotone_constraints='()', n_estimators=800,\n              n_jobs=40, num_parallel_tree=1, predictor='auto', random_state=0,\n              reg_alpha=0, reg_lambda=1, ...)\n\n\n\nsf.plot_evauate(trainX_2, trainY_2, testX_2, testY_2, xgb_2)\n\n\n\n\n\n\n\n\n\n\n\n침체구간에서 데이터셋의 개수는 train: 269977, test:73435 개로 가장 많은 데이터의 개수가 나왔으며, auc score를 봤을 때, 가장 높은 분류 성능을 보였다.\n\ntrainX_3, trainY_3, lst_code_date_3 = make_dataset_cci(trading=100000000, cci_u=-100, train=True)\ntestX_3, testY_3, lst_code_date_test_3 = make_dataset_cci(trading=100000000, cci_u=-100, train=False)\n\n100%|██████████████████████████████████████████████████████████████████████████████| 1561/1561 [02:16<00:00, 11.45it/s]\n100%|██████████████████████████████████████████████████████████████████████████████| 1561/1561 [00:43<00:00, 36.16it/s]\n\n\n\nprint('train dataset: ', trainX_3.shape, trainY_3.shape)\nprint('test dataset: ', testX_3.shape, testY_3.shape)\n\ntrain dataset:  (269977, 560) (269977,)\ntest dataset:  (73435, 560) (73435,)\n\n\n\nfrom xgboost import XGBClassifier\nxgb_3 = XGBClassifier(\n                   n_jobs=40,\n                   scale_pos_weight=4,\n                   learning_rate=0.01,\n                   max_depth=3,\n                   n_estimators=800,\n                   ) \n\nxgb_3.fit(trainX_3, trainY_3)\n\nXGBClassifier(base_score=0.5, booster='gbtree', callbacks=None,\n              colsample_bylevel=1, colsample_bynode=1, colsample_bytree=1,\n              early_stopping_rounds=None, enable_categorical=False,\n              eval_metric=None, gamma=0, gpu_id=-1, grow_policy='depthwise',\n              importance_type=None, interaction_constraints='',\n              learning_rate=0.01, max_bin=256, max_cat_to_onehot=4,\n              max_delta_step=0, max_depth=3, max_leaves=0, min_child_weight=1,\n              missing=nan, monotone_constraints='()', n_estimators=800,\n              n_jobs=40, num_parallel_tree=1, predictor='auto', random_state=0,\n              reg_alpha=0, reg_lambda=1, ...)\n\n\n\nsf.plot_evauate(trainX_3, trainY_3, testX_3, testY_3, xgb_3)\n\n\n\n\n\n\n\n\n데이터셋 저장\n\n각 학습, 시험 데이터셋과 XGBoost 모델을 pickle 파일로 저장한다.\n\n#collapse-hide\nimport pickle \ndic_dataset_model = {'CCI -20~20': [trainX_1, trainY_1, testX_1, testY_1, lst_code_date_1, lst_code_date_test_1, xgb_1],\n                'CCI 100~': [trainX_2, trainY_2, testX_2, testY_2, lst_code_date_2, lst_code_date_test_2, xgb_2],\n                'CCI ~-100': [trainX_3, trainY_3, testX_3, testY_3, lst_code_date_3, lst_code_date_test_3, xgb_3]}\n\nwith open('dataset_cci_filtering.pickle', 'wb') as handle:\n    pickle.dump(dic_dataset_model, handle, protocol=pickle.HIGHEST_PROTOCOL)\n\n\n다음 글에서는 위에서 CCI 구간별로 학습한 XGBoost 모델에 XAI 기법 중 하나인 SHAP value를 적용하여 데이터를 변환하고, 각 구간마다 summary plot을 그려보며 중요한 특성과 그에 대한 해석을 진행하도록 한다."
  },
  {
    "objectID": "posts/06.shap_value.html",
    "href": "posts/06.shap_value.html",
    "title": "[stock prediction] 3.1. 설명가능 AI (XAI), SHAP value",
    "section": "",
    "text": "이전 글 ( [Stock Research] 2.3. CCI를 이용한 주가 데이터 필터링) 까지 데이터 전저리를 통해 baseline model에 비해 성능을 향상시킬 수 있었다. 이번 글에서는 설명가능 AI 기법중에서도 SHAP value가 무엇인지 알아보고, 앞서 학습했던 CCI 구간 별 XGBoost 모델을 사용하여 SHAP value를 계산함으로써 주가 이진분류 예측에 영향을 미친 중요 변수를 해석해 본다. 여기서 summary plot의 결과로 나온 상위 중요 변수로 데이터를 필터링하여 특정 집단을 구성하고, 공통된 특징이 있는지 확인한다.\n\n\n\n\n설명가능 AI (XAI), SHAP value란?\n\n\nCCI 구간 별 summary plot 비교\n\n\n\n\n라이브러리 import\n\n\nimport pandas as pd\nimport numpy as np\nfrom tqdm import tqdm\nimport pymysql\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom ipywidgets import interact, interact_manual\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.graph_objects as go\nimport plotly.offline as pyo\n%matplotlib inline\n%pylab inline\npylab.rcParams['figure.figsize'] = (12,5)\n\nimport StockFunc as sf\n\n%pylab is deprecated, use %matplotlib inline and import the required libraries.\nPopulating the interactive namespace from numpy and matplotlib\n\n\n \n\n\n\n\n\n\n오늘날 인공지능의 성능이 좋아짐에 따라 모델의 복잡도 또한 높아지게 되었다. 여기서 예측 결과에 대한 모델의 신뢰성 문제가 따라오게된다. 설명가능 AI는 이러한 문제점을 해결하여 인공지능 알고리즘으로 작성된 결과와 출력을 인간인 사용자가 이해하고 신뢰할 수 있도록 해준다.\n본 프로젝트에서는 설명가능 AI 기법 중에서 shap value를 활용하여 변수 중요도를 파악하고, 변환된 SHAP 표준화 데이터셋을 이용한 연구를 진행한다. SHAP(SHAPley Additional Descriptions)는 모든 기계 학습 모델의 출력을 설명하기 위한 게임 이론적 접근법이다. 게임 이론의 고전적인 shapley value를 계산하여 입력 변수와 모델의 결과값 사이의 관계를 분석하는 XAI 기법 중 하나이다.\n*shapley value\n-각 변수가 예측 결과물에 주는 영향력의 크기\n-해당 변수가 어떤 영향을 주는지 파악\n- (ex)\n- 각 선수가 팀 성적에 주는 영향력 크기\n- 해당 선수가 어떠한 영향을 주는가\n- (선수 A가 있는 팀 B의 승률) - (선수 A가 없는 팀 B의 승률) = 7%\n-> “선수 A는 팀 승률에 7% 만큼의 영향력이 있다.”\n\n[참고]\n-설명 가능한 AI\n-shap github\n-A Unified Approach to Interpreting Model Predictions\n \n\n\n\n\n\n데이터셋 불러오기\n\n\n#collapse-hide\nimport pickle\nwith open('dataset_cci_filtering.pickle', 'rb') as handle:\n    dic_dataset_model = pickle.load(handle)\n\n\n컬림 리스트 생성\n\n\n#collapse-hide\nlst_col_info = []\n\nlst_col = ['Open', 'High', 'Low', 'Close', 'trading_value', 'MA5', 'MA20', 'MA60', 'MA120', \n           'VMAP', 'BHB', 'BLB', 'KCH', 'KCL', 'KCM', 'DCH', 'DCL', 'DCM',\n           'SMA', 'EMA', 'WMA', 'Ichimoku', 'Parabolic SAR', 'KAMA','MACD']   +  ['Change', 'Volume', 'MFI', 'ADI', 'OBV', \n                                                                                   'CMF', 'FI', 'EOM, EMV', 'VPT', 'NVI', 'ATR', 'UI',\n                                                                                   'ADX', '-VI', '+VI', 'TRIX', 'MI', 'CCI', 'DPO', 'KST', \n                                                                                   'STC', 'RSI', 'SRSI', 'TSI', 'UO', 'SR',\n                                                                                   'WR', 'AO', 'ROC', 'PPO', 'PVO'] \n           \n\nfor day in range(9, -1, -1): \n    for col in lst_col: \n        lst_col_info.append(f'D-{day}_{col}')\n\n\n\n\n\n중립구간 데이터셋 불러오기\n\n\ntrainX_1, trainY_1, testX_1, testY_1, lst_code_date_1, lst_code_date_test_1, xgb_1 = dic_dataset_model['CCI -20~20']\nprint('train dataset: ', trainX_1.shape, trainY_1.shape)\nprint('test dataset: ', testX_1.shape, testY_1.shape)\n\ntrain dataset:  (123321, 560) (123321,)\ntest dataset:  (36885, 560) (36885,)\n\n\n\nnumpy -> DataFrame\n\n\n##### train\ndf_trainX_1 = pd.DataFrame(trainX_1)\ndf_trainX_1.columns = lst_col_info\ndf_trainY_1 = pd.DataFrame(trainY_1)\n\n##### test \ndf_testX_1 = pd.DataFrame(testX_1)\ndf_testX_1.columns = lst_col_info\ndf_testY_1 = pd.DataFrame(testY_1)\n\ndf_trainX_1.head()\n\n\n\n\n\n  \n    \n      \n      D-9_Open\n      D-9_High\n      D-9_Low\n      D-9_Close\n      D-9_trading_value\n      D-9_MA5\n      D-9_MA20\n      D-9_MA60\n      D-9_MA120\n      D-9_VMAP\n      ...\n      D-0_RSI\n      D-0_SRSI\n      D-0_TSI\n      D-0_UO\n      D-0_SR\n      D-0_WR\n      D-0_AO\n      D-0_ROC\n      D-0_PPO\n      D-0_PVO\n    \n  \n  \n    \n      0\n      1.005714\n      1.048000\n      1.000000\n      1.040000\n      462369.440000\n      0.994514\n      0.968971\n      0.981010\n      1.040524\n      0.993304\n      ...\n      52.767201\n      0.057077\n      20.137832\n      43.856662\n      46.236559\n      -53.763441\n      433.235294\n      2.564103\n      1.624561\n      -3.039166\n    \n    \n      1\n      1.000000\n      1.011236\n      0.974157\n      0.988764\n      94755.235955\n      1.004719\n      0.987640\n      0.954307\n      1.015178\n      1.002500\n      ...\n      58.029378\n      0.779487\n      6.199506\n      48.326356\n      75.903614\n      -24.096386\n      53.588235\n      -0.885936\n      0.530558\n      2.429462\n    \n    \n      2\n      1.007463\n      1.012793\n      0.996802\n      1.005330\n      105709.495736\n      0.974840\n      0.944350\n      0.910856\n      0.953198\n      0.950598\n      ...\n      46.479620\n      0.000000\n      16.267954\n      46.534386\n      14.444444\n      -85.555556\n      343.500000\n      0.898876\n      1.645809\n      10.986297\n    \n    \n      3\n      1.007423\n      1.033934\n      0.975610\n      1.033934\n      327239.156946\n      0.987063\n      0.943160\n      0.907971\n      0.948206\n      0.958880\n      ...\n      52.721907\n      0.225976\n      14.411703\n      44.991463\n      40.909091\n      -59.090909\n      289.705882\n      1.651982\n      1.484638\n      9.126668\n    \n    \n      4\n      0.994872\n      0.994872\n      0.950769\n      0.974359\n      168981.128205\n      0.966974\n      0.914769\n      0.879402\n      0.917145\n      0.932738\n      ...\n      49.489503\n      0.108960\n      11.827189\n      44.008782\n      26.136364\n      -73.863636\n      221.794118\n      -2.985075\n      1.229302\n      5.503306\n    \n  \n\n5 rows × 560 columns\n\n\n\n\nSHAP 표준화\n\n\nimport shap\nshap.initjs()\nexplainer = shap.TreeExplainer(xgb_1)\nshap_values_1 = explainer.shap_values(df_trainX_1) # train \nshap_values_test_1 = explainer.shap_values(df_testX_1) # test \n\n\n\n\n\n\n\n\nsummary plot 해석 방법\n\nSummary plot에서 X축은 SHAP 값으로, 모델 예측 값에 영향을 준 정도의 수치를 의미한다. (-1, 1) 사이의 값이며 영향력이 없을 수록 0에 가까운 값이다. 이러한 SHAP value가 양수 값이면 긍정적인 영향 (양의 영향), 음수 값이면 부정적인 영향 (음의 영향)을 끼쳤음을 뜻한다. 해당 모델에서는 양수의 값이 클 수록 label 1로 예측하는 데 영향을 많이 미쳤다는 뜻으로 해석할 수 있다. Y축은 설명 변수이고, 색깔은 설명 변수의 개별 데이터 값의 크기를 말한다. 빨간색일 수록 데이터의 값이 상대적으로 크고, 파란색일 수록 값이 상대적으로 작은 데이터임을 뜻한다.\n\nshap.summary_plot(shap_values_1, df_trainX_1)\n\n\n\n\n\n\n\n\nshap.summary_plot(shap_values_test_1, df_testX_1)\n\n\n\n\n중립구간에서 train, test 데이터셋의 중요 변수 상위 6개가 동일하게 나왔음을 알 수 있다. 중요도 상위 설명 변수 3개를 해석해보면, D-0_KCH의 값이 클 수록 양의 영향 (다음 날 종가 2% 상승할 것이라 예측하는 데 영향)을 미쳤으며, D-0_DCL 의 값이 작을 수록 양의 영향, D-0_KCL의 값이 작을 수록 양의 영향을 끼쳤다.\n\n\n\n\n\n\n과매수구간 데이터셋 불러오기\n\n\ntrainX_2, trainY_2, testX_2, testY_2, lst_code_date_2, lst_code_date_test_2, xgb_2 = dic_dataset_model['CCI 100~']\nprint('train dataset: ', trainX_2.shape, trainY_2.shape)\nprint('test dataset: ', testX_2.shape, testY_2.shape)\n\ntrain dataset:  (256399, 560) (256399,)\ntest dataset:  (64768, 560) (64768,)\n\n\n\nnumpy -> DataFrame\n\n\n##### train\ndf_trainX_2 = pd.DataFrame(trainX_2)\ndf_trainX_2.columns = lst_col_info\ndf_trainY_2 = pd.DataFrame(trainY_2)\n\n##### test \ndf_testX_2 = pd.DataFrame(testX_2)\ndf_testX_2.columns = lst_col_info\ndf_testY_2 = pd.DataFrame(testY_2)\n\ndf_trainX_2.head()\n\n\n\n\n\n  \n    \n      \n      D-9_Open\n      D-9_High\n      D-9_Low\n      D-9_Close\n      D-9_trading_value\n      D-9_MA5\n      D-9_MA20\n      D-9_MA60\n      D-9_MA120\n      D-9_VMAP\n      ...\n      D-0_RSI\n      D-0_SRSI\n      D-0_TSI\n      D-0_UO\n      D-0_SR\n      D-0_WR\n      D-0_AO\n      D-0_ROC\n      D-0_PPO\n      D-0_PVO\n    \n  \n  \n    \n      0\n      1.014235\n      1.014235\n      0.990510\n      1.002372\n      50865.391459\n      1.012100\n      1.039442\n      1.004251\n      1.067230\n      1.062162\n      ...\n      67.826155\n      1.000000\n      14.884190\n      55.338001\n      100.952381\n      0.952381\n      223.411765\n      10.613208\n      1.337481\n      6.227088\n    \n    \n      1\n      0.984615\n      1.023669\n      0.984615\n      0.997633\n      55152.152663\n      1.000947\n      1.036391\n      1.002367\n      1.063619\n      1.055599\n      ...\n      68.784853\n      1.000000\n      18.642720\n      51.178165\n      94.067797\n      -5.932203\n      310.029412\n      10.941176\n      1.688808\n      4.010564\n    \n    \n      2\n      0.990510\n      1.064057\n      0.990510\n      1.026097\n      78619.572954\n      1.007355\n      1.039739\n      1.005575\n      1.065055\n      1.054236\n      ...\n      74.103331\n      1.000000\n      23.963771\n      66.127796\n      100.000000\n      -0.000000\n      393.558824\n      15.658363\n      2.221982\n      14.120940\n    \n    \n      3\n      0.996532\n      1.040462\n      0.996532\n      1.038150\n      210838.980347\n      0.992832\n      1.016358\n      0.980732\n      1.037177\n      1.021560\n      ...\n      64.812483\n      0.721663\n      25.077575\n      61.734791\n      82.517483\n      -17.482517\n      451.588235\n      12.426036\n      2.389092\n      13.705978\n    \n    \n      4\n      1.013363\n      1.013363\n      0.982183\n      0.983296\n      303571.073497\n      0.965256\n      0.980401\n      0.944933\n      0.998181\n      0.984157\n      ...\n      66.615529\n      0.774168\n      26.576492\n      63.519219\n      89.510490\n      -10.489510\n      525.558824\n      13.879004\n      2.576580\n      18.168169\n    \n  \n\n5 rows × 560 columns\n\n\n\n\nSHAP 표준화\n\n\n##### train\nimport shap\nshap.initjs()\nexplainer = shap.TreeExplainer(xgb_2)\nshap_values_2 = explainer.shap_values(df_trainX_2)\nshap_values_test_2 = explainer.shap_values(df_testX_2)\n\n\n\n\n\n\n\n\nshap.summary_plot(shap_values_2, df_trainX_2)\n\n\n\n\n\n\n\n\nshap.summary_plot(shap_values_test_2, df_testX_2)\n\n\n\n\n과매수구간도 마찬가지로 train, test 데이터셋은 유사한 변수 중요도를 가진다. D-0_High의 값이 클 수록, D-0_KCL의 값이 작을 수록, D-0_DCH 값이 클 수록 양의 영향을 미쳤다고 해석된다.\n\n\n\n\n\n\n과매수구간 데이터셋 불러오기\n\n\ntrainX_3, trainY_3, testX_3, testY_3, lst_code_date_3, lst_code_date_test_3, xgb_3 = dic_dataset_model['CCI ~-100']\nprint('train dataset: ', trainX_3.shape, trainY_3.shape)\nprint('test dataset: ', testX_3.shape, testY_3.shape)\n\ntrain dataset:  (269977, 560) (269977,)\ntest dataset:  (73435, 560) (73435,)\n\n\n\nnumpy -> DataFrame\n\n\n##### train\ndf_trainX_3 = pd.DataFrame(trainX_3)\ndf_trainX_3.columns = lst_col_info\ndf_trainY_3 = pd.DataFrame(trainY_3)\n\n##### test \ndf_testX_3 = pd.DataFrame(testX_3)\ndf_testX_3.columns = lst_col_info\ndf_testY_3 = pd.DataFrame(testY_3)\n\ndf_trainX_3.head()\n\n\n\n\n\n  \n    \n      \n      D-9_Open\n      D-9_High\n      D-9_Low\n      D-9_Close\n      D-9_trading_value\n      D-9_MA5\n      D-9_MA20\n      D-9_MA60\n      D-9_MA120\n      D-9_VMAP\n      ...\n      D-0_RSI\n      D-0_SRSI\n      D-0_TSI\n      D-0_UO\n      D-0_SR\n      D-0_WR\n      D-0_AO\n      D-0_ROC\n      D-0_PPO\n      D-0_PVO\n    \n  \n  \n    \n      0\n      0.991238\n      1.004381\n      0.971522\n      0.980285\n      140309.140197\n      0.984885\n      0.943045\n      0.936528\n      0.995007\n      0.969697\n      ...\n      40.723504\n      0.000000\n      5.486901\n      35.175106\n      8.421053\n      -91.578947\n      86.911765\n      -7.362637\n      0.401602\n      -13.360869\n    \n    \n      1\n      1.019084\n      1.019084\n      0.938931\n      0.946565\n      690931.786260\n      0.964885\n      0.889427\n      0.787226\n      0.717487\n      0.930415\n      ...\n      45.121178\n      0.022302\n      7.541308\n      40.870588\n      10.000000\n      -90.000000\n      43.235294\n      -13.127413\n      1.251577\n      -3.674717\n    \n    \n      2\n      0.991935\n      1.040323\n      0.987903\n      1.004032\n      317878.620968\n      1.026613\n      0.945484\n      0.836371\n      0.760813\n      0.987418\n      ...\n      42.403451\n      0.000000\n      3.945359\n      39.056401\n      1.960784\n      -98.039216\n      -168.382353\n      -13.725490\n      0.615608\n      -4.840261\n    \n    \n      3\n      1.000000\n      1.020080\n      0.979920\n      0.991968\n      350971.128514\n      1.012851\n      0.946787\n      0.837590\n      0.760281\n      0.986872\n      ...\n      36.691473\n      0.000000\n      -1.153744\n      34.272840\n      0.000000\n      -100.000000\n      -473.676471\n      -20.610687\n      -0.308624\n      -0.268302\n    \n    \n      4\n      1.004049\n      1.004049\n      0.955466\n      0.979757\n      327074.267206\n      1.010526\n      0.958907\n      0.848691\n      0.768900\n      1.001767\n      ...\n      38.186006\n      0.041239\n      -4.753380\n      32.124091\n      4.761905\n      -95.238095\n      -627.794118\n      -15.322581\n      -0.969999\n      0.192505\n    \n  \n\n5 rows × 560 columns\n\n\n\n\nSHAP 표준화\n\n\n##### train\nimport shap\nshap.initjs()\nexplainer = shap.TreeExplainer(xgb_3)\nshap_values_3 = explainer.shap_values(df_trainX_3)\nshap_values_test_3 = explainer.shap_values(df_testX_3)\n\n\n\n\n\n\n\n\nshap.summary_plot(shap_values_3, df_trainX_3)\n\n\n\n\n\n\n\n\nshap.summary_plot(shap_values_test_3, df_testX_3)\n\n\n\n\n과매도구간 또한 train, test 데이터셋이 유사한 변수 중요도를 가졌다. 과매도구간은 모델 학습 당시 모델의 성능이 가장 좋게 나왔으며, summary plot 에서도 색깔의 경계가 가장 뚜렷하게 나타났다. D-0_KCH의 값이 클 수록, D-0_Close의 값이 작을 수록, D-0_SMA 값이 클 수록 양의 영향을 미쳤다고 해석할 수 있다.\n \n\n\n\n\n색의 경계가 가장 뚜렷하게 나타난 과매도 구간에서 상위 변수 3개를 이용하여 데이터를 필터링 하고, 특징이 나타나는지 확인해본다.\n\ndf_testX_3['Code'] = lst_code_date_test_3[:, 0]\ndf_testX_3['Date'] = lst_code_date_test_3[:, 1]\n\ndf_shap_test_3 = pd.DataFrame(shap_values_test_3, columns=lst_col_info)\n\ncondition1 = df_shap_test_3['D-0_KCH'] > 0.3\ncondition2 = df_shap_test_3['D-0_Close'] > 0\ncondition3 = df_shap_test_3['D-0_SMA'] > 0.05\n\nprint('조건을 만족하는 데이터의 개수:', len(df_testX_3.loc[condition1 & condition2 & condition3]))\nprint()\nprint(\"<빈도수 상위 날짜 Top 5>\")\ndisplay(df_testX_3.loc[condition1 & condition2 & condition3, ['Date']].value_counts().head())\n\n조건을 만족하는 데이터의 개수: 467\n\n<빈도수 상위 날짜 Top 5>\n\n\nDate      \n2021-10-06    58\n2021-08-20    45\n2021-03-10    16\n2021-10-01    13\n2021-10-05    13\ndtype: int64\n\n\n조건을 만족하는 데이터 467개 중 빈도수가 높게 나오는 날짜들이 있다. 2021년 10월 6일, 2021년 8월 20일의 데이터를 살펴보도록 한다. 이 때 종목코드는 sample 함수를 사용하여 랜덤으로 선정한다.\n\n조건을 만족하는 데이터 중 상위 빈도수 날짜의 랜덤 종목 차트 확인\n\n\ndf_1006 = df_testX_3.loc[condition1 & condition2 & condition3 & (df_testX_3['Date'] == '2021-10-06'), ['Date', 'Code']].sample(5).reset_index(drop=True)\ndf_0820 = df_testX_3.loc[condition1 & condition2 & condition3 & (df_testX_3['Date'] == '2021-08-20'), ['Date', 'Code']].sample(5).reset_index(drop=True)\n\npd.concat([df_1006, df_0820], axis=1)\n\n\n\n\n\n  \n    \n      \n      Date\n      Code\n      Date\n      Code\n    \n  \n  \n    \n      0\n      2021-10-06\n      154040\n      2021-08-20\n      003830\n    \n    \n      1\n      2021-10-06\n      092070\n      2021-08-20\n      103590\n    \n    \n      2\n      2021-10-06\n      101390\n      2021-08-20\n      043340\n    \n    \n      3\n      2021-10-06\n      057030\n      2021-08-20\n      039610\n    \n    \n      4\n      2021-10-06\n      084870\n      2021-08-20\n      025620\n    \n  \n\n\n\n\n2021년 10월 6일\n\n\n“종목코드: 154040, 092070”\n2021년 8월 20일\n \n“종목코드: 003830, 103590”\n이미지 출처-Naver 증권\n\n랜덤으로 선택한 4가지 종목들의 차트를 확인해 보았을 때, 하락을 종료하고 상승의 초입 부분에서 매매 시그널을 보였다.\n\nSHAP 표준화 데이터셋 저장\n\n\n#collapse-hide\nimport pickle \ndic_shap_dataset = {'CCI -20~20': [shap_values_1, shap_values_test_1],\n                'CCI 100~': [shap_values_2, shap_values_test_2],\n                'CCI ~-100': [shap_values_3, shap_values_test_3]}\n\nwith open('shap_dataset_cci_filtering.pickle', 'wb') as handle:\n    pickle.dump(dic_shap_dataset, handle, protocol=pickle.HIGHEST_PROTOCOL)\n\n \n본 프로젝트 방향은 SHAP을 활용하여 유사한 특징을 갖고 있는 특정 집단을 발견하고 해석해내는 것이라고 할 수 있다. 위에서 변수 중요도를 시각화 하고, 상위 3개 변수에 대하여 양의 영향을 미치는 비슷한 크기의 값인 데이터들만 필터링하여 차트를 확인하였다. 이번 글에서 SHAP value summary plot을 활용하여 직접 집단을 구성해보았다면, 다음에는 이러한 집단을 알고리즘을 사용하여, 구체적으로 형성하는 방법을 연구한다. 다음 글에서는 저장한 SHAP 표준화 데이터셋과 주가 데이터셋을 2차원 평면에 시각화 하여 군집들이 형성되는지 확인하는 시간을 가진다."
  },
  {
    "objectID": "posts/07.tsne.html",
    "href": "posts/07.tsne.html",
    "title": "[stock prediction] 3.2 t-SNE를 사용한 주가데이터 2차원 시각화",
    "section": "",
    "text": "이전 글 ( 3.1. 설명가능 AI (XAI), SHAP value ) 에서는 XAI 기법 중 하나인 SHAP을 사용하여 변수 중요도를 계산하고, summary plot의 결과로 데이터를 필터링하여 특정 집단을 구성해보았다. 이번 글에서는 주가 데이터셋과, SHAP표준화 데이터셋을 2차원 평면에 시각화함으로써 집단 형성이 이루어지는 것을 확인한다.\n\n\n\n\nSHAP 표준화 데이터셋\n\n\nT-SNE를 사용한 2차원 시각화 비교 (원본 데이터셋 vs SHAP transform 데이터셋)\n\n라이브러리 import\n\n\nimport pandas as pd\nimport numpy as np\nfrom tqdm import tqdm\nimport pymysql\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom ipywidgets import interact, interact_manual\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.graph_objects as go\nimport plotly.offline as pyo\n%matplotlib inline\n%pylab inline\npylab.rcParams['figure.figsize'] = (12,5)\n\nimport StockFunc as sf\n\n%pylab is deprecated, use %matplotlib inline and import the required libraries.\nPopulating the interactive namespace from numpy and matplotlib\n\n\n \n\n\n\n\nSHAP 표준화 데이터셋이란 주가 예측 머신러닝 모델(XGBoost)로 각 데이터에 대한 Shap value를 계산하여 기존의 주가 데이터셋을 변환한 데이터를 말한다. 기존 주가 데이터가 SHAP 표준화 데이터로 변환되었을 때, 이진분류 주가 예측 모델의 중요 변수를 상대적으로 가중하게됨으로써 데이터에서의 내재되어있는 패턴이 강조되는 효과가 있다. 따라서 기존의 주가 데이터셋과 SHAP 표준화 데이터셋을 2차원 평면에 함께 시각화 하여 SHAP 표준화 데이터셋의 군집의 경계가 더 명확하게 드러나는 것을 확인한다.\n\nCCI 구간 별 주가 데이터셋 불러오기\n\n\nimport pickle\nwith open('dataset_cci_filtering.pickle', 'rb') as handle:\n    dic_dataset_model = pickle.load(handle)\n\nCCI filtering 글에서 생성한 CCI 구간 별 데이터셋과 모델을 불러온다.\n\n컬럼 리스트 생성\n\n\n#collapse-hide\nlst_col_info = []\n\nlst_col = ['Open', 'High', 'Low', 'Close', 'trading_value', 'MA5', 'MA20', 'MA60', 'MA120', \n           'VMAP', 'BHB', 'BLB', 'KCH', 'KCL', 'KCM', 'DCH', 'DCL', 'DCM',\n           'SMA', 'EMA', 'WMA', 'Ichimoku', 'Parabolic SAR', 'KAMA','MACD']   +  ['Change', 'Volume', 'MFI', 'ADI', 'OBV', \n                                                                                   'CMF', 'FI', 'EOM, EMV', 'VPT', 'NVI', 'ATR', 'UI',\n                                                                                   'ADX', '-VI', '+VI', 'TRIX', 'MI', 'CCI', 'DPO', 'KST', \n                                                                                   'STC', 'RSI', 'SRSI', 'TSI', 'UO', 'SR',\n                                                                                   'WR', 'AO', 'ROC', 'PPO', 'PVO'] \n           \n\nfor day in range(9, -1, -1): \n    for col in lst_col: \n        lst_col_info.append(f'D-{day}_{col}')\n\n뒤에서 1년 단위로 2차원 시각화를 하기 위해 train, test 데이터셋을 합치고, 각 CCI 구간 별 SHAP transform 데이터를 생성한다.\n\n\n\n\ntrainX_1, trainY_1, testX_1, testY_1, lst_code_date_1, lst_code_date_test_1, xgb_1 = dic_dataset_model['CCI -20~20']\nprint('train dataset: ', trainX_1.shape, trainY_1.shape)\nprint('test dataset: ', testX_1.shape, testY_1.shape)\n\ntrain dataset:  (123321, 560) (123321,)\ntest dataset:  (36885, 560) (36885,)\n\n\n\ncode, date, label컬럼 추가 및 train, test을 합친 데이터프레임 생성\n\n\n#collapse-hide\n# train + test (array)\narr_data_1 = np.concatenate((trainX_1, testX_1), axis=0)\narr_label_1 = np.concatenate((trainY_1, testY_1), axis=0)\narr_code_date_1 = np.concatenate((lst_code_date_1, lst_code_date_test_1), axis=0)\n\n# code + date + dataset + label (dataframe)\ndf_data_1 = pd.DataFrame(arr_data_1, columns=lst_col_info)\ndf_data_1['Code'], df_data_1['Date'], df_data_1['Label'] = arr_code_date_1[:, 0], arr_code_date_1[:, 1], arr_label_1\ndf_data_1 = df_data_1[['Code', 'Date'] + lst_col_info + ['Label']]\n\nprint('df_data_1 shape:', df_data_1.shape)\ndisplay(df_data_1.head())\n\ndf_data_1 shape: (160206, 563)\n\n\n\n\n\n\n  \n    \n      \n      Code\n      Date\n      D-9_Open\n      D-9_High\n      D-9_Low\n      D-9_Close\n      D-9_trading_value\n      D-9_MA5\n      D-9_MA20\n      D-9_MA60\n      ...\n      D-0_SRSI\n      D-0_TSI\n      D-0_UO\n      D-0_SR\n      D-0_WR\n      D-0_AO\n      D-0_ROC\n      D-0_PPO\n      D-0_PVO\n      Label\n    \n  \n  \n    \n      0\n      050120\n      2017-01-20\n      1.005714\n      1.048000\n      1.000000\n      1.040000\n      462369.440000\n      0.994514\n      0.968971\n      0.981010\n      ...\n      0.057077\n      20.137832\n      43.856662\n      46.236559\n      -53.763441\n      433.235294\n      2.564103\n      1.624561\n      -3.039166\n      0\n    \n    \n      1\n      050120\n      2017-02-06\n      1.000000\n      1.011236\n      0.974157\n      0.988764\n      94755.235955\n      1.004719\n      0.987640\n      0.954307\n      ...\n      0.779487\n      6.199506\n      48.326356\n      75.903614\n      -24.096386\n      53.588235\n      -0.885936\n      0.530558\n      2.429462\n      0\n    \n    \n      2\n      050120\n      2017-02-24\n      1.007463\n      1.012793\n      0.996802\n      1.005330\n      105709.495736\n      0.974840\n      0.944350\n      0.910856\n      ...\n      0.000000\n      16.267954\n      46.534386\n      14.444444\n      -85.555556\n      343.500000\n      0.898876\n      1.645809\n      10.986297\n      1\n    \n    \n      3\n      050120\n      2017-02-27\n      1.007423\n      1.033934\n      0.975610\n      1.033934\n      327239.156946\n      0.987063\n      0.943160\n      0.907971\n      ...\n      0.225976\n      14.411703\n      44.991463\n      40.909091\n      -59.090909\n      289.705882\n      1.651982\n      1.484638\n      9.126668\n      0\n    \n    \n      4\n      050120\n      2017-02-28\n      0.994872\n      0.994872\n      0.950769\n      0.974359\n      168981.128205\n      0.966974\n      0.914769\n      0.879402\n      ...\n      0.108960\n      11.827189\n      44.008782\n      26.136364\n      -73.863636\n      221.794118\n      -2.985075\n      1.229302\n      5.503306\n      0\n    \n  \n\n5 rows × 563 columns\n\n\n\n\nSHAP transform 데이터셋 생성\n\n\n#collapse-hide\nimport shap\nshap.initjs()\nexplainer = shap.TreeExplainer(xgb_1)\nshap_values = explainer.shap_values(df_data_1.drop(columns=['Code', 'Date', 'Label']))\n\ndf_shap_values_1 = pd.DataFrame(shap_values, columns=lst_col_info)\ndf_shap_values_1['Code'], df_shap_values_1['Date'], df_shap_values_1['Label'] = arr_code_date_1[:, 0], arr_code_date_1[:, 1], arr_label_1\ndf_shap_values_1 = df_shap_values_1[['Code', 'Date'] + lst_col_info + ['Label']]\n\nprint('df_shap_values_1 shape:', df_shap_values_1.shape)\ndisplay(df_shap_values_1.head())\n\n\n\n\n\ndf_shap_values_1 shape: (160206, 563)\n\n\n\n\n\n\n  \n    \n      \n      Code\n      Date\n      D-9_Open\n      D-9_High\n      D-9_Low\n      D-9_Close\n      D-9_trading_value\n      D-9_MA5\n      D-9_MA20\n      D-9_MA60\n      ...\n      D-0_SRSI\n      D-0_TSI\n      D-0_UO\n      D-0_SR\n      D-0_WR\n      D-0_AO\n      D-0_ROC\n      D-0_PPO\n      D-0_PVO\n      Label\n    \n  \n  \n    \n      0\n      050120\n      2017-01-20\n      -0.007915\n      -0.001929\n      -0.000805\n      0.001416\n      -0.000197\n      0.0\n      0.000018\n      -0.000571\n      ...\n      0.0\n      0.0\n      0.003014\n      0.000004\n      0.0\n      0.000062\n      -0.003845\n      0.000004\n      -0.001181\n      0\n    \n    \n      1\n      050120\n      2017-02-06\n      -0.007741\n      -0.001461\n      0.000470\n      0.000085\n      -0.000197\n      0.0\n      0.000012\n      -0.000229\n      ...\n      0.0\n      0.0\n      0.001020\n      0.000009\n      0.0\n      0.000052\n      0.004082\n      0.000004\n      0.007927\n      0\n    \n    \n      2\n      050120\n      2017-02-24\n      -0.009699\n      -0.001854\n      -0.000975\n      -0.000129\n      -0.000197\n      0.0\n      0.000018\n      -0.000571\n      ...\n      0.0\n      0.0\n      0.001876\n      -0.000005\n      0.0\n      0.000062\n      -0.004085\n      0.000004\n      0.007150\n      1\n    \n    \n      3\n      050120\n      2017-02-27\n      -0.008703\n      -0.001563\n      0.000816\n      0.000360\n      -0.000197\n      0.0\n      0.000018\n      -0.000571\n      ...\n      0.0\n      0.0\n      0.001332\n      0.000009\n      0.0\n      0.000062\n      -0.003878\n      0.000004\n      0.008917\n      0\n    \n    \n      4\n      050120\n      2017-02-28\n      -0.003862\n      0.007390\n      0.000849\n      0.000148\n      -0.000197\n      0.0\n      0.000018\n      -0.000571\n      ...\n      0.0\n      0.0\n      0.001248\n      -0.000005\n      0.0\n      0.000062\n      0.004082\n      0.000004\n      0.008872\n      0\n    \n  \n\n5 rows × 563 columns\n\n\n\n\n\n\n\n\ntrainX_2, trainY_2, testX_2, testY_2, lst_code_date_2, lst_code_date_test_2, xgb_2 = dic_dataset_model['CCI 100~']\nprint('train dataset: ', trainX_2.shape, trainY_2.shape)\nprint('test dataset: ', testX_2.shape, testY_2.shape)\n\ntrain dataset:  (256399, 560) (256399,)\ntest dataset:  (64768, 560) (64768,)\n\n\n\ncode, date, label컬럼 추가 및 train, test을 합친 데이터프레임 생성\n\n\n#collapse-hide\n# train + test (array)\narr_data_2 = np.concatenate((trainX_2, testX_2), axis=0)\narr_label_2 = np.concatenate((trainY_2, testY_2), axis=0)\narr_code_date_2 = np.concatenate((lst_code_date_2, lst_code_date_test_2), axis=0)\n\n# code + date + dataset + label (dataframe)\ndf_data_2 = pd.DataFrame(arr_data_2, columns=lst_col_info)\ndf_data_2['Code'], df_data_2['Date'], df_data_2['Label'] = arr_code_date_2[:, 0], arr_code_date_2[:, 1], arr_label_2\ndf_data_2 = df_data_2[['Code', 'Date'] + lst_col_info + ['Label']]\n\nprint('df_data_2 shape:', df_data_2.shape)\ndisplay(df_data_2.head())\n\ndf_data_2 shape: (321167, 563)\n\n\n\n\n\n\n  \n    \n      \n      Code\n      Date\n      D-9_Open\n      D-9_High\n      D-9_Low\n      D-9_Close\n      D-9_trading_value\n      D-9_MA5\n      D-9_MA20\n      D-9_MA60\n      ...\n      D-0_SRSI\n      D-0_TSI\n      D-0_UO\n      D-0_SR\n      D-0_WR\n      D-0_AO\n      D-0_ROC\n      D-0_PPO\n      D-0_PVO\n      Label\n    \n  \n  \n    \n      0\n      050120\n      2017-02-10\n      1.014235\n      1.014235\n      0.990510\n      1.002372\n      50865.391459\n      1.012100\n      1.039442\n      1.004251\n      ...\n      1.000000\n      14.884190\n      55.338001\n      100.952381\n      0.952381\n      223.411765\n      10.613208\n      1.337481\n      6.227088\n      0\n    \n    \n      1\n      050120\n      2017-02-13\n      0.984615\n      1.023669\n      0.984615\n      0.997633\n      55152.152663\n      1.000947\n      1.036391\n      1.002367\n      ...\n      1.000000\n      18.642720\n      51.178165\n      94.067797\n      -5.932203\n      310.029412\n      10.941176\n      1.688808\n      4.010564\n      1\n    \n    \n      2\n      050120\n      2017-02-14\n      0.990510\n      1.064057\n      0.990510\n      1.026097\n      78619.572954\n      1.007355\n      1.039739\n      1.005575\n      ...\n      1.000000\n      23.963771\n      66.127796\n      100.000000\n      -0.000000\n      393.558824\n      15.658363\n      2.221982\n      14.120940\n      0\n    \n    \n      3\n      050120\n      2017-02-15\n      0.996532\n      1.040462\n      0.996532\n      1.038150\n      210838.980347\n      0.992832\n      1.016358\n      0.980732\n      ...\n      0.721663\n      25.077575\n      61.734791\n      82.517483\n      -17.482517\n      451.588235\n      12.426036\n      2.389092\n      13.705978\n      0\n    \n    \n      4\n      050120\n      2017-02-16\n      1.013363\n      1.013363\n      0.982183\n      0.983296\n      303571.073497\n      0.965256\n      0.980401\n      0.944933\n      ...\n      0.774168\n      26.576492\n      63.519219\n      89.510490\n      -10.489510\n      525.558824\n      13.879004\n      2.576580\n      18.168169\n      0\n    \n  \n\n5 rows × 563 columns\n\n\n\n\nSHAP transform 데이터셋 생성\n\n\n#collapse-hide\nimport shap\nshap.initjs()\nexplainer = shap.TreeExplainer(xgb_2)\nshap_values = explainer.shap_values(df_data_2.drop(columns=['Code', 'Date', 'Label']))\n\ndf_shap_values_2 = pd.DataFrame(shap_values, columns=lst_col_info)\ndf_shap_values_2['Code'], df_shap_values_2['Date'], df_shap_values_2['Label'] = arr_code_date_2[:, 0], arr_code_date_2[:, 1], arr_label_2\ndf_shap_values_2 = df_shap_values_2[['Code', 'Date'] + lst_col_info + ['Label']]\n\nprint('df_shap_values_2 shape:', df_shap_values_2.shape)\ndisplay(df_shap_values_2.head())\n\n\n\n\n\ndf_shap_values_2 shape: (321167, 563)\n\n\n\n\n\n\n  \n    \n      \n      Code\n      Date\n      D-9_Open\n      D-9_High\n      D-9_Low\n      D-9_Close\n      D-9_trading_value\n      D-9_MA5\n      D-9_MA20\n      D-9_MA60\n      ...\n      D-0_SRSI\n      D-0_TSI\n      D-0_UO\n      D-0_SR\n      D-0_WR\n      D-0_AO\n      D-0_ROC\n      D-0_PPO\n      D-0_PVO\n      Label\n    \n  \n  \n    \n      0\n      050120\n      2017-02-10\n      -0.000128\n      1.306584e-06\n      -0.002761\n      0.0\n      0.000007\n      0.000003\n      0.000001\n      0.0\n      ...\n      -0.001276\n      0.000247\n      0.0\n      -0.010375\n      0.0\n      0.000695\n      -0.005244\n      0.001141\n      -0.002147\n      0\n    \n    \n      1\n      050120\n      2017-02-13\n      0.002046\n      -8.618027e-07\n      -0.002228\n      0.0\n      0.000004\n      -0.000001\n      0.000001\n      0.0\n      ...\n      -0.000481\n      0.000249\n      0.0\n      -0.011002\n      0.0\n      0.000695\n      -0.005244\n      0.001374\n      -0.002653\n      1\n    \n    \n      2\n      050120\n      2017-02-14\n      -0.000148\n      -8.618027e-07\n      -0.002317\n      0.0\n      0.000007\n      0.000041\n      0.000001\n      0.0\n      ...\n      -0.000370\n      0.000249\n      0.0\n      -0.011764\n      0.0\n      0.000695\n      -0.002502\n      0.001067\n      -0.002653\n      0\n    \n    \n      3\n      050120\n      2017-02-15\n      -0.000173\n      -8.618027e-07\n      -0.002759\n      0.0\n      0.000046\n      -0.000001\n      0.000001\n      0.0\n      ...\n      0.000536\n      0.000249\n      0.0\n      -0.012226\n      0.0\n      0.000695\n      -0.005757\n      0.001003\n      -0.003974\n      0\n    \n    \n      4\n      050120\n      2017-02-16\n      -0.000172\n      1.306584e-06\n      -0.001784\n      0.0\n      0.000024\n      -0.000020\n      0.000001\n      0.0\n      ...\n      0.000400\n      -0.000031\n      0.0\n      -0.011209\n      0.0\n      0.000255\n      -0.004513\n      0.000797\n      0.010798\n      0\n    \n  \n\n5 rows × 563 columns\n\n\n\n\n\n\n\n\ntrainX_3, trainY_3, testX_3, testY_3, lst_code_date_3, lst_code_date_test_3, xgb_3 = dic_dataset_model['CCI ~-100']\nprint('train dataset: ', trainX_3.shape, trainY_3.shape)\nprint('test dataset: ', testX_3.shape, testY_3.shape)\n\ntrain dataset:  (269977, 560) (269977,)\ntest dataset:  (73435, 560) (73435,)\n\n\n\ncode, date, label컬럼 추가 및 train, test을 합친 데이터프레임 생성\n\n\n#collapse-hide\n# train + test (array)\narr_data_3 = np.concatenate((trainX_3, testX_3), axis=0)\narr_label_3 = np.concatenate((trainY_3, testY_3), axis=0)\narr_code_date_3 = np.concatenate((lst_code_date_3, lst_code_date_test_3), axis=0)\n\n# code + date + dataset + label (dataframe)\ndf_data_3 = pd.DataFrame(arr_data_3, columns=lst_col_info)\ndf_data_3['Code'], df_data_3['Date'], df_data_3['Label'] = arr_code_date_3[:, 0], arr_code_date_3[:, 1], arr_label_3\ndf_data_3 = df_data_3[['Code', 'Date'] + lst_col_info + ['Label']]\n\nprint('df_data_3 shape:', df_data_3.shape)\ndisplay(df_data_3.head())\n\ndf_data_3 shape: (343412, 563)\n\n\n\n\n\n\n  \n    \n      \n      Code\n      Date\n      D-9_Open\n      D-9_High\n      D-9_Low\n      D-9_Close\n      D-9_trading_value\n      D-9_MA5\n      D-9_MA20\n      D-9_MA60\n      ...\n      D-0_SRSI\n      D-0_TSI\n      D-0_UO\n      D-0_SR\n      D-0_WR\n      D-0_AO\n      D-0_ROC\n      D-0_PPO\n      D-0_PVO\n      Label\n    \n  \n  \n    \n      0\n      050120\n      2017-01-25\n      0.991238\n      1.004381\n      0.971522\n      0.980285\n      140309.140197\n      0.984885\n      0.943045\n      0.936528\n      ...\n      0.000000\n      5.486901\n      35.175106\n      8.421053\n      -91.578947\n      86.911765\n      -7.362637\n      0.401602\n      -13.360869\n      0\n    \n    \n      1\n      050120\n      2017-05-18\n      1.019084\n      1.019084\n      0.938931\n      0.946565\n      690931.786260\n      0.964885\n      0.889427\n      0.787226\n      ...\n      0.022302\n      7.541308\n      40.870588\n      10.000000\n      -90.000000\n      43.235294\n      -13.127413\n      1.251577\n      -3.674717\n      0\n    \n    \n      2\n      050120\n      2017-05-19\n      0.991935\n      1.040323\n      0.987903\n      1.004032\n      317878.620968\n      1.026613\n      0.945484\n      0.836371\n      ...\n      0.000000\n      3.945359\n      39.056401\n      1.960784\n      -98.039216\n      -168.382353\n      -13.725490\n      0.615608\n      -4.840261\n      0\n    \n    \n      3\n      050120\n      2017-05-22\n      1.000000\n      1.020080\n      0.979920\n      0.991968\n      350971.128514\n      1.012851\n      0.946787\n      0.837590\n      ...\n      0.000000\n      -1.153744\n      34.272840\n      0.000000\n      -100.000000\n      -473.676471\n      -20.610687\n      -0.308624\n      -0.268302\n      0\n    \n    \n      4\n      050120\n      2017-05-23\n      1.004049\n      1.004049\n      0.955466\n      0.979757\n      327074.267206\n      1.010526\n      0.958907\n      0.848691\n      ...\n      0.041239\n      -4.753380\n      32.124091\n      4.761905\n      -95.238095\n      -627.794118\n      -15.322581\n      -0.969999\n      0.192505\n      0\n    \n  \n\n5 rows × 563 columns\n\n\n\n\nSHAP transform 데이터셋 생성\n\n\n#collapse-hide\nimport shap\nshap.initjs()\nexplainer = shap.TreeExplainer(xgb_3)\nshap_values = explainer.shap_values(df_data_3.drop(columns=['Code', 'Date', 'Label']))\n\ndf_shap_values_3 = pd.DataFrame(shap_values, columns=lst_col_info)\ndf_shap_values_3['Code'], df_shap_values_3['Date'], df_shap_values_3['Label'] = arr_code_date_3[:, 0], arr_code_date_3[:, 1], arr_label_3\ndf_shap_values_3 = df_shap_values_3[['Code', 'Date'] + lst_col_info + ['Label']]\n\nprint('df_shap_values_3 shape:', df_shap_values_3.shape)\ndisplay(df_shap_values_3.head())\n\n\n\n\n\ndf_shap_values_3 shape: (343412, 563)\n\n\n\n\n\n\n  \n    \n      \n      Code\n      Date\n      D-9_Open\n      D-9_High\n      D-9_Low\n      D-9_Close\n      D-9_trading_value\n      D-9_MA5\n      D-9_MA20\n      D-9_MA60\n      ...\n      D-0_SRSI\n      D-0_TSI\n      D-0_UO\n      D-0_SR\n      D-0_WR\n      D-0_AO\n      D-0_ROC\n      D-0_PPO\n      D-0_PVO\n      Label\n    \n  \n  \n    \n      0\n      050120\n      2017-01-25\n      0.0\n      0.000021\n      0.0\n      0.001773\n      0.0\n      -0.000586\n      0.0\n      -0.000619\n      ...\n      0.000374\n      0.0\n      0.004017\n      -0.024822\n      0.0\n      0.0\n      -0.011074\n      0.0\n      0.000174\n      0\n    \n    \n      1\n      050120\n      2017-05-18\n      0.0\n      0.000238\n      0.0\n      -0.032219\n      0.0\n      -0.000341\n      0.0\n      -0.000356\n      ...\n      0.000017\n      0.0\n      0.003150\n      -0.035779\n      0.0\n      0.0\n      0.019929\n      0.0\n      0.000174\n      0\n    \n    \n      2\n      050120\n      2017-05-19\n      0.0\n      0.000282\n      0.0\n      0.001756\n      0.0\n      0.000070\n      0.0\n      -0.000222\n      ...\n      0.000374\n      0.0\n      0.004979\n      0.058416\n      0.0\n      0.0\n      0.019747\n      0.0\n      0.000174\n      0\n    \n    \n      3\n      050120\n      2017-05-22\n      0.0\n      0.000177\n      0.0\n      0.001917\n      0.0\n      0.000153\n      0.0\n      -0.000356\n      ...\n      0.000374\n      0.0\n      0.003013\n      0.243910\n      0.0\n      0.0\n      0.019765\n      0.0\n      0.000029\n      0\n    \n    \n      4\n      050120\n      2017-05-23\n      0.0\n      -0.000847\n      0.0\n      0.002323\n      0.0\n      0.001116\n      0.0\n      -0.001705\n      ...\n      0.000856\n      0.0\n      0.002451\n      -0.002878\n      0.0\n      0.0\n      0.024483\n      0.0\n      0.000029\n      0\n    \n  \n\n5 rows × 563 columns\n\n\n\n\n모든 dataframe 저장\n\n\n#collapse-hide\nimport pickle \ndict_all_dataset = {1: [df_data_1, df_shap_values_1],\n                  2: [df_data_2, df_shap_values_2],\n                  3: [df_data_3, df_shap_values_3]}\n\nwith open(f'all_dataset', 'wb') as handle:\n    pickle.dump(dict_all_dataset, handle, protocol=pickle.HIGHEST_PROTOCOL)\n\n \n\n\n\n\n2차원 시각화는 CCI 구간별, 연도별로 데이터를 분할하여 진행한다. 이 때 원본 주가 데이터셋과 SHAP 표준화 데이터셋을 같이 시각화하여 군집이 형성되는 형태를 비교한다.\n데이터 분할에 사용되는 CCI 구간, 연도는 다음과 같다.\n*CCI 구간\n1) 중립구간 - CCI : (-20, 20)\n2) 과열구간 - CCI : (100, \\(\\infty\\))\n3) 침체 구간 - CCI : (-\\(\\infty\\),-100)\n*연도\n1) 2019년\n2) 2020년\n3) 2021년\n\nt-SNE 2차원 시각화 함수 생성\n\n원본 주가 데이터셋과 SHAP 표준화 데이터셋, 연도, cci 구간 정보를 넣어주면, 두 데이터셋에 대한 2차원 시각화 결과를 반환하는 함수를 생성한다. 시간 단축을 위해 최초 실행할 때 tsne_load=False로 설정하여 tsne 데이터를 저장하고, tsne_loae=True로 바꾸어 바로 시각화 할 수 있도록 한다. 2차원 시각화 할 때는 label별로 색깔을 다르게 지정하여 특정 label의 분포가 집중되어 있는 군집이 존재하는지 확인한다.\n\ndef tsne_visualization(dataset, shap_dataset, year, cci_type, tsne_load=False, alpha=0.4, size=3):\n    \"\"\"\n    dataset: pd.DataFrame() / 원본 주가 데이터셋 \n    shap_dataset: pd.DataFrame() / SHAP 표준화 데이터셋\n    year: Int / 시각화 할 연도 \n    cci_type: Int / 1(중립구간), 2(과열구간), 3(침체구간) - 파일 저장 및 로드를 위함 \n    tsne_load(default:True): Boolean / False: 최초 실행 시 tsne data 생성 후 저장, True: 저장된 tsne data를 불러와서 시각화 - 시간 절약을 위함 \n    alpha(default:0.4): Float / 투명도 \n    size=3(default:3): Int / 점 크기 \n    \"\"\"\n    \n    from sklearn.manifold import TSNE\n    import pickle \n    plt.rcParams['axes.unicode_minus'] = False\n    plt.rc('font', family='NanumGothic')\n    # plt.style.use('default')\n    \n    fig = plt.figure(figsize=(15, 5))\n    ax1, ax2 = fig.subplots(1, 2)\n    \n    ##### 원본 데이터 #####\n    dataset_year = dataset[(dataset['Date'] >= f'{year}-01-01') & (dataset['Date'] <= f'{year}-12-31')].reset_index(drop=True)\n    \n    if not tsne_load:\n        np_tsne = TSNE(n_components=2, random_state=42).fit_transform(dataset_year.drop(columns=['Code', 'Date', 'Label'])) # 2차원 t-sne 임베딩  \n        # save np_tsne\n        with open(f'np_tsne_{year}_{cci_type}', 'wb') as handle:\n            pickle.dump(np_tsne, handle, protocol=pickle.HIGHEST_PROTOCOL)\n    else: \n        # load np_tsne\n        with open(f'np_tsne_{year}_{cci_type}', 'rb') as handle:\n            np_tsne = pickle.load(handle)\n    \n    df_tsne = pd.DataFrame(np_tsne, columns=['component0', 'component1']) # numpy array -> Dataframe     \n    df_tsne['Label'] = dataset_year['Label'] # Label 정보 불러오기\n\n    # Label 별 분리 \n    df_tsne_0 = df_tsne[df_tsne['Label']==0]\n    df_tsne_1 = df_tsne[df_tsne['Label']==1]\n\n    # Label 별 시각화 \n    ax1.scatter(df_tsne_0['component0'], df_tsne_0['component1'], color = 'green', label = 'Label 0', alpha=alpha, s=size)\n    ax1.scatter(df_tsne_1['component0'], df_tsne_1['component1'], color = 'pink', label = 'Label 1', alpha=alpha, s=size)\n\n    ax1.set_title('원본 데이터셋')\n    ax1.set_xlabel('component 0')\n    ax1.set_ylabel('component 1')\n    ax1.legend()\n    \n    ##### SHAP transform data #####\n    shap_dataset_year = shap_dataset[(shap_dataset['Date'] >= f'{year}-01-01') & (shap_dataset['Date'] <= f'{year}-12-31')].reset_index(drop=True)\n    \n    if not tsne_load:\n        np_tsne_shap = TSNE(n_components=2, random_state=42).fit_transform(shap_dataset_year.drop(columns=['Code', 'Date', 'Label'])) # 2차원 t-sne 임베딩  \n        # save np_tsne\n        with open(f'np_tsne_shap_{year}_{cci_type}', 'wb') as handle:\n            pickle.dump(np_tsne_shap, handle, protocol=pickle.HIGHEST_PROTOCOL)\n    else: \n        # load np_tsne\n        with open(f'np_tsne_shap_{year}_{cci_type}', 'rb') as handle:\n            np_tsne_shap = pickle.load(handle)\n   \n    df_tsne_shap = pd.DataFrame(np_tsne_shap, columns=['component0', 'component1']) # numpy array -> Dataframe     \n    df_tsne_shap['Label'] = shap_dataset_year['Label'] # Label 정보 불러오기\n\n    # Label 별 분리 \n    df_tsne_shap_0 = df_tsne_shap[df_tsne_shap['Label']==0]\n    df_tsne_shap_1 = df_tsne_shap[df_tsne_shap['Label']==1]\n\n    # Label 별 시각화 \n    ax2.scatter(df_tsne_shap_0['component0'], df_tsne_shap_0['component1'], color = 'green', label = 'Label 0', alpha=alpha, s=size)\n    ax2.scatter(df_tsne_shap_1['component0'], df_tsne_shap_1['component1'], color = 'pink', label = 'Label 1', alpha=alpha, s=size)\n\n    ax2.set_title('SHAP transform 데이터셋')\n    ax2.set_xlabel('component 0')\n    ax2.set_ylabel('component 1')\n    ax2.legend()   \n    \n    \n    plt.show()\n\n\nCCI 구간 별, 연도 별 t-SNE 시각화를 진행한다. 원본 데이터셋과 SHAP transform 데이터셋의 그림을 비교하여 집단 형성이 이루어지는지 확인한다.\n\n\n\n\n\n\ntsne_visualization(dataset=df_data_1, shap_dataset=df_shap_values_1, year=2019, cci_type=1, tsne_load=True, alpha=0.5, size=3)\n\n\n\n\n\n\n\n\ntsne_visualization(dataset=df_data_1, shap_dataset=df_shap_values_1, year=2020, cci_type=1, tsne_load=True, alpha=0.5, size=3)\n\n\n\n\n\n\n\n\ntsne_visualization(dataset=df_data_1, shap_dataset=df_shap_values_1, year=2021, cci_type=1, tsne_load=True, alpha=0.5, size=3)\n\n\n\n\n\n\n\n\n\n\n\n\ntsne_visualization(dataset=df_data_2, shap_dataset=df_shap_values_2, year=2019, cci_type=2, tsne_load=True, alpha=0.4, size=3)\n\n\n\n\n\n\n\n\ntsne_visualization(dataset=df_data_2, shap_dataset=df_shap_values_2, year=2020, cci_type=2, tsne_load=True, alpha=0.4, size=3)\n\n\n\n\n\n\n\n\ntsne_visualization(dataset=df_data_2, shap_dataset=df_shap_values_2, year=2021, cci_type=2, tsne_load=True, alpha=0.4, size=3)\n\n\n\n\n\n\n\n\n\n\n\n\ntsne_visualization(dataset=df_data_3, shap_dataset=df_shap_values_3, year=2019, cci_type=3, tsne_load=True, alpha=0.4, size=3)\n\n\n\n\n\n\n\n\ntsne_visualization(dataset=df_data_3, shap_dataset=df_shap_values_3, year=2020, cci_type=3, tsne_load=True, alpha=0.4, size=3)\n\n\n\n\n\n\n\n\ntsne_visualization(dataset=df_data_3, shap_dataset=df_shap_values_3, year=2021, cci_type=3, tsne_load=True, alpha=0.4, size=3)\n\n\n\n\n모든 CCI 구간, 연도에서 원본 주가 데이터셋 보다 SHAP 표준화 데이터셋의 군집의 경계가 더 명확하게 드러났으며, 특정 label의 분포 또한 집중되어있는 군집이 존재함을 확인할 수 있다. 이는 SHAP 표준화 데이터셋을 사용함으로써, 예측 모델의 중요 변수를 상대적으로 가중하게되어 데이터에서의 내재되어있는 패턴이 강조되는 효과가 나타난 것으로 해석된다. 본 글에서 군집이 나누어지는 것을 확인하였다면, 다음 글에서는 SHAP 표준화 데이터셋에 대하여 실제로 클러스터링 알고리즘을 적용하여 군집을 명시적으로 분류하는 시간을 가진다."
  },
  {
    "objectID": "posts/08.cluster_filtering.html",
    "href": "posts/08.cluster_filtering.html",
    "title": "[stock prediction] 3.3 클러스터 탐색을 통한 주가상승 패턴 검출",
    "section": "",
    "text": "지난 글 ( 3.2 t-SNE를 사용한 주가데이터 2차원 시각화 ) 에서는 원본 주가데이터셋과 SHAP 표준화 데이터셋에 대한 t-SNE 2차원 산점도 시각화를 수행하여 SHAP 표준화 데이터셋의 군집의 경계가 더 명확하게 드러남을 확인하였다. 이번 글에서는 SHAP 표준화 데이터셋에 계층적 클러스터링 알고리즘을 적용하여 명시적으로 군집을 분류해보고, label1의 비율로 군집을 필터링하여 상승 추세 군집을 선택한다. 선택된 상승 추세 군집에서 빈도수 상위 날짜들의 개별 종목 차트를 확인하여 공통된 패턴을 검출한다.\n\n\n\n\n계층적 클러스터링 & 상승 추세 군집 선택\n\n\n\n정리\n\n라이브러리 import\n\n\nimport pandas as pd\nimport numpy as np\nfrom tqdm import tqdm\nimport pymysql\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom ipywidgets import interact, interact_manual\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.graph_objects as go\nimport plotly.offline as pyo\n%matplotlib inline\n%pylab inline\npylab.rcParams['figure.figsize'] = (12,5)\nplt.rcParams['axes.unicode_minus'] = False\nplt.rc('font', family='NanumGothic')\n\nimport StockFunc as sf\n\n%pylab is deprecated, use %matplotlib inline and import the required libraries.\nPopulating the interactive namespace from numpy and matplotlib\n\n\n \n\n\n\n\n상승 추세 군집이란 기준일 (D0) 대비 다음 날 (D+1) 종가 2% 이상 상승한 데이터들이 일정 비율 이상 속하는 군집들을 의미한다. 계층적 클러스터링을 통해 생성된 군집들을 label1의 비율로 필터링하여 상승 추세 군집을 선택한다.\n\nlinkage, 덴드로그램 시각화 함수\n\n계층적 클러스터링에서의 덴드로그램 시각화를 수행하여 트리의 높이를 결정할 수 있도록 한다.\n\ndef hierarchical_clustering_plot(method, year, cci_type, dendrogram=False, n_clusters=5, min_samples=5, alpha=0.3, size=4):\n    '''\n    method: str / complete, average, ward\n    year: int / 2019, 2020, 2021\n    cci_type:int / 1, 2, 3\n    dendrogram:Boolean / True, False(default) - 시간이 오래걸리므로 선택 \n    n_clusters: int / default:5\n    min_samples: int / default:5\n    alpha: float / default: 0.3\n    size: int / default: 4\n    '''\n\n    import pickle # tsne 파일 불러오기 \n    with open(f'np_tsne_shap_{year}_{cci_type}', 'rb') as handle: \n        np_tsne = pickle.load(handle)\n    \n    if method in ('complete', 'average', 'ward'): # linkage method 선택 \n        from scipy.cluster.hierarchy import linkage, dendrogram\n        import matplotlib.pyplot as plt\n        clusters = linkage(y=np_tsne, method=method, metric='euclidean')\n        print(\"linkage complete\")\n        \n        if dendrogram: # True: 덴드로그램 시각화 \n            plt.title(f\"{year} dendrogram\", fontsize=15)\n            dendrogram(clusters, leaf_rotation=90, leaf_font_size=12,)\n            plt.show() \n            \n        return clusters \n        \n    else:\n        print(\"method를 잘못 입력하였습니다.\")\n\n\n계층적 클러스터링, 상승 추세 군집 선택 시각화\n\n덴드로그램을 참고하여 40 ~ 60개의 군집이 형성되도록 t를 설정한다. 상승 추세 군집을 선택하기 위해 적절한 ratio(특정 집단에서의 label1의 비율)값을 지정해주는데, 군집의 개수는 4 ~ 10개, 데이터의 개수는 3000 ~ 6000개 사이가 되도록 설정한다.\n\ndef fcluster_plot_and_filtering(np_clusters, year, cci_type, t=30, ratio=0.5, alpha=0.3, size=4, xlim=70, ylim=70):\n    '''\n    np_clusters: np.array\n    year: int / 2019, 2020, 2021\n    cci_type: int / 1, 2, 3 \n    t: int / default: 30 (덴드로그램 트리의 높이) \n    ratio: float / default:0.5 (1의 비율이 지정)  \n    '''\n    \n    from scipy.cluster.hierarchy import fcluster # 지정한 클러스터 자르기\n    import pickle     \n    \n    \n    with open('all_dataset', 'rb') as handle: # Code, Date, Label 정보가 모두 들어있는 데이터셋 불러오기\n        dict_all_dataset = pickle.load(handle)\n        \n    with open(f'np_tsne_shap_{year}_{cci_type}', 'rb') as handle: # 연도, CCI 구간에 맞는 tsne 데이터셋 불러오기\n        np_tsne = pickle.load(handle)\n    \n    df_shap_cci = dict_all_dataset[cci_type][1]\n    df_shap_year = df_shap_cci[(df_shap_cci['Date'] >= f'{year}-01-01') & (df_shap_cci['Date'] <= f'{year}-12-31')].reset_index(drop=True) # 연도에 맞는 데이터 필터링\n    \n    cut_tree = fcluster(np_clusters, t=t, criterion='distance') # 군집화 결과 데이터 \n    print(\"군집의 개수:\", len(pd.DataFrame(cut_tree)[0].unique())) # 군집의 개수 출력 \n    \n    ##### 클러스터링 시각화 \n    fig = plt.figure(figsize=(15, 5))\n    ax1, ax2 = fig.subplots(1, 2)\n        \n    scatter = ax1.scatter(x=np_tsne[:, 0], y=np_tsne[:, 1], c=cut_tree, cmap='gist_rainbow', alpha=alpha, s=size) # 군집(cut_tree)별로 시각화 \n    ax1.legend(*scatter.legend_elements())\n    ax1.set_title(f\"{year} Hierarchical Clustering\", fontsize=15)\n    \n    ##### 라벨 1의 비율을 사용한 클러스터 필터링: 상승 추세 군집 선택\n    df_tsne = pd.DataFrame(np_tsne, columns=['component1', 'component2'])\n    df_tsne['Code'], df_tsne['Date'], df_tsne['Label'], df_tsne['Cluster'] = df_shap_year['Code'], df_shap_year['Date'], df_shap_year['Label'], cut_tree\n    \n    gb = df_tsne.groupby('Cluster')['Label'].value_counts(sort=False).unstack() # 군집 별 라벨 개수\n    idx_label_1 = gb[gb[1]/(gb[0]+gb[1]) > ratio].index # label 1의 비율이 ratio 이상인 군집 번호\n    print(f'label 1 > {ratio} 군집 번호: ', idx_label_1)\n    df_tsne_1 = df_tsne[df_tsne['Cluster'].isin(idx_label_1)] # 라벨 1의 비율이 ratio 이상인 군집 추출 (상승 추세 군집)\n    print(\"데이터의 개수:\", len(df_tsne_1))\n    print(\"종목의 종류:\", df_tsne_1['Code'].nunique(), \" | \", \"날짜의 종류: \", df_tsne_1['Date'].nunique())\n    \n    ##### 상승 추세 군집 시각화 \n    ax2.set_title(f\"label 1 > {ratio}\", fontsize=15)\n    scatter = ax2.scatter(df_tsne_1['component1'],df_tsne_1['component2'],c=df_tsne_1['Cluster'], cmap='gist_rainbow', s=3, alpha=0.4)\n    ax2.legend(*scatter.legend_elements())\n    ax2.set_ylim(-ylim, ylim) # tsne 범위와 맞추기\n    ax2.set_xlim(-xlim, xlim)\n        \n    return df_shap_year, df_tsne_1\n\n\n주 컬럼 생성 및 빈도수 상위 주 시각화 함수\n\n[주 컬럼 생성 참고]\n-(Python) 그 날짜가 몇 주째인지 계산하기\n\ndef visualization_week(data_list, week_num=3, day_num=5): # 빈도수 상위 주 시각화 \n    ##### 주 컬럼 생성\n    def get_week_no(target):  \n        from datetime import timedelta\n\n        firstday = target.replace(day=1)\n\n        if firstday.weekday() == 6:\n            origin = firstday\n        elif firstday.weekday() < 3:\n            origin = firstday - timedelta(days=firstday.weekday() + 1)\n        else:\n            origin = firstday + timedelta(days=6-firstday.weekday())\n\n        return f'{target.month}월 {(target - origin).days // 7 + 1}주차'\n\n    mpl.rcParams['font.family'] = 'NanumSquare'    \n    fig = plt.figure(figsize=(11, 5))\n    ax = fig.subplots(2, 3)\n    lst_year = [2019, 2020, 2021]\n\n    ##### 연도 별 상위 빈도수 주, 날짜 시각화 \n    for i, data in enumerate(data_list): # 연도 별 데이터셋 \n        data['Date'] = pd.to_datetime(data['Date']).dt.date # datetime type 변경 \n        data['date_month_week'] = data['Date'].apply(get_week_no) # xx월 xx주차 컬럼 생성 \n        df_week = pd.DataFrame(data['date_month_week'].value_counts().head(week_num)).reset_index().rename(columns={'index':'month-week', 'date_month_week':'count'}) # 빈도수 상위 주차 5개 \n        df_day = pd.DataFrame(data['Date'].value_counts().head(day_num)).reset_index().rename(columns={'index':'Date', 'Date':'count'}) # 빈도수 상위 주차 5개 \n        \n        ax[0, i].set_title(f\"<{lst_year[i]}년 빈도수 상위 주>\")\n        sns.barplot(data=df_week, x='count', y='month-week', palette=\"Pastel1\", ax=ax[0, i])\n\n        for height, p in enumerate(ax[0, i].patches):\n            width = p.get_width()\n            ax[0, i].text(width, height+0.2, f'{round(p.get_width())}', ha = 'center', size = 13)\n\n        ax[1, i].set_title(f\"<{lst_year[i]}년 빈도수 상위 날짜>\")\n        sns.barplot(data=df_day, x='count', y='Date', palette=\"Pastel1\", ax=ax[1, i])\n\n        for height, p in enumerate(ax[1, i].patches):\n            width = p.get_width()\n            ax[1, i].text(width , height+0.2, f'{round(p.get_width())}', ha = 'center', size = 13)\n    \n    plt.xlabel(\"\")\n    plt.ylabel(\"\")\n    \n    plt.tight_layout()\n    plt.show()\n\n\n\n\n\n\n\n덴드로그램\n\n\nclusters_2019_1 = hierarchical_clustering_plot(method='average', year=2019, cci_type=1, dendrogram=True)\n\nlinkage complete\n\n\n\n\n\n\n계층적 클러스터링 & 상승 추세 군집 선택\n\n\ndf_shap_2019_1, df_tsne_2019_1_1 = fcluster_plot_and_filtering(clusters_2019_1, 2019, 1, t=13, ratio=0.22, xlim=80, ylim=80)\n\n군집의 개수: 53\nlabel 1 > 0.22 군집 번호:  Int64Index([21, 23, 24, 26, 27, 36, 47, 49], dtype='int64', name='Cluster')\n데이터의 개수: 4313\n종목의 종류: 1024  |  날짜의 종류:  246\n\n\n\n\n\n\n\n\n\n\n덴드로그램\n\n\nclusters_2020_1 = hierarchical_clustering_plot(method='average', year=2020, cci_type=1, dendrogram=True)\n\nlinkage complete\n\n\n\n\n\n\n계층적 클러스터링 & 상승 추세 군집 선택\n\n\ndf_shap_2020_1, df_tsne_2020_1_1 = fcluster_plot_and_filtering(clusters_2020_1, 2020, 1, t=13, ratio=0.29, xlim=80, ylim=80)\n\n군집의 개수: 53\nlabel 1 > 0.29 군집 번호:  Int64Index([24, 26, 27, 30, 38, 40, 41, 42, 43], dtype='int64', name='Cluster')\n데이터의 개수: 4827\n종목의 종류: 1306  |  날짜의 종류:  247\n\n\n\n\n\n\n\n\n\n\n덴드로그램\n\n\nclusters_2021_1 = hierarchical_clustering_plot(method='average', year=2021, cci_type=1, dendrogram=True)\n\nlinkage complete\n\n\n\n\n\n\n계층적 클러스터링 & 상승 추세 군집 선택\n\n\ndf_shap_2021_1, df_tsne_2021_1_1 = fcluster_plot_and_filtering(clusters_2021_1, 2021, 1, t=13, ratio=0.207)\n\n군집의 개수: 51\nlabel 1 > 0.207 군집 번호:  Int64Index([3, 4, 13, 18, 21, 22, 24, 25], dtype='int64', name='Cluster')\n데이터의 개수: 4237\n종목의 종류: 1092  |  날짜의 종류:  237\n\n\n\n\n\n\n\n\n\n\n연도별 빈도수 상위 주 & 날짜\n\n\nlst_data = [df_tsne_2019_1_1, df_tsne_2020_1_1, df_tsne_2021_1_1]\nvisualization_week(data_list=lst_data, week_num=5, day_num=5)\n\n\n\n\n중립구간에서 상승 추세 군집 데이터의 날짜 빈도수는 2019년도는 8월 3주차, 2020년도는 4월 1주차, 2021년도는 2월 1주차가 가장 높게 나왔다. 그 중에서도 가장 큰 날짜 빈도 차이를 보인 연도는 2020년이었다.\n\n\n연도별 상위 날짜의 개별 종목 차트 확인\n\n\n연도별 상위 날짜의 랜덤 종목코드 데이터프레임 생성\n\n상위 날짜들을 바꿔가며 실행하고, 랜덤으로 나오는 종목코드들의 개별 종목 차트를 확인한다.\n\n#collapse-hide\nimport datetime\ndf_tsne_2019_1_1['Date'] = df_tsne_2019_1_1['Date'].astype(str)\ndf_tsne_2020_1_1['Date'] = df_tsne_2020_1_1['Date'].astype(str)\ndf_tsne_2021_1_1['Date'] = df_tsne_2021_1_1['Date'].astype(str)\n\ndf1 = df_tsne_2019_1_1.loc[df_tsne_2019_1_1['Date'] == '2019-08-20', ['Date', 'Code']].sample(5).reset_index(drop=True)\ndf2 = df_tsne_2020_1_1.loc[df_tsne_2020_1_1['Date'] == '2020-04-01', ['Date', 'Code']].sample(5).reset_index(drop=True)\ndf3 = df_tsne_2021_1_1.loc[df_tsne_2021_1_1['Date'] == '2021-02-01', ['Date', 'Code']].sample(5).reset_index(drop=True)\n\npd.concat([df1, df2, df3], axis=1)\n\n\n\n\n\n  \n    \n      \n      Date\n      Code\n      Date\n      Code\n      Date\n      Code\n    \n  \n  \n    \n      0\n      2019-08-20\n      007540\n      2020-04-01\n      160980\n      2021-02-01\n      023800\n    \n    \n      1\n      2019-08-20\n      047400\n      2020-04-01\n      119850\n      2021-02-01\n      033240\n    \n    \n      2\n      2019-08-20\n      010820\n      2020-04-01\n      041190\n      2021-02-01\n      038110\n    \n    \n      3\n      2019-08-20\n      020120\n      2020-04-01\n      024110\n      2021-02-01\n      039020\n    \n    \n      4\n      2019-08-20\n      081150\n      2020-04-01\n      002900\n      2021-02-01\n      001810\n    \n  \n\n\n\n\n\n차트 확인 예시\n\n2019년 8월 20일\n\n2020년 4월 1일\n\n2021년 2월 1일\n\n여러개의 차트 확인을 해보았을 때, 가장 큰 날짜 편중을 보였던 2020년도는 차트의 패턴이 가장 일정하게 나타났으며, 2019,2021년도는 2020년도 만큼 일정한 패턴을 보이지는 않았다. 하지만 세 연도에서 공통적으로 나오는 패턴이 존재했다. 해당 패턴을 분석해보았을 때, 위의 세 사진과 같이, 하락 추세에서 상승 추세로 전환되는 V자 형태였으며, 그 중에서도 기준일(D0)[회색 선]이 오른쪽에 위치한다는 공통점이 있었다.\n \n\n\n\n\n\n\n\n덴드로그램\n\n\nclusters_2019_2 = hierarchical_clustering_plot(method='average', year=2019, cci_type=2, dendrogram=True)\n\nlinkage complete\n\n\n\n\n\n\n계층적 클러스터링 & 상승 추세 군집 선택\n\n\ndf_shap_2019_2, df_tsne_2019_1_2 = fcluster_plot_and_filtering(clusters_2019_2, 2019, 2, t=12, ratio=0.25)\n\n군집의 개수: 48\nlabel 1 > 0.25 군집 번호:  Int64Index([8, 16, 17, 21, 30], dtype='int64', name='Cluster')\n데이터의 개수: 5016\n종목의 종류: 1023  |  날짜의 종류:  246\n\n\n\n\n\n\n\n\n\n\n덴드로그램\n\n\nclusters_2020_2 = hierarchical_clustering_plot(method='average', year=2020, cci_type=2, dendrogram=True)\n\nlinkage complete\n\n\n\n\n\n\n계층적 클러스터링 & 상승 추세 군집 선택\n\n\ndf_shap_2020_2, df_tsne_2020_1_2 = fcluster_plot_and_filtering(clusters_2020_2, 2020, 2, t=11, ratio=0.31, xlim=65, ylim=65)\n\n군집의 개수: 55\nlabel 1 > 0.31 군집 번호:  Int64Index([1, 14, 16, 22, 36, 39], dtype='int64', name='Cluster')\n데이터의 개수: 5457\n종목의 종류: 1190  |  날짜의 종류:  247\n\n\n\n\n\n\n\n\n\n\n덴드로그램\n\n\nclusters_2021_2 = hierarchical_clustering_plot(method='average', year=2021, cci_type=2, dendrogram=True)\n\nlinkage complete\n\n\n\n\n\n\n계층적 클러스터링 & 상승 추세 군집 선택\n\n\ndf_shap_2021_2, df_tsne_2021_1_2 = fcluster_plot_and_filtering(clusters_2021_2, 2021, 2, t=12, ratio=0.268)\n\n군집의 개수: 52\nlabel 1 > 0.268 군집 번호:  Int64Index([5, 9, 14, 16, 31, 32, 43, 51], dtype='int64', name='Cluster')\n데이터의 개수: 5632\n종목의 종류: 1110  |  날짜의 종류:  237\n\n\n\n\n\n\n\n\n\n\n연도별 빈도수 상위 주 & 날짜\n\n\nlst_data = [df_tsne_2019_1_2, df_tsne_2020_1_2, df_tsne_2021_1_2]\nvisualization_week(data_list=lst_data, week_num=5, day_num=5)\n\n\n\n\n과매수구간에서 빈도수가 높은 날짜는 2019년 1월 4주차, 2020년 4월 3주차, 2021년 1월 3주차임을 알 수 있 습니다. 다른 CCI 구간에 비해 상위 빈도수의 크기 차이가 많지 않았다. 가장 높은 날짜 빈도 차이를 보이는 연도는 중립구간과 마찬가지로 2020년도였다.\n\n\n연도별 상위 날짜의 개별 종목 차트 확인\n\n\n연도별 상위 날짜의 랜덤 종목코드 데이터프레임 생성\n\n상위 날짜들을 바꿔가며 실행하고, 랜덤으로 나오는 종목코드들의 개별 종목 차트를 확인한다.\n\n#collapse-hide\nimport datetime\ndf_tsne_2019_1_2['Date'] = df_tsne_2019_1_2['Date'].astype(str)\ndf_tsne_2020_1_2['Date'] = df_tsne_2020_1_2['Date'].astype(str)\ndf_tsne_2021_1_2['Date'] = df_tsne_2021_1_2['Date'].astype(str)\n\ndf1 = df_tsne_2019_1_2.loc[df_tsne_2019_1_2['Date'] == '2019-01-17', ['Date', 'Code']].sample(5).reset_index(drop=True)\ndf2 = df_tsne_2020_1_2.loc[df_tsne_2020_1_2['Date'] == '2020-04-17', ['Date', 'Code']].sample(5).reset_index(drop=True)\ndf3 = df_tsne_2021_1_2.loc[df_tsne_2021_1_2['Date'] == '2021-01-20', ['Date', 'Code']].sample(5).reset_index(drop=True)\n\npd.concat([df1, df2, df3], axis=1)\n\n\n\n\n\n  \n    \n      \n      Date\n      Code\n      Date\n      Code\n      Date\n      Code\n    \n  \n  \n    \n      0\n      2019-01-17\n      042370\n      2020-04-17\n      008250\n      2021-01-20\n      095340\n    \n    \n      1\n      2019-01-17\n      187220\n      2020-04-17\n      017550\n      2021-01-20\n      025880\n    \n    \n      2\n      2019-01-17\n      024900\n      2020-04-17\n      035620\n      2021-01-20\n      011500\n    \n    \n      3\n      2019-01-17\n      106240\n      2020-04-17\n      011370\n      2021-01-20\n      010690\n    \n    \n      4\n      2019-01-17\n      007390\n      2020-04-17\n      048430\n      2021-01-20\n      000270\n    \n  \n\n\n\n\n\n차트 확인 예시\n\n2019년 1월 17일\n\n2020년 4월 17일\n\n2021년 1월 20일\n\n과매수구간은 20일 이동평균선의 위에 극단적으로 떨어져있는 데이터들이므로, 기준일(D0)[회색 선]이 상승추세에서의 중간 ~ 끝 무렵에 위치하였다. 다른 CCI 구간보다 패턴이 가장 불규칙적이었지만, 하락추세에서 상승추세로 전환되는 V자 형태에서 상승추세 끝무렵에 위치하는 공통된 패턴을 일부 데이터에서 검출할 수 있었다.\n \n\n\n\n\n\n\n\n덴드로그램\n\n\nclusters_2019_3 = hierarchical_clustering_plot(method='average', year=2019, cci_type=3, dendrogram=True)\n\nlinkage complete\n\n\n\n\n\n\n계층적 클러스터링 & 상승 추세 군집 선택\n\n\ndf_shap_2019_3, df_tsne_2019_1_3 = fcluster_plot_and_filtering(clusters_2019_3, 2019, 3, t=10, ratio=0.365)\n\n군집의 개수: 58\nlabel 1 > 0.365 군집 번호:  Int64Index([1, 2, 25, 26, 50], dtype='int64', name='Cluster')\n데이터의 개수: 3959\n종목의 종류: 994  |  날짜의 종류:  222\n\n\n\n\n\n\n\n\n\n\n덴드로그램\n\n\nclusters_2020_3 = hierarchical_clustering_plot(method='average', year=2020, cci_type=3, dendrogram=True)\n\nlinkage complete\n\n\n\n\n\n\n계층적 클러스터링 & 상승 추세 군집 선택\n\n\ndf_shap_2020_3, df_tsne_2020_1_3 = fcluster_plot_and_filtering(clusters_2020_3, 2020, 3, t=12, ratio=0.5, xlim=80, ylim=80)\n\n군집의 개수: 47\nlabel 1 > 0.5 군집 번호:  Int64Index([2, 3, 15, 28, 37], dtype='int64', name='Cluster')\n데이터의 개수: 3874\n종목의 종류: 1356  |  날짜의 종류:  78\n\n\n\n\n\n\n\n\n\n\n덴드로그램\n\n\nclusters_2021_3 = hierarchical_clustering_plot(method='average', year=2021, cci_type=3, dendrogram=True)\n\nlinkage complete\n\n\n\n\n\n\n계층적 클러스터링 & 상승 추세 군집 선택\n\n\ndf_shap_2021_3, df_tsne_2021_1_3 = fcluster_plot_and_filtering(clusters_2021_3, 2021, 3, t=10, ratio=0.35)\n\n군집의 개수: 53\nlabel 1 > 0.35 군집 번호:  Int64Index([5, 7, 10, 15], dtype='int64', name='Cluster')\n데이터의 개수: 4174\n종목의 종류: 1163  |  날짜의 종류:  230\n\n\n\n\n\n\n\n\n\n\n연도별 빈도수 상위 주 & 날짜\n\n\nlst_data = [df_tsne_2019_1_3, df_tsne_2020_1_3, df_tsne_2021_1_3]\nvisualization_week(data_list=lst_data, week_num=5, day_num=5)\n\n\n\n\n과매도구간의 빈도수 상위 날짜는 2019년 8월 1주차, 2020년 3월 4주차, 2021년 10월 1주차였다. CCI 구간 중 유일하게 모든 연도에서 높은 날짜 빈도 차이를 보였다.\n\n\n연도별 상위 날짜의 개별 종목 차트 확인\n\n\n연도별 상위 날짜의 랜덤 종목코드 데이터프레임 생성\n\n상위 날짜들을 바꿔가며 실행하고, 랜덤으로 나오는 종목코드들의 개별 종목 차트를 확인한다.\n\n#collapse-hide\nimport datetime\ndf_tsne_2019_1_3['Date'] = df_tsne_2019_1_3['Date'].astype(str)\ndf_tsne_2020_1_3['Date'] = df_tsne_2020_1_3['Date'].astype(str)\ndf_tsne_2021_1_3['Date'] = df_tsne_2021_1_3['Date'].astype(str)\n\ndf1 = df_tsne_2019_1_3.loc[df_tsne_2019_1_3['Date'] == '2019-08-06', ['Date', 'Code']].sample(5).reset_index(drop=True)\ndf2 = df_tsne_2020_1_3.loc[df_tsne_2020_1_3['Date'] == '2020-03-19', ['Date', 'Code']].sample(5).reset_index(drop=True)\ndf3 = df_tsne_2021_1_3.loc[df_tsne_2021_1_3['Date'] == '2021-10-07', ['Date', 'Code']].sample(5).reset_index(drop=True)\n\npd.concat([df1, df2, df3], axis=1)\n\n\n\n\n\n  \n    \n      \n      Date\n      Code\n      Date\n      Code\n      Date\n      Code\n    \n  \n  \n    \n      0\n      2019-08-06\n      033100\n      2020-03-19\n      126880\n      2021-10-07\n      028300\n    \n    \n      1\n      2019-08-06\n      060480\n      2020-03-19\n      000850\n      2021-10-07\n      088130\n    \n    \n      2\n      2019-08-06\n      014190\n      2020-03-19\n      170920\n      2021-10-07\n      042110\n    \n    \n      3\n      2019-08-06\n      123860\n      2020-03-19\n      189860\n      2021-10-07\n      081000\n    \n    \n      4\n      2019-08-06\n      067990\n      2020-03-19\n      079160\n      2021-10-07\n      002690\n    \n  \n\n\n\n\n\n차트 확인 예시\n\n2019년 8월 6일\n\n2020년 3월 19일\n\n2021년 10월 17일\n\n위의 사진과 같이, 모든 연도에서 비슷한 패턴을 보였다. 하락추세에서 상승추세로 전환되는 V자 형태의 공통된 패턴에서, 기준일(D0)[회색 선]이 꼭지점 근처에 위치하였다.\n \n\n\n\n\n\n지금까지 분류 머신러닝 모델학습, SHAP 표준화, 클러스터링 분석의 과정을 거쳐 10일 간의 주가 시계열 데이터로부터 주가 상승 추세 패턴을 검출하였다.\nCCI 구간 별, 연도 별로 나누어 연구를 진행하였는데,\n1) CCI 구간 별 공통적으로 나타나는 패턴은 하락추세에서 상승추세로 전환되는 V자 상승반전형 패턴이었으며, 기준일(D0)의 위치가 CCI 구간에 따라 다르게 나타났다. 특히나 과매도구간에서의 패턴이 다른 구간에 비해 가장 정확하고 유사한 것을 확인할 수 있었다.\n2) 모든 CCI 구간을 통틀어 2019, 2020, 2021년도 중 높은 빈도로 나타나는 날짜의 편중이 가장 큰 연도는 2020년이었으며, 2020년도에서 패턴의 모양 또한 가장 유사하게 나타났다."
  },
  {
    "objectID": "posts/c01.EDA.html#eda",
    "href": "posts/c01.EDA.html#eda",
    "title": "[cropdoctor] 1. 노지작물 데이터 EDA",
    "section": "1. EDA",
    "text": "1. EDA\n\n사용 데이터셋\n노지 작물 질병 진단 이미지\n인공지능 기반의 웹 서비스 개발 프로젝트에서 작물의 질병을 진단 주제를 선정했습니다. 해당 기능이 웹 서비스의 메인이며, 그에 따라 작물 질병 진단 인공지능을 개발합니다. 609GB나 되는 대용량 이미지 데이터여서 모든 데이터를 다운받지는 못 했지만, 팀원들과 우여곡절 끝에 학습을 진행할 수 있을 만큼의 데이터 다운로드는 성공했습니다. 결론적으로, 프로젝트 환경의 디스크 최대 용량이 300GB인 것을 고려하여 증강 데이터를 뺀 약 200GB 중 다운로드를 성공한 데이터 약 150GB를 사용하여 진행하였습니다.\n\nimport os\nimport zipfile\nimport random \n\nfrom PIL import Image, ImageDraw\nimport json\nimport pickle\nfrom tqdm import tqdm\nimport numpy as np \n\nimport matplotlib.pyplot as plt\nplt.rc(\"font\", family=\"NanumGothic\", size=13) \n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n수집한 데이터는 라벨데이터와 원천(이미지) 데이터로 이루어져있으며, 다음과 같은 폴더 구조로 VM에 적재하였습니다.\ndata\n    |_ training  \n        |_ image_class   \n            |_ 고추 정상\n            |_ 고추 질병 1 \n            |_ ...\n        |_ label  \n\n    |_ validation   \n        |_ image_class   \n            |_ 고추 정상\n            |_ 고추 질병 1 \n            |_ ...\n        |_ label  \n    \n\n# 경로 설정  \npath_train_img = \"data/training/image_class/\"\npath_train_label = \"data/training/label/\"\n\npath_valid_img = \"data/validation/image_class/\"\npath_valid_label = \"data/validation/label/\"\n\n# 경로에 들어있는 파일 리스트\nlst_train_img = os.listdir(path_train_img)\nlst_train_label = os.listdir(path_train_label)\n\nlst_valid_img = os.listdir(path_valid_img)\nlst_valid_label = os.listdir(path_valid_label)\n\n폴더 구조에 맞게 경로를 설정해주고, 그 안에 들어있는 파일이름을 각 변수에 리스트로 담아줍니다."
  },
  {
    "objectID": "posts/c01.EDA.html#라벨-데이터-예시",
    "href": "posts/c01.EDA.html#라벨-데이터-예시",
    "title": "[cropdoctor] 1. 노지작물 데이터 EDA",
    "section": "라벨 데이터 예시",
    "text": "라벨 데이터 예시\n먼저, 라벨 데이터 하나가 어떻게 생겼는지 확인해봅니다.\n\nmyzip_r = zipfile.ZipFile(path_train_label+\"[라벨]잎마름병(토마토)_1.질병.zip\", 'r')\n\nprint(myzip_r.namelist()[:3])\n\njson_str = myzip_r.read('V006_79_1_15_07_03_12_1_2656z_20201104_19.jpg.json')\n\njson.loads(json_str)\n\n['V006_79_1_15_07_03_12_1_2656z_20201104_19.jpg.json', 'V006_79_1_15_07_03_12_1_2656z_20201104_37.jpg.json', 'V006_79_1_15_07_03_12_1_2656z_20201105_38.jpg.json']\n\n\n{'description': {'image': 'V006_79_1_15_07_03_12_1_2656z_20201104_19.jpg',\n  'date': '2020/11/04',\n  'worker': '',\n  'height': 3024,\n  'width': 4032,\n  'task': 79,\n  'type': 1,\n  'region': 5},\n 'annotations': {'disease': 15,\n  'crop': 7,\n  'area': 3,\n  'grow': 12,\n  'risk': 1,\n  'points': [{'xtl': 1746, 'ytl': 1472, 'xbr': 2528, 'ybr': 3024}]}}\n\n\n위와 같이 json 형식으로 구성되어있습니다. 파일 하나당 하나의 이미지로 되어있기 때문에, 라벨 데이터를 사용할 때마다 각각을 불러오기에는 비효율적이라는 생각이 들어, 필요한 데이터들만 추출하여 하나의 딕셔너리에 담는 처리를 진행하겠습니다.\n모델 학습 시 필요한 데이터들을 추려보면 질병, 작물, 바운딩박스, 이미지 크기 정도 입니다. 따라서 {image: {disease, crop, points, (width, height)}, …} 와 같은 형태로 구성되도록 처리합니다.\n\n\n\n\nimage.png\n\n\n한가지 더 고려할 점은 콩과 토마토입니다. 데이터 검수를 진행하던 중 aihub 홈페이지와 라벨링이 다르게 되어있는 점을 발견하였습니다. aihub에서는 콩이 7, 토마토가 8인데, 라벨 데이터에서 토마토가 7, 콩이 8 로 되어있었습니다. 이 점도 함께 고려하여 라벨데이터를 처리합니다."
  },
  {
    "objectID": "posts/c01.EDA.html#라벨-데이터-처리",
    "href": "posts/c01.EDA.html#라벨-데이터-처리",
    "title": "[cropdoctor] 1. 노지작물 데이터 EDA",
    "section": "라벨 데이터 처리",
    "text": "라벨 데이터 처리\n라벨 데이터에서 처리해야 하는 항목은 다음과 같습니다.\n\n라벨링 처리\n\n콩, 토마토 라벨 처리: aihub 라벨과 다른 오류를 처리합니다.\n수집한 작물만 남겨놓고, 라벨 재설정: 고추, 무, 배추, 애호박, 콩, 토마토, 호박만 수집 성공하였으므로 이외의 라벨 데이터는 없애줍니다.\n\n작물 별 정상 라벨링: 모든 작물의 정상 데이터는 0으로 되어있었는데, 작물 별로 구분하여 재설정합니다.\n\n\n\n\n이미지 별 작물, 질병, 바운딩 박스, 이미지 사이즈 딕셔너리 생성\n\n{image: {disease, crop, points, (width, height)}, …} 의 형태로, 라벨 데이터에서 필요한 데이터만 추출하여 하나의 딕셔너리에 모아줍니다.\n\n\n\n\n이름 변환 딕셔너리\n우선 라벨 데이터를 처리하기 위한 딕셔너리들을 선언합니다.\n\n##### 작물 #####\n# 원본 라벨\ncrop2name = {\n             1: \"고추\", \n             2: \"무\", \n             3: \"배추\", \n             4: \"애호박\", \n             5: \"양배추\", \n             6: \"오이\", \n             8: \"콩\", \n             7:\"토마토\", \n             9: \"파\", \n             10: \"호박\"}\n\n# 수집한 데이터만 라벨 재설정 \ncrop2name_pre = {\n             1: 1, \n             2: 2, \n             3: 3, \n             4: 4, \n             8: 5, \n             7: 6, \n             10: 7}\n\n# 작물 이름과 매치 \ncrop2name_new = {\n             1: \"고추\", \n             2: \"무\", \n             3: \"배추\", \n             4: \"애호박\", \n             5: \"콩\", \n             6: \"토마토\", \n             7: \"호박\"}\n\n\n##### 질병 #####\n# 원본 질병 라벨 \ndisease2name = {\n                0: \"정상\",\n                1: \"고추탄저병\",\n               2: \"고추흰가루병\",\n               3: \"무검은무늬병\", \n               4: \"무노균병\", \n               5: \"배추검음썩음병\",\n               6: \"배추노균병\",\n               7: \"애호박노균병\",\n               8: \"애호박흰가루병\",\n               9: \"양배추균핵병\",\n               10: \"양배추무름병\",\n               11: \"오이노균병\",\n               12: \"오이흰가루병\", \n               13: \"콩불마름병\",\n               14: \"콩점무늬병\",\n               15: \"토마토잎마름병\", \n               16: \"파검은무늬병\",\n               17: \"파노균병\",\n               18: \"파녹병\",\n               19: \"호박노균병\",\n               20: \"호박흰가루병\"}\n\n# 수집한 데이터만 라벨 재설정 (정상 데이터 라벨 비워놓기)\ndisease2name_pre = {\n                1: 1,\n               2: 2,\n    \n               3: 4, \n               4: 5, \n               \n               5: 7,\n               6: 8,\n               \n               7: 10,\n               8: 11,\n               \n               13: 13,\n               14: 14,\n               \n               15: 16, \n\n               19: 18,\n               20: 19}\n\n# 정상 데이터를 포함하여 질병 이름과 매치\ndisease2name_new = {\n               0: \"고추정상\",\n               1: \"고추탄저병\",\n               2: \"고추흰가루병\",\n               \n               3: \"무정상\",\n               4: \"무검은무늬병\", \n               5: \"무노균병\", \n    \n               6: \"배추정상\",\n               7: \"배추검음썩음병\",\n               8: \"배추노균병\",\n    \n               9: \"애호박정상\",\n               10: \"애호박노균병\",\n               11: \"애호박흰가루병\", \n               \n               12: \"콩정상\",\n               13: \"콩불마름병\",\n               14: \"콩점무늬병\",\n    \n               15: \"토마토정상\",\n               16: \"토마토잎마름병\", \n    \n               17: \"호박정상\",\n               18: \"호박노균병\",\n               19: \"호박흰가루병\"}\n\n\n# 작물 라벨에 따른 질병의 정상 라벨  \ncrop2normal =  {\n             1: 0, \n             2: 3, \n             3: 6, \n             4: 9, \n             5: 12, \n             6: 15, \n             7: 17}\n\n\n\n\nimage별 딕셔너리 생성\n위에서 선언한 라벨 딕셔너리를 활용하여, 이미지 별로 작물, 질병 라벨을 처리하여 담고, 바운딩 박스, 이미지 사이즈도 함께 담아 하나의 딕셔너리에 모아줍니다.\n\n\ntrain\n\ndic_img2label_train = {}\n\nfor file in tqdm(lst_train_label[:-1]): \n    myzip_r = zipfile.ZipFile(path_train_label+file, 'r') \n    for name in myzip_r.namelist():\n        j = json.loads(myzip_r.read(name))    \n        disease, crop, points = j['annotations']['disease'], crop2name_pre[j['annotations']['crop']], j['annotations']['points'][0]\n        width, height = j['description']['width'], j['description']['height']\n        \n        if disease == 0: \n            disease = crop2normal[crop]\n        else: \n            disease = disease2name_pre[disease]\n        \n        dic_img2label_train[j['description']['image']] = {'disease': disease, 'crop': crop, 'points': points, 'size': (width, height)}\n\n100%|██████████| 14/14 [00:04<00:00,  3.28it/s]\n\n\n\n\ntest\n\ndic_img2label_val = {}\n\nfor file in tqdm(lst_valid_label[:-1]): \n    myzip_r = zipfile.ZipFile(path_valid_label+file, 'r') \n    for name in myzip_r.namelist():\n        j = json.loads(myzip_r.read(name))    \n        disease, crop, points = j['annotations']['disease'], crop2name_pre[j['annotations']['crop']], j['annotations']['points'][0]\n        \n        if disease == 0: \n            disease = crop2normal[crop]\n        else: \n            disease = disease2name_pre[disease]\n        dic_img2label_val[j['description']['image']] = {'disease': disease, 'crop': crop, 'points': points, 'size': (width, height)}\n\n100%|██████████| 14/14 [00:00<00:00, 18.79it/s]\n\n\n처리한 딕셔너리를 피클로 저장하여 필요할 때마다 로드할 수 있도록 합니다.\n\n# 저장 \n# train\nwith open(\"data_preprocessing/dic_img2label_train.pickle\",\"wb\") as fw:\n    pickle.dump(dic_img2label_train, fw)\n    \n# validation\nwith open(\"data_preprocessing/dic_img2label_val.pickle\",\"wb\") as fw:\n    pickle.dump(dic_img2label_val, fw)\n\n\n# 로드 \n# train\nwith open(\"data_preprocessing/dic_img2label_train.pickle\",\"rb\") as fr:\n    dic_img2label_train = pickle.load(fr)\n    \n# validation\nwith open(\"data_preprocessing/dic_img2label_val.pickle\",\"rb\") as fr:\n    dic_img2label_val = pickle.load(fr)"
  },
  {
    "objectID": "posts/c01.EDA.html#라벨-데이터-살펴보기",
    "href": "posts/c01.EDA.html#라벨-데이터-살펴보기",
    "title": "[cropdoctor] 1. 노지작물 데이터 EDA",
    "section": "라벨 데이터 살펴보기",
    "text": "라벨 데이터 살펴보기\n위에서 생성한 이미지 별 라벨 데이터 딕셔너리를 사용하여 데이터의 라벨 비율을 살펴보겠습니다.\n\n노지작물 비율\n\nlst_crop_cnt = [0] * 7\nfor dic_value in tqdm(dic_img2label_train.values()): \n    lst_crop_cnt[dic_value['crop']-1] += 1     \n\n100%|██████████| 78335/78335 [00:00<00:00, 2008011.02it/s]\n\n\n\nfig = plt.figure(figsize=(13, 7))\nax = fig.subplots()\n\nbars = ax.bar(range(len(lst_crop_cnt)), lst_crop_cnt, color='#8ebe8d', edgecolor = 'black')\nax.set_xticks(range(len(lst_crop_cnt)))\nax.set_xticklabels([\"고추\", \"무\",  \"배추\", \"애호박\", \"콩\", \"토마토\", \"호박\"], fontsize=20)\nax.set_ylim(0, 13000)\n\nplt.title(\"노지작물 별 개수\", fontsize=20, pad=10)\n\nfor i, b in enumerate(bars.patches):\n    _=plt.text(b.get_x()+b.get_width()*(1/2),b.get_height()+300, \\\n            round(b.get_height()),ha='center',fontsize=15, color='k')\n\nplt.show()\n\n\n\n\n각 작물의 개수는 균등한 것으로 보입니다.\n\n\n\n정상 / 질병 데이터의 비율\n\ncnt_normal = 0\nfor dic in tqdm(dic_img2label_train.values()):\n    if dic['disease'] in [0, 3, 6, 9, 12, 15, 17]: \n        cnt_normal += 1\n\n100%|██████████| 78335/78335 [00:00<00:00, 2056499.80it/s]\n\n\n\nprint(\"데이터 총 개수: \", sum(lst_disease_cnt))\nprint(\"정상 데이터의 개수: \", cnt_normal)\nprint(\"질병 데이터의 개수: \", sum(lst_disease_cnt) - cnt_normal)\n\nfig = plt.figure(figsize=(5, 3))\nax = fig.subplots()\nbars = ax.bar(range(2), [cnt_normal, sum(lst_disease_cnt) - cnt_normal], color='#e0a4b2', edgecolor = 'black')\nax.set_xticks(range(2))\nax.set_xticklabels([\"정상\", \"질병\"], fontsize=12)\nax.set_ylim(0, 90000)\n\nplt.title(\"정상 vs 질병\", fontsize=13, pad=10)\n\nfor i, b in enumerate(bars.patches):\n    _=plt.text(b.get_x()+b.get_width()*(1/2),b.get_height()+5000, \\\n            round(b.get_height()),ha='center',fontsize=12, color='k')\n\nplt.show()\n\n데이터 총 개수:  78335\n정상 데이터의 개수:  71421\n질병 데이터의 개수:  6914\n\n\n\n\n\n정상 데이터와 질병 데이터의 개수를 시각화 해보았을 때, 불균형이 심한 것을 확인하였습니다. 더 자세히 보기 위해 작물 별 정상 데이터와 질병데이터를 확인해보겠습니다.\n\n\n\n레이블 비율\n\nlst_disease_cnt = [0] * 20\nfor dic_value in tqdm(dic_img2label_train.values()): \n    lst_disease_cnt[dic_value['disease']] += 1    \n\n100%|██████████| 78335/78335 [00:00<00:00, 1259471.33it/s]\n\n\n\ndic_num2disease = {\n    1: [\"고추정상\", \"고추탄저병\", \"고추흰가루병\"],\n    2: [\"무정상\", \"무검은무늬병\", \"무노균병\"],\n    3: [\"배추정상\", \"배추검음썩음병\", \"배추노균병\"],\n    4: [\"애호박정상\", \"애호박노균병\", \"애호박흰가루병\"],\n    5: [\"콩정상\", \"콩불마름병\", \"콩점무늬병\"],\n    6: [\"토마토정상\", \"토마토잎마름병\"],\n    7: [\"호박정상\", \"호박노균병\", \"호박흰가루병\"]\n}\n\nlst_c = ['#8b1e0d', '#8b1e0d', '#8b1e0d', 'w', 'w', 'w', '#99b563', '#99b563', '#99b563', '#d4de3a', '#d4de3a', '#d4de3a', '#4f4f4f', '#4f4f4f', '#4f4f4f', 'r', 'r', '#d57b13', '#d57b13', '#d57b13']\n\nfig = plt.figure(figsize=(15, 5))\nax = fig.subplots()\n\nbars = ax.bar(range(len(lst_disease_cnt)), lst_disease_cnt, color=lst_c, edgecolor = 'black')\nax.set_xticks(range(len(lst_disease_cnt)))\nax.set_xticklabels([\"고추정상\", \"고추탄저병\", \"고추흰가루병\", \n                    \"무정상\", \"무검은무늬병\", \"무노균병\", \n                    \"배추정상\", \"배추검음썩음병\", \"배추노균병\", \n                    \"애호박정상\", \"애호박노균병\", \"애호박흰가루병\", \n                    \"콩정상\", \"콩불마름병\", \"콩점무늬병\", \n                    \"토마토정상\", \"토마토잎마름병\", \n                    \"호박정상\", \"호박노균병\", \"호박흰가루병\"], fontsize=15, rotation=45)\nax.set_ylim(0, 12500)\n\nplt.title(\"노지작물 질병 별 개수\", fontsize=20, pad=10)\n\nfor i, b in enumerate(bars.patches):\n    _=plt.text(b.get_x()+b.get_width()*(1/2),b.get_height()+200, \\\n            round(b.get_height()),ha='center',fontsize=12, color='k')\n\nplt.show()\n\n\n\n\n작물 별 정상/질병 데이터는 총 20개의 카테고리로, 최종적인 모델학습의 레이블이기도 합니다.\n시각화하여 살펴보니 작물 별 정상/질병 데이터의 불균형 문제가 존재하는 것을 확인하였습니다.\n따라서 다음과 같은 방법을 사용하고자 합니다.\n1. 랜덤샘플링\n각 작물마다 정상 데이터에서 1000장을 랜덤으로 샘플링합니다. 이는 불균형을 줄이기 위해서도 맞지만, 그와 동시에 용량을 줄이기 위해 진행합니다. 제공받은 GPU 성능 및 디스크 용량으로 약 8000장의 모델 학습을 돌리기엔 어려움이 있었기에, 이 방법을 우선 선택하였습니다.\n2. WCE (Weighted cross entropy)\n정상 데이터를 1000장 랜덤 샘플링 한 후에도 몇몇 레이블은 불균형이 여전히 심할 것입니다. 따라서 모델 학습 시 WCE를 사용하여 가중치를 주면서 불균형을 극복하고자 합니다."
  },
  {
    "objectID": "posts/c01.EDA.html#랜덤-샘플링",
    "href": "posts/c01.EDA.html#랜덤-샘플링",
    "title": "[cropdoctor] 1. 노지작물 데이터 EDA",
    "section": "랜덤 샘플링",
    "text": "랜덤 샘플링\n데이터가 너무 많고, 불균형을 줄이기 위해 정상 데이터에서 1000장을 랜덤으로 추출하여 데이터의 크기를 줄였으며, 팀원들과 수작업으로 진행하였습니다.\n랜덤 샘플링 된 이미지의 라벨 데이터만 모아서 앞과 동일하게 시각화로 확인 합니다.\n\n라벨 데이터 재 처리\n앞서 생성한 딕셔너리 dic_img2label_train에서 랜덤 샘플링된 이미지만 추출하여 dic_img2label_train_sampling 딕셔너리에 넣어주고, 똑같이 pickle 파일로 저장해줍니다.\n\nlst_crop_folder_train = os.listdir(path_train_img)\nlst_crop_folder_valid = os.listdir(path_valid_img)\n\ndic_img2label_train_sampling = {}\ndic_img2label_valid_sampling = {}\n\nfor folder in tqdm(lst_crop_folder_train): \n    for img in os.listdir(path_train_img + folder):  \n        if img == '.ipynb_checkpoints': \n            continue \n        dic_img2label_train_sampling[img] = dic_img2label_train[img]\n\nfor folder in tqdm(lst_crop_folder_valid): \n    for img in os.listdir(path_valid_img + folder):  \n        if img == '.ipynb_checkpoints': \n            continue \n        dic_img2label_valid_sampling[img] = dic_img2label_val[img]\n\n100%|██████████| 20/20 [00:00<00:00, 951.73it/s]\n100%|██████████| 20/20 [00:00<00:00, 1587.10it/s]\n\n\n\n# 저장 \n# train\nwith open(\"data_preprocessing/dic_img2label_train_sampling.pickle\",\"wb\") as fw:\n    pickle.dump(dic_img2label_train_sampling, fw)\n    \n# validation\nwith open(\"data_preprocessing/dic_img2label_valid_sampling.pickle\",\"wb\") as fw:\n    pickle.dump(dic_img2label_valid_sampling, fw)\n\n\n# 로드 \n# train\nwith open(\"data_preprocessing/dic_img2label_train_sampling.pickle\",\"rb\") as fr:\n    dic_img2label_train_sampling = pickle.load(fr)\n    \n# validation\nwith open(\"data_preprocessing/dic_img2label_valid_sampling.pickle\",\"rb\") as fr:\n    dic_img2label_valid_sampling = pickle.load(fr)\n\n\n\n\n노지작물 비율\n\nlst_crop_cnt = [0] * 7\nfor dic_value in tqdm(dic_img2label_train_sampling.values()): \n    lst_crop_cnt[dic_value['crop']-1] += 1     \n\n100%|██████████| 13915/13915 [00:00<00:00, 1197745.45it/s]\n\n\n\nfig = plt.figure(figsize=(13, 7))\nax = fig.subplots()\n\nbars = ax.bar(range(len(lst_crop_cnt)), lst_crop_cnt, color='#8ebe8d', edgecolor = 'black')\nax.set_xticks(range(len(lst_crop_cnt)))\nax.set_xticklabels([\"고추\", \"무\",  \"배추\", \"애호박\", \"콩\", \"토마토\", \"호박\"], fontsize=20)\nax.set_ylim(0, 3100)\n\nplt.title(\"랜덤 샘플링 후: 노지작물 별 개수\", fontsize=20, pad=10)\n\nfor i, b in enumerate(bars.patches):\n    _=plt.text(b.get_x()+b.get_width()*(1/2),b.get_height()+50, \\\n            round(b.get_height()),ha='center',fontsize=15, color='k')\n\nplt.show()\n\n\n\n\n정상 데이터의 개수를 줄이니, 질병 데이터의 불균형이 전체 데이터에서도 드러나는 것을 확인할 수 있습니다.\n\n\n\n정상 / 비정상 데이터의 비율\n\ncnt_normal = 0\nfor dic in tqdm(dic_img2label_train_sampling.values()):\n    if dic['disease'] in [0, 3, 6, 9, 12, 15, 17]: \n        cnt_normal += 1\n\n100%|██████████| 13915/13915 [00:00<00:00, 2021955.31it/s]\n\n\n\nprint(\"데이터 총 개수: \", sum(lst_disease_cnt))\nprint(\"정상 데이터의 개수: \", cnt_normal)\nprint(\"질병 데이터의 개수: \", sum(lst_disease_cnt) - cnt_normal)\n\nfig = plt.figure(figsize=(5, 3))\nax = fig.subplots()\nbars = ax.bar(range(2), [cnt_normal, sum(lst_disease_cnt) - cnt_normal], color='#e0a4b2', edgecolor = 'black')\nax.set_xticks(range(2))\nax.set_xticklabels([\"정상\", \"질병\"], fontsize=12)\nax.set_ylim(0, 9000)\n\nplt.title(\"랜덤 샘플링 후: 정상 vs 질병\", fontsize=13, pad=10)\n\nfor i, b in enumerate(bars.patches):\n    _=plt.text(b.get_x()+b.get_width()*(1/2),b.get_height()+500, \\\n            round(b.get_height()),ha='center',fontsize=12, color='k')\n\nplt.show()\n\n데이터 총 개수:  13915\n정상 데이터의 개수:  7001\n질병 데이터의 개수:  6914\n\n\n\n\n\n데이터를 모두 합쳤을 때는 정상 데이터와 질병 데이터의 불균형이 확연히 줄어든 것처럼 보이지만 위에서 확인한 것과 같이 그렇지 않습니다. 데이터의 총 개수를 보면, 랜덤샘플링 전 78335개에서 13915개로 대폭 줄어든 것을 확인할 수 있습니다.\n\n\n\n레이블 비율\n\nlst_disease_cnt = [0] * 20\nfor dic_value in tqdm(dic_img2label_train_sampling.values()): \n    lst_disease_cnt[dic_value['disease']] += 1  \n\n100%|██████████| 13915/13915 [00:00<00:00, 346903.51it/s]\n\n\n\ndic_num2disease = {\n    1: [\"고추정상\", \"고추탄저병\", \"고추흰가루병\"],\n    2: [\"무정상\", \"무검은무늬병\", \"무노균병\"],\n    3: [\"배추정상\", \"배추검음썩음병\", \"배추노균병\"],\n    4: [\"애호박정상\", \"애호박노균병\", \"애호박흰가루병\"],\n    5: [\"콩정상\", \"콩불마름병\", \"콩점무늬병\"],\n    6: [\"토마토정상\", \"토마토잎마름병\"],\n    7: [\"호박정상\", \"호박노균병\", \"호박흰가루병\"]\n}\n\nlst_c = ['#8b1e0d', '#8b1e0d', '#8b1e0d', 'w', 'w', 'w', '#99b563', '#99b563', '#99b563', '#d4de3a', '#d4de3a', '#d4de3a', '#4f4f4f', '#4f4f4f', '#4f4f4f', 'r', 'r', '#d57b13', '#d57b13', '#d57b13']\n\nfig = plt.figure(figsize=(15, 5))\nax = fig.subplots()\n\nbars = ax.bar(range(len(lst_disease_cnt)), lst_disease_cnt, color=lst_c, edgecolor = 'black')\nax.set_xticks(range(len(lst_disease_cnt)))\nax.set_xticklabels([\"고추정상\", \"고추탄저병\", \"고추흰가루병\", \n                    \"무정상\", \"무검은무늬병\", \"무노균병\", \n                    \"배추정상\", \"배추검음썩음병\", \"배추노균병\", \n                    \"애호박정상\", \"애호박노균병\", \"애호박흰가루병\", \n                    \"콩정상\", \"콩불마름병\", \"콩점무늬병\", \n                    \"토마토정상\", \"토마토잎마름병\", \n                    \"호박정상\", \"호박노균병\", \"호박흰가루병\"], fontsize=15, rotation=45)\nax.set_ylim(0, 1100)\n\nplt.title(\"랜덤 샘플링 후: 노지작물 라벨 별 개수\", fontsize=20, pad=10)\n\nfor i, b in enumerate(bars.patches):\n    _=plt.text(b.get_x()+b.get_width()*(1/2),b.get_height()+20, \\\n            round(b.get_height()),ha='center',fontsize=12, color='k')\n\nplt.show()\n\n\n\n\n정상 이미지가 모두 1000장으로 줄어든 것 확인하였으며, 역시 아직 불균형이 존재합니다. 이는 앞서 말했듯, 모델 학습 시 WCE 기법을 사용하도록 합니다."
  },
  {
    "objectID": "posts/c01.EDA.html#이미지-확인",
    "href": "posts/c01.EDA.html#이미지-확인",
    "title": "[cropdoctor] 1. 노지작물 데이터 EDA",
    "section": "이미지 확인",
    "text": "이미지 확인\n이미지의 라벨데이터를 활용하여 레이블 매치 및 바운딩 박스를 함께 그려 데이터를 확인하는 함수를 생성하였습니다. 인자에 원하는 레이블의 숫자를 넣으면 그에 대한 이미지를 랜덤으로 9장 뽑아옵니다.\n\nlst_train_img_folder = sorted([folder for folder in sorted(lst_train_img) if 'ipynb' not in folder])\n\n# 라벨 별 랜덤 이미지 9장 뽑아오는 함수 \ndef show_images_by_label(folder_num):\n    folder = lst_train_img_folder[folder_num]\n    lst_forder_img = os.listdir(path_train_img + folder)\n    lst_nums = random.sample(range(len(lst_forder_img)), 9)\n\n    fig = plt.figure(figsize=(13, 13))\n    axes = fig.subplots(3, 3).flatten()\n    fig.suptitle(f\"{disease2name_new[folder_num]}\", fontsize=20)\n\n    for i, num in enumerate(lst_nums): \n        img_name = lst_forder_img[num]\n        img = Image.open(path_train_img + folder + '/' + img_name)\n\n        d1 = dic_img2label_train[img_name]\n\n        crop, disease, points = d1['crop'], d1['disease'], d1['points']\n\n        draw = ImageDraw.Draw(img)\n        draw.rectangle([(points['xtl'], points['ytl']), (points['xbr'], points['ybr'])], outline=(255, 0, 255), width=30)\n\n        axes[i].imshow(np.array(img))\n\n\nshow_images_by_label(1)\n\n\n\n\n\nshow_images_by_label(3)\n\n\n\n\n\nshow_images_by_label(5)\n\n\n\n\n\nshow_images_by_label(7)\n\n\n\n\n\nshow_images_by_label(9)\n\n\n\n\n\nshow_images_by_label(11)\n\n\n\n\n\nshow_images_by_label(19)\n\n\n\n\n이미지 데이터를 살펴보았을 때, 질병 작물에 대하여 바운딩 박스 처리가 잘 되어있음을 확인했습니다. image classification 뿐만 아니라, object detection 으로 모델학습을 진행하는 것도 고려할 수 있습니다. 다만, 작물을 이루는 요소가 잎, 줄기, 꽃 등이 있다보니 같은 레이블 안에서도 다른 특징의 이미지로 인식할 수 있다는 점이 우려되었습니다.\n지금까지 노지작물 데이터에 대한 EDA를 진행하였습니다. EDA를 통해 라벨 데이터를 처리하였고, 데이터의 분포가 어떻게 되어있는지 확인할 수 있었습니다. 또한 어떤 방향으로 인공지능 모델 학습을 해야하는지 고민해볼 수 있었습니다."
  },
  {
    "objectID": "posts/c02.aihub_model_evaluate.html#tar-확장자-모델-docker-image-불러오기",
    "href": "posts/c02.aihub_model_evaluate.html#tar-확장자-모델-docker-image-불러오기",
    "title": "[cropdoctor] 2. tar 확장자 모델 docker image 불러오기",
    "section": "2. tar 확장자 모델 docker image 불러오기",
    "text": "2. tar 확장자 모델 docker image 불러오기\n\nimport pickle \nimport os \nfrom tqdm import tqdm\nimport random \n\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torchvision.models as models\nfrom torchsummary import summary\n \nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n    \nfrom PIL import Image, ImageDraw\nimport albumentations\n\nimport matplotlib.pyplot as plt \nplt.rc(\"font\", family=\"NanumGothic\", size=13) \n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\n01. docker 로 tar 파일 불러오기\naihub에서 예시 모델을 하나 제공해주어서 그것을 베이스라인 모델로 선정하고자 했습니다. 해당 모델을 사용하기 위해, tar 확장자로 저장되어 있는 aihub의 예시 모델을 docker로 불러와야 했습니다.\n\n1) 모델 불러오기 오류\n프로젝트에서 제공받은 VM의 리눅스 터미널에서 모델 불러오는 것을 시도하였습니다. >docker load -i 73.tar\nCannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?\n하지만 위와 같은 오류가 났습니다.\n따라서 데몬이 실행중인지 확인하기 위하여 다음과 같은 명령어를 실행하였습니다.\n\nsudo systemctl status docker\n\nSystem has not been booted with systemd as init system (PID 1). Can’t operate. Failed to connect to bus: Host is down 또 다른 오류를 보았습니다.\n구글링과 chat gpt에 열심히 질문하면서 해결을 시도하였지만 성공하지 못했습니다.  그 당시 노션에 메모해뒀던 오류의 원인입니다.\n마침내 운영진쪽에서 docker를 지원하지 않는 VM이라는 말을 전해듣고, 다른 방법을 택했습니다. 허탈했지만 해결 안되던게 정상이어서 한편으로는 다행이었습니다.\n\n2) 로컬에서 불러와서 VM에 올리기\nVM에서 모델을 불러올 수 없으니, 로컬 git bash에서 명령어를 실행하는 방법을 선택했습니다.\n\n\n도커 이미지 불러오기\n\n\ndocker load -i 73.tar\n\n\n\n\nimage.png\n\n\n모델 불러올 당시 컴퓨터가 멈추고 화면이 까맣게 변했던 기억이 납니다. 다행히 그 와중에도 git bash는 꺼지지 않고 계속 모델이 로드되고 있었습니다.\n\n\n도커 이미지 확인하기\n\n\ndocker images\n\n\n\n\nimage.png\n\n\n도커 이미지가 잘 불러와진 것을 확인할 수 있습니다.\n\n\n도커 컨테이너 실행시키기\n\n\ndocker run -it laonpeople/79_classification:v0.2 ls\n\n\n\n\nimage.png\n\n\n도커 컨테이너를 실행시키고, 그 안에서 파일 목록을 확인해 보았습니다. 드디어 aihub에서 제공해준 모델 파일들을 확인하는 데 성공했습니다.\n\n\n도커 컨테이너 -> 로컬 복사하기\n\n로컬에서 모델을 로드했기 때문에 프로젝트 작업을 진행하는 VM으로 파일들을 옮겨야 했습니다. 따라서 도커 컨테이너 -> 로컬 -> 프로젝트 VM 의 순서로 파일을 옮기기로 했습니다.\n\ndocker run -d –name con laonpeople/79_classification:v0.2\n\n우선 도커 컨테이너를 -d 옵션으로 백그라운드에서 실행시킵니다. 이 때, –name 옵션으로 컨테이너 이름을 con으로 지정해주었습니다.\n\ndocker cp con:/ C:/Users/sooki/Downloads/docker\n\n그 다음, docker cp [컨테이너이름]:[파일위치] [로컬파일위치] 명령어를 사용하여 로컬에 무사히 파일들을 옮겼습니다.\n이제 로컬에서 VM jupyter-lab 환경에 드래그앤드랍으로 파일을 최종적으로 옮겨주었습니다.\n\n\nVM에서 필요한 패키지 설치하기\n\n\npip install -r requirements.txt\n\nrequirements.txt 파일에 필요한 패키지가 들어있었습니다. 이를 vm에서 모두 설치해주면, aihub 모델 불러올 준비를 모두 마치게 됩니다.\n\n\n이름 변환 딕셔너리\n\ndisease2name = {\n                0: \"정상\",\n                1: \"고추탄저병\",\n               2: \"고추흰가루병\",\n               3: \"무검은무늬병\", \n               4: \"무노균병\", \n               5: \"배추검음썩음병\",\n               6: \"배추노균병\",\n               7: \"애호박노균병\",\n               8: \"애호박흰가루병\",\n               9: \"양배추균핵병\",\n               10: \"양배추무름병\",\n               11: \"오이노균병\",\n               12: \"오이흰가루병\", \n               13: \"콩불마름병\",\n               14: \"콩점무늬병\",\n               15: \"토마토잎마름병\", \n               16: \"파검은무늬병\",\n               17: \"파노균병\",\n               18: \"파녹병\",\n               19: \"호박노균병\",\n               20: \"호박흰가루병\"}\n\n\n\n\n\n02. 모델 불러와서 성능 평가해보기\n\nconfig 파일을 보니 04작물에 대해서 3개의 클래스로 분류하는 모델인 것으로 보입니다. 즉 애호박 에 대해서만 정상/ 질병1 / 질병2 이렇게 분류하는 모델입니다. 레이블이 다르므로 학습에 사용하지는 않고, 성능이 얼마정도 나오는지만 확인해보겠습니다.\n\n\n\n모델 불러오기\n\nmodel_path = \"docker/weights/sample.pt\"\n  \ndevice = torch.device(\"cuda\")\n\nmodel_weights = torch.load(model_path, map_location=device)[\"model_state_dict\"]\n\n\n\n\n학습된 모델 구조\n\nimport torch.nn as nn\nimport torchvision.models as models\n\ndict_backbone = {'resnet50' : models.resnet50}\n\ndef get_model(model_name='resnet50', num_classes=3, pretrained=False): # use pretrained backbone\n    assert model_name in dict_backbone.keys()\n    \n    network = dict_backbone[model_name](pretrained=pretrained)\n    network.fc = nn.Linear(network.fc.in_features, num_classes)\n    \n    return network\n\n\nmodel = get_model()\nmodel.load_state_dict(model_weights)\nmodel = model.to(device)\n\n\n\n\n모델 요약\n\nsummary(model, input_size=(3, 512, 512))\n\n----------------------------------------------------------------\n        Layer (type)               Output Shape         Param #\n================================================================\n            Conv2d-1         [-1, 64, 112, 112]           9,408\n       BatchNorm2d-2         [-1, 64, 112, 112]             128\n              ReLU-3         [-1, 64, 112, 112]               0\n         MaxPool2d-4           [-1, 64, 56, 56]               0\n            Conv2d-5           [-1, 64, 56, 56]           4,096\n       BatchNorm2d-6           [-1, 64, 56, 56]             128\n              ReLU-7           [-1, 64, 56, 56]               0\n            Conv2d-8           [-1, 64, 56, 56]          36,864\n       BatchNorm2d-9           [-1, 64, 56, 56]             128\n             ReLU-10           [-1, 64, 56, 56]               0\n           Conv2d-11          [-1, 256, 56, 56]          16,384\n      BatchNorm2d-12          [-1, 256, 56, 56]             512\n           Conv2d-13          [-1, 256, 56, 56]          16,384\n      BatchNorm2d-14          [-1, 256, 56, 56]             512\n             ReLU-15          [-1, 256, 56, 56]               0\n       Bottleneck-16          [-1, 256, 56, 56]               0\n           Conv2d-17           [-1, 64, 56, 56]          16,384\n      BatchNorm2d-18           [-1, 64, 56, 56]             128\n             ReLU-19           [-1, 64, 56, 56]               0\n           Conv2d-20           [-1, 64, 56, 56]          36,864\n      BatchNorm2d-21           [-1, 64, 56, 56]             128\n             ReLU-22           [-1, 64, 56, 56]               0\n           Conv2d-23          [-1, 256, 56, 56]          16,384\n      BatchNorm2d-24          [-1, 256, 56, 56]             512\n             ReLU-25          [-1, 256, 56, 56]               0\n       Bottleneck-26          [-1, 256, 56, 56]               0\n           Conv2d-27           [-1, 64, 56, 56]          16,384\n      BatchNorm2d-28           [-1, 64, 56, 56]             128\n             ReLU-29           [-1, 64, 56, 56]               0\n           Conv2d-30           [-1, 64, 56, 56]          36,864\n      BatchNorm2d-31           [-1, 64, 56, 56]             128\n             ReLU-32           [-1, 64, 56, 56]               0\n           Conv2d-33          [-1, 256, 56, 56]          16,384\n      BatchNorm2d-34          [-1, 256, 56, 56]             512\n             ReLU-35          [-1, 256, 56, 56]               0\n       Bottleneck-36          [-1, 256, 56, 56]               0\n           Conv2d-37          [-1, 128, 56, 56]          32,768\n      BatchNorm2d-38          [-1, 128, 56, 56]             256\n             ReLU-39          [-1, 128, 56, 56]               0\n           Conv2d-40          [-1, 128, 28, 28]         147,456\n      BatchNorm2d-41          [-1, 128, 28, 28]             256\n             ReLU-42          [-1, 128, 28, 28]               0\n           Conv2d-43          [-1, 512, 28, 28]          65,536\n      BatchNorm2d-44          [-1, 512, 28, 28]           1,024\n           Conv2d-45          [-1, 512, 28, 28]         131,072\n      BatchNorm2d-46          [-1, 512, 28, 28]           1,024\n             ReLU-47          [-1, 512, 28, 28]               0\n       Bottleneck-48          [-1, 512, 28, 28]               0\n           Conv2d-49          [-1, 128, 28, 28]          65,536\n      BatchNorm2d-50          [-1, 128, 28, 28]             256\n             ReLU-51          [-1, 128, 28, 28]               0\n           Conv2d-52          [-1, 128, 28, 28]         147,456\n      BatchNorm2d-53          [-1, 128, 28, 28]             256\n             ReLU-54          [-1, 128, 28, 28]               0\n           Conv2d-55          [-1, 512, 28, 28]          65,536\n      BatchNorm2d-56          [-1, 512, 28, 28]           1,024\n             ReLU-57          [-1, 512, 28, 28]               0\n       Bottleneck-58          [-1, 512, 28, 28]               0\n           Conv2d-59          [-1, 128, 28, 28]          65,536\n      BatchNorm2d-60          [-1, 128, 28, 28]             256\n             ReLU-61          [-1, 128, 28, 28]               0\n           Conv2d-62          [-1, 128, 28, 28]         147,456\n      BatchNorm2d-63          [-1, 128, 28, 28]             256\n             ReLU-64          [-1, 128, 28, 28]               0\n           Conv2d-65          [-1, 512, 28, 28]          65,536\n      BatchNorm2d-66          [-1, 512, 28, 28]           1,024\n             ReLU-67          [-1, 512, 28, 28]               0\n       Bottleneck-68          [-1, 512, 28, 28]               0\n           Conv2d-69          [-1, 128, 28, 28]          65,536\n      BatchNorm2d-70          [-1, 128, 28, 28]             256\n             ReLU-71          [-1, 128, 28, 28]               0\n           Conv2d-72          [-1, 128, 28, 28]         147,456\n      BatchNorm2d-73          [-1, 128, 28, 28]             256\n             ReLU-74          [-1, 128, 28, 28]               0\n           Conv2d-75          [-1, 512, 28, 28]          65,536\n      BatchNorm2d-76          [-1, 512, 28, 28]           1,024\n             ReLU-77          [-1, 512, 28, 28]               0\n       Bottleneck-78          [-1, 512, 28, 28]               0\n           Conv2d-79          [-1, 256, 28, 28]         131,072\n      BatchNorm2d-80          [-1, 256, 28, 28]             512\n             ReLU-81          [-1, 256, 28, 28]               0\n           Conv2d-82          [-1, 256, 14, 14]         589,824\n      BatchNorm2d-83          [-1, 256, 14, 14]             512\n             ReLU-84          [-1, 256, 14, 14]               0\n           Conv2d-85         [-1, 1024, 14, 14]         262,144\n      BatchNorm2d-86         [-1, 1024, 14, 14]           2,048\n           Conv2d-87         [-1, 1024, 14, 14]         524,288\n      BatchNorm2d-88         [-1, 1024, 14, 14]           2,048\n             ReLU-89         [-1, 1024, 14, 14]               0\n       Bottleneck-90         [-1, 1024, 14, 14]               0\n           Conv2d-91          [-1, 256, 14, 14]         262,144\n      BatchNorm2d-92          [-1, 256, 14, 14]             512\n             ReLU-93          [-1, 256, 14, 14]               0\n           Conv2d-94          [-1, 256, 14, 14]         589,824\n      BatchNorm2d-95          [-1, 256, 14, 14]             512\n             ReLU-96          [-1, 256, 14, 14]               0\n           Conv2d-97         [-1, 1024, 14, 14]         262,144\n      BatchNorm2d-98         [-1, 1024, 14, 14]           2,048\n             ReLU-99         [-1, 1024, 14, 14]               0\n      Bottleneck-100         [-1, 1024, 14, 14]               0\n          Conv2d-101          [-1, 256, 14, 14]         262,144\n     BatchNorm2d-102          [-1, 256, 14, 14]             512\n            ReLU-103          [-1, 256, 14, 14]               0\n          Conv2d-104          [-1, 256, 14, 14]         589,824\n     BatchNorm2d-105          [-1, 256, 14, 14]             512\n            ReLU-106          [-1, 256, 14, 14]               0\n          Conv2d-107         [-1, 1024, 14, 14]         262,144\n     BatchNorm2d-108         [-1, 1024, 14, 14]           2,048\n            ReLU-109         [-1, 1024, 14, 14]               0\n      Bottleneck-110         [-1, 1024, 14, 14]               0\n          Conv2d-111          [-1, 256, 14, 14]         262,144\n     BatchNorm2d-112          [-1, 256, 14, 14]             512\n            ReLU-113          [-1, 256, 14, 14]               0\n          Conv2d-114          [-1, 256, 14, 14]         589,824\n     BatchNorm2d-115          [-1, 256, 14, 14]             512\n            ReLU-116          [-1, 256, 14, 14]               0\n          Conv2d-117         [-1, 1024, 14, 14]         262,144\n     BatchNorm2d-118         [-1, 1024, 14, 14]           2,048\n            ReLU-119         [-1, 1024, 14, 14]               0\n      Bottleneck-120         [-1, 1024, 14, 14]               0\n          Conv2d-121          [-1, 256, 14, 14]         262,144\n     BatchNorm2d-122          [-1, 256, 14, 14]             512\n            ReLU-123          [-1, 256, 14, 14]               0\n          Conv2d-124          [-1, 256, 14, 14]         589,824\n     BatchNorm2d-125          [-1, 256, 14, 14]             512\n            ReLU-126          [-1, 256, 14, 14]               0\n          Conv2d-127         [-1, 1024, 14, 14]         262,144\n     BatchNorm2d-128         [-1, 1024, 14, 14]           2,048\n            ReLU-129         [-1, 1024, 14, 14]               0\n      Bottleneck-130         [-1, 1024, 14, 14]               0\n          Conv2d-131          [-1, 256, 14, 14]         262,144\n     BatchNorm2d-132          [-1, 256, 14, 14]             512\n            ReLU-133          [-1, 256, 14, 14]               0\n          Conv2d-134          [-1, 256, 14, 14]         589,824\n     BatchNorm2d-135          [-1, 256, 14, 14]             512\n            ReLU-136          [-1, 256, 14, 14]               0\n          Conv2d-137         [-1, 1024, 14, 14]         262,144\n     BatchNorm2d-138         [-1, 1024, 14, 14]           2,048\n            ReLU-139         [-1, 1024, 14, 14]               0\n      Bottleneck-140         [-1, 1024, 14, 14]               0\n          Conv2d-141          [-1, 512, 14, 14]         524,288\n     BatchNorm2d-142          [-1, 512, 14, 14]           1,024\n            ReLU-143          [-1, 512, 14, 14]               0\n          Conv2d-144            [-1, 512, 7, 7]       2,359,296\n     BatchNorm2d-145            [-1, 512, 7, 7]           1,024\n            ReLU-146            [-1, 512, 7, 7]               0\n          Conv2d-147           [-1, 2048, 7, 7]       1,048,576\n     BatchNorm2d-148           [-1, 2048, 7, 7]           4,096\n          Conv2d-149           [-1, 2048, 7, 7]       2,097,152\n     BatchNorm2d-150           [-1, 2048, 7, 7]           4,096\n            ReLU-151           [-1, 2048, 7, 7]               0\n      Bottleneck-152           [-1, 2048, 7, 7]               0\n          Conv2d-153            [-1, 512, 7, 7]       1,048,576\n     BatchNorm2d-154            [-1, 512, 7, 7]           1,024\n            ReLU-155            [-1, 512, 7, 7]               0\n          Conv2d-156            [-1, 512, 7, 7]       2,359,296\n     BatchNorm2d-157            [-1, 512, 7, 7]           1,024\n            ReLU-158            [-1, 512, 7, 7]               0\n          Conv2d-159           [-1, 2048, 7, 7]       1,048,576\n     BatchNorm2d-160           [-1, 2048, 7, 7]           4,096\n            ReLU-161           [-1, 2048, 7, 7]               0\n      Bottleneck-162           [-1, 2048, 7, 7]               0\n          Conv2d-163            [-1, 512, 7, 7]       1,048,576\n     BatchNorm2d-164            [-1, 512, 7, 7]           1,024\n            ReLU-165            [-1, 512, 7, 7]               0\n          Conv2d-166            [-1, 512, 7, 7]       2,359,296\n     BatchNorm2d-167            [-1, 512, 7, 7]           1,024\n            ReLU-168            [-1, 512, 7, 7]               0\n          Conv2d-169           [-1, 2048, 7, 7]       1,048,576\n     BatchNorm2d-170           [-1, 2048, 7, 7]           4,096\n            ReLU-171           [-1, 2048, 7, 7]               0\n      Bottleneck-172           [-1, 2048, 7, 7]               0\nAdaptiveAvgPool2d-173           [-1, 2048, 1, 1]               0\n          Linear-174                    [-1, 3]           6,147\n================================================================\nTotal params: 23,514,179\nTrainable params: 23,514,179\nNon-trainable params: 0\n----------------------------------------------------------------\nInput size (MB): 0.57\nForward/backward pass size (MB): 286.55\nParams size (MB): 89.70\nEstimated Total Size (MB): 376.82\n----------------------------------------------------------------\n\n\n모델의 구조를 확인해보니 resnet 기반의 output이 3개인 애호박 분류기 모델이었습니다.\n\n\n\n샘플 데이터셋 구축 (애호박)\ndocker/transforms.py 에서 직접 확인해보니, 학습된 모델의 이미지 전처리 파이프라인은 다음과 같았습니다. 똑같이 test_aug변수에 넣어 데이터셋을 구축하였습니다.\n\ntest_aug = albumentations.Compose([\n    albumentations.Resize(512, 512),\n    albumentations.Normalize(mean=[0.485, 0.456, 0.406],\n                             std=[0.229, 0.224, 0.225],\n                             max_pixel_value=255.0,\n                             p=1.0)], p=1.0)\n\n\n# validation\nwith open(\"data_preprocessing/dic_img2label_val.pickle\",\"rb\") as fr:\n    dic_img2label_val = pickle.load(fr)\n\n\n# 애호박 데이터만 불러오기 \nlst_valid_img = \"data/validation/images/애호박\"\n\n\ndict_label = {0: 0, 7: 1, 8: 2}\n\nvalidX = []\nvalidY = []\nlst_img = []\n\nfor img_name in tqdm(lst_valid_img): \n    np_img = np.array(Image.open(path_valid_img + img_name))\n    transformed_img = test_aug(image=np_img)['image']\n    \n    validX.append(transformed_img) \n    validY.append(dict_label[dic_img2label_val[img_name]['disease']])\n    lst_img.append(img_name)\n\n100%|██████████| 1479/1479 [02:07<00:00, 11.59it/s]\n\n\nvalidation 데이터의 독립변수, 종속변수 리스트를 생성하고, 후에 바운딩 박스를 그리기 위하여 이미지 이름 리스트도 함께 생성하였습니다.\n\n\n\n모델 평가\n\nbatch_size = 32\nvalidX_batches = [validX[i:i+batch_size] for i in range(0, len(validX), batch_size)]\nvalidY_batches = [validY[i:i+batch_size] for i in range(0, len(validY), batch_size)]\n\n\nnp.array(validX_batches[0]).shape\n\n(32, 512, 512, 3)\n\n\n배치 사이즈를 32로 하였고, 크기를 확인하니 (32, 512, 512, 3)과 같이 정상적으로 적용된 것을 확인하였습니다.\n\nlst_preds = [] \n\nwith torch.no_grad():\n    for i, (X_batch, Y_batch) in enumerate(tqdm(zip(validX_batches, validY_batches))):\n        X_batch = torch.tensor(X_batch).to(device)\n        Y_batch = torch.tensor(Y_batch).to(device) \n        \n        new_shape = (len(X_batch), 3, 512, 512)\n        X_batch = X_batch.permute(0, 3, 1, 2)  # channel을 맨 앞으로 보내기 위해 permute\n        X_batch = X_batch.view(new_shape)\n        \n        preds = model(X_batch)\n        \n        _, pred_labels = torch.max(preds, dim=1) \n        \n        # 미니배치 마다의 preds 추가 \n        lst_preds.extend(pred_labels.cpu().tolist()) \n        \n\n# 전체 데이터의 평가 지표 계산 \naccuracy = accuracy_score(validY, lst_preds)\nprecision = precision_score(validY, lst_preds, average='macro')\nrecall = recall_score(validY, lst_preds, average='macro')\nf1 = f1_score(validY, lst_preds, average='macro')\n\nprint('Accuracy:', accuracy)\nprint('Precision:', precision)\nprint('Recall:', recall)\nprint('F1 score:', f1)\n\n47it [12:01, 15.36s/it]\n\n\nAccuracy: 0.530764029749831\nPrecision: 0.429514860306709\nRecall: 0.7757727491003014\nF1 score: 0.3874735945413221\n\n\n\n\n\naihub의 애호박 분류기 모델을 평가해본 결과 정확도 약 53%, 나머지 지표도 그리 좋은 성능을 보이지는 않았습니다. 이후에 학습을 진행할 때는, 이를 기준으로 더 높은 지표 결과가 나오도록 학습을 진행하는 것을 목표로 잡았습니다.\n\n\n\n이미지로 확인해보기\n\nlst_nums = random.sample(range(len(lst_img)), 9)\n\ndict_name = {0: 0, 1: 7, 2: 8}\n\nfig = plt.figure(figsize=(13, 13))\naxes = fig.subplots(3, 3).flatten()\n\nfor i, num in enumerate(lst_nums): \n    img_name = lst_img[num]\n    img = Image.open(path_valid_img + img_name)\n\n    d1 = dic_img2label_val[img_name]\n\n    crop, disease, points = d1['crop'], d1['disease'], d1['points']\n    pred = dict_name[lst_preds[num]]\n\n    draw = ImageDraw.Draw(img)\n    draw.rectangle([(points['xtl'], points['ytl']), (points['xbr'], points['ybr'])], outline=(255, 0, 255), width=30)\n\n    axes[i].set_title(f\"실제: {disease2name[disease]}  ||  예측: {disease2name[pred]}\")\n    axes[i].imshow(np.array(img))\n\n\n\n\n실제 이미지를 랜덤으로 9장을 뽑아봤을 때, 꽤 잘 예측하는 듯 합니다. 정확도는 50%지만, 운이 좋게 잘 뽑힌 것 같습니다. 또, 애호박이라도 부위가 다르다는 점도 확인하였습니다. 잎도 있고 줄기도 있고, 꽃도 있고, 열매도 있기에, 모델 학습이 제대로 되지 않은 원인도 있어보입니다.\n지금까지 AIHUB 샘플 모델을 확인해 보았고, 다음 글에서는 EDA에서 처리해 놓은 20개의 레이블에 대한 모델 학습을 직접 진행해보도록 하겠습니다."
  },
  {
    "objectID": "posts/c03.image_classification.html#작물-질병-진단-모델-학습-및-평가-mobilenetv2",
    "href": "posts/c03.image_classification.html#작물-질병-진단-모델-학습-및-평가-mobilenetv2",
    "title": "[cropdoctor] 3. 작물 질병 진단 모델 학습 및 평가 (mobilenetV2)",
    "section": "3. 작물 질병 진단 모델 학습 및 평가 (mobilenetV2)",
    "text": "3. 작물 질병 진단 모델 학습 및 평가 (mobilenetV2)\n\nimport pickle \nimport os \nfrom tqdm import tqdm\nimport random  \nimport time   \n    \nfrom PIL import Image, ImageDraw, ImageFile \n\nimport albumentations\n\nimport matplotlib.pyplot as plt               \nplt.rc(\"font\", family=\"NanumGothic\", size=13) \n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\nfrom sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torchvision.models as models\nfrom torchsummary import summary\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision.transforms as transforms\nimport torchvision.datasets as datasets\nfrom torchvision.datasets import ImageFolder\nfrom torch.utils.data import DataLoader\nfrom torch.utils.tensorboard import SummaryWriter\nfrom torch.utils.data.sampler import WeightedRandomSampler\nimport torchvision.transforms as transforms\n\nImageFile.LOAD_TRUNCATED_IMAGES = True"
  },
  {
    "objectID": "posts/c03.image_classification.html#모델-학습",
    "href": "posts/c03.image_classification.html#모델-학습",
    "title": "[cropdoctor] 3. 작물 질병 진단 모델 학습 및 평가 (mobilenetV2)",
    "section": "01. 모델 학습",
    "text": "01. 모델 학습\n이번 글에서는 노지작물의 질병을 진단하는 모델을 학습합니다. 이번 글에서는 베이스라인 모델로, Image Classification 방법을 사용합니다. 그 다음, 사용하는 데이터셋의 라벨 데이터에 바운딩 박스가 포함되어 있으므로, Object Detection 모델 또한 학습해 볼 예정입니다.\n먼저, Image Classification 전이학습을 진행하며, 사전학습 모델은 mobilenet v2을 사용합니다. mobilenet V2는 경량화된 아키텍처로, 파라미터 수를 줄이고 연산량을 최적화된 모델입니다. 작은 모델 크기와 적은 연산 요구로 제한된 자원을 가진 환경에서도 효율적으로 실행될 수 있습니다. 작은 모델 크기와 낮은 연산 요구를 유지하면서도 정확한 예측 결과를 얻을 수 있으므로, 프로젝트에서 제공받은 VM의 성능에서 무리 없이 돌아가게 하면서도, 높은 분류 성능을 제공해주므로 해당 모델을 선정하였습니다.\n시간이나 용량 등 물리적 자원이 여유로웠다면 ImageNet의 여러 모델로 학습시켜 성능을 비교해보고 싶었지만, 그럴만한 여유는 없었기에 mobilenet V2 모델로 간단한 파라미터 조정을 통한 비교만 진행합니다.\n\n\n\nimage.png\n\n\n위의 도식화 사진과 같이, 이미지의 input 사이즈는 (224, 224, 3)으로 두었고, output은 우리 데이터 레이블 개수에 맞게 (1, 20)으로 설정하여 모델을 구성하였습니다.\n\n\n모델 학습 코드\nmobilenetV2_1.py\n\n# Device 정의\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# 하이퍼 파라미터 정의\nnum_classes = 20\nbatch_size = 64\nlearning_rate = 0.001\nnum_epochs = 10\n\n# 사전학습된 mobilenet V2 불러오기 \nmodel = torch.hub.load('pytorch/vision:v0.9.0', 'mobilenet_v2', pretrained=True)\n\n# 마지막 FC 레이어를 우리 데이터셋에 맞게 조정해주기 \nnum_ftrs = model.classifier[-1].in_features\nmodel.classifier[-1] = nn.Linear(num_ftrs, num_classes)\nmodel = model.to(device)\n\n# loss function 과 optimizer 를 설정 \ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=learning_rate)\n\n\n# 입력 이미지 데이터셋 전처리 (사이즈 224, 224, tensor 타입 변경, 스케일 표준화)\ndata_transforms = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])\n\n\n\n# 데이터셋 경로 설정 및 데이터셋 불러오기 \npath_train_img = \"data/training/image_class/\"\npath_valid_img = \"data/validation/image_class/\"\n\ntrain_dataset = ImageFolder(root=path_train_img, transform=data_transforms) # 데이터 증강 O\ntest_dataset = ImageFolder(root=path_valid_img, transform=data_transforms) # test_dataset -> val_dataset, 데이터 증강 X\ntrain_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\ntest_loader = DataLoader(dataset=test_dataset, batch_size=1, shuffle=True) # test -> val, batch = 1\n\n\n\n# TensorBoard writer를 통해 학습과정을 시각화해보기\nwriter = SummaryWriter()\n\n# 모델 학습\ntotal_step = len(train_loader)\nstart_time = time.time()\nfor epoch in range(num_epochs):\n    model.train() # 학습 모드로 설정 \n    for i, (images, labels) in tqdm(enumerate(train_loader), total=len(train_loader), leave=False):\n        images = images.to(device)\n        labels = labels.to(device)\n        \n        # 순전파\n        outputs = model(images)\n        loss = criterion(outputs, labels)\n\n        # 역전파 및 optimize\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        # TensorBoard에서 accuracy 와 loss 추적  \n        _, predicted = torch.max(outputs.data, 1)\n        total = labels.size(0)\n        correct = (predicted == labels).sum().item()\n        acc = 100 * correct / total\n        writer.add_scalar('Train/Loss', loss.item(), epoch * total_step + i)\n        writer.add_scalar('Train/Accuracy', acc, epoch * total_step + i)\n    writer.flush()\n    \n    \n    model.eval() # 평가 모드로 설정 \n    # validation 데이터셋으로 모델 평가하기 \n    with torch.no_grad():\n        correct = 0\n        total = 0\n        for i, (images, labels) in tqdm(enumerate(test_loader), total=len(test_loader), leave=False):\n            images = images.to(device)\n            labels = labels.to(device)\n            outputs = model(images)\n            _, predicted = torch.max(outputs.data, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n\n        test_acc = 100 * correct / total\n        writer.add_scalar('Test/Accuracy', test_acc, epoch)\n        writer.flush()\n\n    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}, Training Accuracy: {acc:.2f}%, Test Accuracy: {test_acc:.2f}%')\n\nend_time = time.time()\nprint(f'Training complete in {(end_time - start_time):.0f} seconds')\n\n\n# 모델 정보가 담긴 가중치 파일을 ckpt 확장자로 저장\ntorch.save(model.state_dict(), 'model/mobilenet_v2_1.ckpt')\nprint('Model saved as mobilenet_v2_1.ckpt')\n\n\n# Tensorboard writer 종료 \nwriter.close()\n\n\n\n\n💡 tmux 사용\n제공받은 VM 환경에서 1 epoch 당 약 1시간 정도의 시간이 걸렸습니다. 10 epoch만 돌려도 10시간 이상이 걸렸기에, 서버가 재부팅되지 않는 이상 학습 중인 세션이 종료되지 않을 수 있는 tmux를 사용하여 모델학습을 진행하였습니다.\n1. 설치\n\nsudo apt-get install tmux\n\n\n2. 세션 시작\n\ntmux new -s NAME\n\n위와 같이 tmux 세션을 시작합니다.\n\n3. 모델 학습 실행\n\npython mobilenetV2_1.py\n\n시작된 tmux 세션 안의 터미널에서 위의 명령어로 모델 학습을 진행시켰습니다."
  },
  {
    "objectID": "posts/c03.image_classification.html#모델-평가",
    "href": "posts/c03.image_classification.html#모델-평가",
    "title": "[cropdoctor] 3. 작물 질병 진단 모델 학습 및 평가 (mobilenetV2)",
    "section": "02. 모델 평가",
    "text": "02. 모델 평가\n\n\n이름 변환 딕셔너리\n\ndisease2name_new = {\n               0: \"고추정상\",\n               1: \"고추탄저병\",\n               2: \"고추흰가루병\",\n               \n               3: \"무정상\",\n               4: \"무검은무늬병\", \n               5: \"무노균병\", \n    \n               6: \"배추정상\",\n               7: \"배추검음썩음병\",\n               8: \"배추노균병\",\n    \n               9: \"애호박정상\",\n               10: \"애호박노균병\",\n               11: \"애호박흰가루병\", \n               \n               12: \"콩정상\",\n               13: \"콩불마름병\",\n               14: \"콩점무늬병\",\n    \n               15: \"토마토정상\",\n               16: \"토마토잎마름병\", \n    \n               17: \"호박정상\",\n               18: \"호박노균병\",\n               19: \"호박흰가루병\"}\n\nlst_label_name = [\"고추정상\", \"고추탄저병\", \"고추흰가루병\", \n                    \"무정상\", \"무검은무늬병\", \"무노균병\", \n                    \"배추정상\", \"배추검음썩음병\", \"배추노균병\", \n                    \"애호박정상\", \"애호박노균병\", \"애호박흰가루병\", \n                    \"콩정상\", \"콩불마름병\", \"콩점무늬병\", \n                    \"토마토정상\", \"토마토잎마름병\", \n                    \"호박정상\", \"호박노균병\", \"호박흰가루병\"]\n\n\n\n\n모델 요약\n\nmodel_path = \"model/mobilenet_v2_1.ckpt\"\n  \ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nmodel_weights = torch.load(model_path, map_location=device)\n\n학습하여 저장된 ckpt 가중치 파일을 불러옵니다.\n\n\n\n모델 생성\n\nmodel = torch.hub.load('pytorch/vision:v0.9.0', 'mobilenet_v2', pretrained=False)\nnum_ftrs = model.classifier[-1].in_features\nmodel.classifier[-1] = nn.Linear(num_ftrs, 20)\nmodel.load_state_dict(model_weights)\nmodel = model.to(device)\n\nUsing cache found in /home/elicer/.cache/torch/hub/pytorch_vision_v0.9.0\n\n\n불러온 모델의 가중치 파일을 학습한 모델 구조에 맞게 불러옵니다.\n\n\n\n모델 요약\n\nsummary(model, input_size=(3, 224, 224))\n\n----------------------------------------------------------------\n        Layer (type)               Output Shape         Param #\n================================================================\n            Conv2d-1         [-1, 32, 112, 112]             864\n       BatchNorm2d-2         [-1, 32, 112, 112]              64\n             ReLU6-3         [-1, 32, 112, 112]               0\n            Conv2d-4         [-1, 32, 112, 112]             288\n       BatchNorm2d-5         [-1, 32, 112, 112]              64\n             ReLU6-6         [-1, 32, 112, 112]               0\n            Conv2d-7         [-1, 16, 112, 112]             512\n       BatchNorm2d-8         [-1, 16, 112, 112]              32\n  InvertedResidual-9         [-1, 16, 112, 112]               0\n           Conv2d-10         [-1, 96, 112, 112]           1,536\n      BatchNorm2d-11         [-1, 96, 112, 112]             192\n            ReLU6-12         [-1, 96, 112, 112]               0\n           Conv2d-13           [-1, 96, 56, 56]             864\n      BatchNorm2d-14           [-1, 96, 56, 56]             192\n            ReLU6-15           [-1, 96, 56, 56]               0\n           Conv2d-16           [-1, 24, 56, 56]           2,304\n      BatchNorm2d-17           [-1, 24, 56, 56]              48\n InvertedResidual-18           [-1, 24, 56, 56]               0\n           Conv2d-19          [-1, 144, 56, 56]           3,456\n      BatchNorm2d-20          [-1, 144, 56, 56]             288\n            ReLU6-21          [-1, 144, 56, 56]               0\n           Conv2d-22          [-1, 144, 56, 56]           1,296\n      BatchNorm2d-23          [-1, 144, 56, 56]             288\n            ReLU6-24          [-1, 144, 56, 56]               0\n           Conv2d-25           [-1, 24, 56, 56]           3,456\n      BatchNorm2d-26           [-1, 24, 56, 56]              48\n InvertedResidual-27           [-1, 24, 56, 56]               0\n           Conv2d-28          [-1, 144, 56, 56]           3,456\n      BatchNorm2d-29          [-1, 144, 56, 56]             288\n            ReLU6-30          [-1, 144, 56, 56]               0\n           Conv2d-31          [-1, 144, 28, 28]           1,296\n      BatchNorm2d-32          [-1, 144, 28, 28]             288\n            ReLU6-33          [-1, 144, 28, 28]               0\n           Conv2d-34           [-1, 32, 28, 28]           4,608\n      BatchNorm2d-35           [-1, 32, 28, 28]              64\n InvertedResidual-36           [-1, 32, 28, 28]               0\n           Conv2d-37          [-1, 192, 28, 28]           6,144\n      BatchNorm2d-38          [-1, 192, 28, 28]             384\n            ReLU6-39          [-1, 192, 28, 28]               0\n           Conv2d-40          [-1, 192, 28, 28]           1,728\n      BatchNorm2d-41          [-1, 192, 28, 28]             384\n            ReLU6-42          [-1, 192, 28, 28]               0\n           Conv2d-43           [-1, 32, 28, 28]           6,144\n      BatchNorm2d-44           [-1, 32, 28, 28]              64\n InvertedResidual-45           [-1, 32, 28, 28]               0\n           Conv2d-46          [-1, 192, 28, 28]           6,144\n      BatchNorm2d-47          [-1, 192, 28, 28]             384\n            ReLU6-48          [-1, 192, 28, 28]               0\n           Conv2d-49          [-1, 192, 28, 28]           1,728\n      BatchNorm2d-50          [-1, 192, 28, 28]             384\n            ReLU6-51          [-1, 192, 28, 28]               0\n           Conv2d-52           [-1, 32, 28, 28]           6,144\n      BatchNorm2d-53           [-1, 32, 28, 28]              64\n InvertedResidual-54           [-1, 32, 28, 28]               0\n           Conv2d-55          [-1, 192, 28, 28]           6,144\n      BatchNorm2d-56          [-1, 192, 28, 28]             384\n            ReLU6-57          [-1, 192, 28, 28]               0\n           Conv2d-58          [-1, 192, 14, 14]           1,728\n      BatchNorm2d-59          [-1, 192, 14, 14]             384\n            ReLU6-60          [-1, 192, 14, 14]               0\n           Conv2d-61           [-1, 64, 14, 14]          12,288\n      BatchNorm2d-62           [-1, 64, 14, 14]             128\n InvertedResidual-63           [-1, 64, 14, 14]               0\n           Conv2d-64          [-1, 384, 14, 14]          24,576\n      BatchNorm2d-65          [-1, 384, 14, 14]             768\n            ReLU6-66          [-1, 384, 14, 14]               0\n           Conv2d-67          [-1, 384, 14, 14]           3,456\n      BatchNorm2d-68          [-1, 384, 14, 14]             768\n            ReLU6-69          [-1, 384, 14, 14]               0\n           Conv2d-70           [-1, 64, 14, 14]          24,576\n      BatchNorm2d-71           [-1, 64, 14, 14]             128\n InvertedResidual-72           [-1, 64, 14, 14]               0\n           Conv2d-73          [-1, 384, 14, 14]          24,576\n      BatchNorm2d-74          [-1, 384, 14, 14]             768\n            ReLU6-75          [-1, 384, 14, 14]               0\n           Conv2d-76          [-1, 384, 14, 14]           3,456\n      BatchNorm2d-77          [-1, 384, 14, 14]             768\n            ReLU6-78          [-1, 384, 14, 14]               0\n           Conv2d-79           [-1, 64, 14, 14]          24,576\n      BatchNorm2d-80           [-1, 64, 14, 14]             128\n InvertedResidual-81           [-1, 64, 14, 14]               0\n           Conv2d-82          [-1, 384, 14, 14]          24,576\n      BatchNorm2d-83          [-1, 384, 14, 14]             768\n            ReLU6-84          [-1, 384, 14, 14]               0\n           Conv2d-85          [-1, 384, 14, 14]           3,456\n      BatchNorm2d-86          [-1, 384, 14, 14]             768\n            ReLU6-87          [-1, 384, 14, 14]               0\n           Conv2d-88           [-1, 64, 14, 14]          24,576\n      BatchNorm2d-89           [-1, 64, 14, 14]             128\n InvertedResidual-90           [-1, 64, 14, 14]               0\n           Conv2d-91          [-1, 384, 14, 14]          24,576\n      BatchNorm2d-92          [-1, 384, 14, 14]             768\n            ReLU6-93          [-1, 384, 14, 14]               0\n           Conv2d-94          [-1, 384, 14, 14]           3,456\n      BatchNorm2d-95          [-1, 384, 14, 14]             768\n            ReLU6-96          [-1, 384, 14, 14]               0\n           Conv2d-97           [-1, 96, 14, 14]          36,864\n      BatchNorm2d-98           [-1, 96, 14, 14]             192\n InvertedResidual-99           [-1, 96, 14, 14]               0\n          Conv2d-100          [-1, 576, 14, 14]          55,296\n     BatchNorm2d-101          [-1, 576, 14, 14]           1,152\n           ReLU6-102          [-1, 576, 14, 14]               0\n          Conv2d-103          [-1, 576, 14, 14]           5,184\n     BatchNorm2d-104          [-1, 576, 14, 14]           1,152\n           ReLU6-105          [-1, 576, 14, 14]               0\n          Conv2d-106           [-1, 96, 14, 14]          55,296\n     BatchNorm2d-107           [-1, 96, 14, 14]             192\nInvertedResidual-108           [-1, 96, 14, 14]               0\n          Conv2d-109          [-1, 576, 14, 14]          55,296\n     BatchNorm2d-110          [-1, 576, 14, 14]           1,152\n           ReLU6-111          [-1, 576, 14, 14]               0\n          Conv2d-112          [-1, 576, 14, 14]           5,184\n     BatchNorm2d-113          [-1, 576, 14, 14]           1,152\n           ReLU6-114          [-1, 576, 14, 14]               0\n          Conv2d-115           [-1, 96, 14, 14]          55,296\n     BatchNorm2d-116           [-1, 96, 14, 14]             192\nInvertedResidual-117           [-1, 96, 14, 14]               0\n          Conv2d-118          [-1, 576, 14, 14]          55,296\n     BatchNorm2d-119          [-1, 576, 14, 14]           1,152\n           ReLU6-120          [-1, 576, 14, 14]               0\n          Conv2d-121            [-1, 576, 7, 7]           5,184\n     BatchNorm2d-122            [-1, 576, 7, 7]           1,152\n           ReLU6-123            [-1, 576, 7, 7]               0\n          Conv2d-124            [-1, 160, 7, 7]          92,160\n     BatchNorm2d-125            [-1, 160, 7, 7]             320\nInvertedResidual-126            [-1, 160, 7, 7]               0\n          Conv2d-127            [-1, 960, 7, 7]         153,600\n     BatchNorm2d-128            [-1, 960, 7, 7]           1,920\n           ReLU6-129            [-1, 960, 7, 7]               0\n          Conv2d-130            [-1, 960, 7, 7]           8,640\n     BatchNorm2d-131            [-1, 960, 7, 7]           1,920\n           ReLU6-132            [-1, 960, 7, 7]               0\n          Conv2d-133            [-1, 160, 7, 7]         153,600\n     BatchNorm2d-134            [-1, 160, 7, 7]             320\nInvertedResidual-135            [-1, 160, 7, 7]               0\n          Conv2d-136            [-1, 960, 7, 7]         153,600\n     BatchNorm2d-137            [-1, 960, 7, 7]           1,920\n           ReLU6-138            [-1, 960, 7, 7]               0\n          Conv2d-139            [-1, 960, 7, 7]           8,640\n     BatchNorm2d-140            [-1, 960, 7, 7]           1,920\n           ReLU6-141            [-1, 960, 7, 7]               0\n          Conv2d-142            [-1, 160, 7, 7]         153,600\n     BatchNorm2d-143            [-1, 160, 7, 7]             320\nInvertedResidual-144            [-1, 160, 7, 7]               0\n          Conv2d-145            [-1, 960, 7, 7]         153,600\n     BatchNorm2d-146            [-1, 960, 7, 7]           1,920\n           ReLU6-147            [-1, 960, 7, 7]               0\n          Conv2d-148            [-1, 960, 7, 7]           8,640\n     BatchNorm2d-149            [-1, 960, 7, 7]           1,920\n           ReLU6-150            [-1, 960, 7, 7]               0\n          Conv2d-151            [-1, 320, 7, 7]         307,200\n     BatchNorm2d-152            [-1, 320, 7, 7]             640\nInvertedResidual-153            [-1, 320, 7, 7]               0\n          Conv2d-154           [-1, 1280, 7, 7]         409,600\n     BatchNorm2d-155           [-1, 1280, 7, 7]           2,560\n           ReLU6-156           [-1, 1280, 7, 7]               0\n         Dropout-157                 [-1, 1280]               0\n          Linear-158                   [-1, 20]          25,620\n================================================================\nTotal params: 2,249,492\nTrainable params: 2,249,492\nNon-trainable params: 0\n----------------------------------------------------------------\nInput size (MB): 0.57\nForward/backward pass size (MB): 152.86\nParams size (MB): 8.58\nEstimated Total Size (MB): 162.02\n----------------------------------------------------------------\n\n\ninput이 (3, 224, 224)이고, output이 (1, 20)인 mobilenet V2 전이학습 모델의 구조임을 확인합니다.\n\n\n\n모델 평가\n모델 학습 시 에폭마다 정확도를 출력해주었는데, 해당 코드와 거의 유사합니다. 몇가지 추가된 점은, 정확도 외의 정밀도, 재현율, f1 score 지표 계산과, 혼동행렬의 시각화입니다.\n\n\n데이터 로드 및 전처리\n\nvalid_transform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])\n\n\npath_valid_img = \"data/validation/image_class/\"\n\nvalid_dataset = ImageFolder(root=path_valid_img, transform=valid_transform)\nvalid_loader = DataLoader(dataset=valid_dataset, batch_size=1)\n\n학습할 때 train 데이터와 마찬가지로 224x224로 리사이징하며, 스케일을 표준화 하여 validation 이미지 데이터를 불러옵니다.\n\n\n\n모델 예측\n\nmodel.eval() # 중요 \nwith torch.no_grad():\n    lst_pred = []\n    lst_labels = []\n\n    correct = 0\n    total = 0\n    \n    for i, (images, labels) in tqdm(enumerate(valid_loader), total=len(valid_loader)):\n        images = images.to(device)\n        labels = labels.to(device)\n        outputs = model(images)\n        _, predicted = torch.max(outputs.data, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n        lst_pred.extend(predicted.cpu().tolist())\n        lst_labels.extend(labels.cpu().tolist())\n\n100%|██████████| 7873/7873 [19:50<00:00,  6.62it/s]\n\n\n불러온 모델과 데이터로 예측을 진행합니다. 밑에서 평가지표와 혼동행렬을 구하기 위해 예측 값과 정답값을 각각 lst_pred, lst_labels 리스트에 추가합니다.\n\n\n\n평가 지표\n\n# 전체 validation 데이터셋의 평가 지표 계산 \naccuracy = accuracy_score(lst_labels, lst_pred)\nprecision = precision_score(lst_labels, lst_pred, average='macro')\nrecall = recall_score(lst_labels, lst_pred, average='macro')\nf1 = f1_score(lst_labels, lst_pred, average='macro')\n\n\nprint('Accuracy:', accuracy)\nprint('Precision:', precision)\nprint('Recall:', recall)\nprint('F1 score:', f1)\n\n\nfig = plt.figure(figsize=(5, 3))\nax = fig.subplots()\nbars = ax.bar(range(4), [accuracy, precision, recall, f1], color='#e0a4b2', edgecolor = 'black')\nax.set_xticks(range(4))\nax.set_xticklabels([\"Accuracy\", \"Precision\", \"Recall\", \"F1 score\"], fontsize=12)\nax.set_ylim(0, 1.1)\n\nplt.title(\"mobilenet_v2_1 평가지표\", fontsize=13, pad=10)\n\nfor i, b in enumerate(bars.patches):\n    _=plt.text(b.get_x()+b.get_width()*(1/2),b.get_height()+0.05, \\\n            round(b.get_height(), 2),ha='center',fontsize=12, color='k')\n\nplt.show()\n\nAccuracy: 0.9350946272069097\nPrecision: 0.846919676923046\nRecall: 0.8981016401529176\nF1 score: 0.8572522290077309\n\n\n\n\n\n정확도 0.94, 정밀도 0.85, 재현율 0.9, F1 score 0.86으로, 평가지표의 값은 꽤 높은 수치를 보였습니다.\n\n\n\n혼동행렬, confusion matrix\n\nimport seaborn as sns \n\n\nplt.figure(figsize=(15, 15))\ncm = confusion_matrix(lst_labels, lst_pred)\n_=sns.heatmap(cm, annot=True, fmt='d', cmap='Reds', square=True)\n\n\n\nplt.xticks(np.arange(len(lst_label_name))+0.5, lst_label_name, rotation=45)\nplt.yticks(np.arange(len(lst_label_name))+0.5, lst_label_name, rotation=45)\n\n\n# 그래프 제목, x축 라벨, y축 라벨 설정\nplt.title('Confusion Matrix')\nplt.xlabel('Predicted Label')\nplt.ylabel('True Label')\n\n# 그래프 출력\nplt.show()\n\n\n\n\n40개 이상 잘 못 예측한 레이블은 고추 정상 -> 콩정상, 애호박 정상 -> 콩 정상으로 예측한 값들입니다. 세 작물은 이파리나 줄기가 초록색으며, 비슷하게 생겼다는 점에서 오예측의 가능성이 있을 것으로 예상됩니다.\n\n\n\n\n이미지로 확인해보기\n\n# validation\nwith open(\"data_preprocessing/dic_img2label_valid_sampling.pickle\",\"rb\") as fr:\n    dic_img2label_valid_sampling = pickle.load(fr)\n\n\ndef get_prediction():\n    model.eval()\n    path_valid_img = \"./datasets/validation/images\"\n    lst_img = os.listdir(path_valid_img)\n    lst_random_img = random.sample(lst_img, 9)\n\n    fig = plt.figure(figsize=(13, 13))\n    axes = fig.subplots(3, 3).flatten()\n    fig.suptitle(\"실제 vs 예측 비교\", fontsize=20)\n    \n    \n    for i, img in enumerate(lst_random_img): \n        \n        image = Image.open(path_valid_img+ \"/\" + img)\n        preprocessed_image = valid_transform(image) \n\n        pred_image = preprocessed_image.view(1, 3, 224, 224).to(device) \n\n\n        pred = model(pred_image).data.cuda()\n        _, disease_pred = torch.max(pred, dim=1)\n        disease_pred = int(disease_pred.cpu())\n\n\n        d1 = dic_img2label_valid_sampling[img]\n\n        crop, disease, points = d1['crop'], d1['disease'], d1['points']\n\n        draw = ImageDraw.Draw(image)\n        draw.rectangle([(points['xtl'], points['ytl']), (points['xbr'], points['ybr'])], outline=(255, 0, 255), width=30)\n\n        axes[i].imshow(np.array(image))\n        axes[i].set_title(f\"real: {disease2name_new[disease]}, pred: {disease2name_new[disease_pred]}\", fontsize=15)\n\n\nget_prediction()\n\n\n\n\n이미지로 랜덤 사진을 뽑아 직접 확인해보았는데, 앞서 언급한 애호박 정상과 고추정상이 콩정상으로 예측되는 예시가 모두 나와서 확인해볼 수 있었습니다. 맞게 예측한 고추 정상의 경우 잎의 사진은 모두 잘 맞혔지만, 잘 못 예측한 두 경우는 모두 줄기 사진이었고, 콩 정상의 모습과 비슷한 모양새를 띄는 것으로 보입니다."
  },
  {
    "objectID": "posts/c03.image_classification.html#데이터-증강-및-데이터-불균형-처리",
    "href": "posts/c03.image_classification.html#데이터-증강-및-데이터-불균형-처리",
    "title": "[cropdoctor] 3. 작물 질병 진단 모델 학습 및 평가 (mobilenetV2)",
    "section": "03. 데이터 증강 및 데이터 불균형 처리",
    "text": "03. 데이터 증강 및 데이터 불균형 처리\nEDA에서 불균형과 용량을 줄이기 위해 랜덤샘플링으로, 약 8만장의 이미지 데이터를 약 15000장으로 줄였습니다. 데이터의 수가 감소한 것과, 아직 남아있는 불균형을 처리하기 위해 학습 시 데이터 증강과, WCE 기법을 추가해보도록 하겠습니다.\n위의 mobilenetv2_1.py와 모두 동일하며, 아래 두 부분만 추가하여 학습하였습니다.\n\n\n데이터 증강 추가\n\ntrain_transform = transforms.Compose([\n    transforms.RandomRotation(degrees=30),\n    transforms.RandomResizedCrop(size=(224, 224)),\n    transforms.RandomHorizontalFlip(p=0.5),\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])\n\n\nval_transform = transforms.Compose([ # 이미지 크기, 정규화만 진행 \n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])\n\n학습 데이터셋에만 transform을 적용하여 데이터를 증강합니다. 이 때, 작물 이미지는 색깔에 영향을 크게 받으므로, Colorjitter와 같은 색깔 변형은 주지 않도록 합니다.\n\n\n\nWCE 추가\n\nnSamples = [1000, 973, 915, 1000, 470, 227, 1000, 802, 458, 1000, 543, 484, 1000, 538, 854, 1000, 216, 1001, 208, 226]\nnormedWeights = [1 - (x / sum(nSamples)) for x in nSamples]\nnormedWeights = torch.FloatTensor(normedWeights).to(device)\n\ncriterion = nn.CrossEntropyLoss(weight=normedWeights)\n\n각 클래스 별 개수를 리스트에 적어주고, 그에 따른 가중치를 주는 WCE 작업을 추가해주었습니다.\n\n\n\n모델 평가 결과\n하이퍼 파라미터는 위와 모두 동일했고, 같은 방법으로 모델을 평가한 결과입니다.\n\n평가지표 결과\n\n\n\nimage.png\n\n\n\n\n혼동행렬\n\n\n\nimage.png\n\n\n\n\n이미지 확인\n\n\n\nimage.png\n\n\n모델 평가지표나 혼동행렬을 보면 베이스 모델의 성능이 더 좋은 결과가 나왔습니다. 외부 데이터셋에 더 강건해지게 만들기 위해 두번째 방법의 에폭을 늘려 학습해보는 것도 시도해보고 싶습니다.\n\n사실 프로젝트 시간은 제한되어있고, 학습은 한 번에 10시간 이상이 소요되었기 때문에 epoch 10으로 밖에 하지 못 했지만, 턱 없이 부족한 수라는 생각이 들었습니다. 시간이나 용량 등 물리적 자원의 여유가 된다면 epoch을 50~100 정도로 늘려 학습해보고싶었습니다. 또한 epoch 뿐만 아니라 다른 하이퍼 파라미터 조정이나, 다른 사전학습 모델을 사용하여 실험해보지 못 한 점이 아쉬움으로 남았습니다. 그렇지만 이전에 샘플로 제공받은 aihub 모델의 평가지표와 비교해봤을 때, 성능이 훨씬 괜찮게 나왔고, 이미지로 직접 확인해 보았을 때도 작물의 질병을 꽤 잘 맞히는 것으로 보이는 것으로 만족하려고 합니다. 다음 글에서는 object detection 모델로 학습을 진행한 후, 어떤 모델을 웹 서비스에 서빙할 것인지 선택하도록 하겠습니다."
  },
  {
    "objectID": "posts/c04.object_detection.html#작물-질병-데이터셋으로-커스텀-yolo-모델-학습",
    "href": "posts/c04.object_detection.html#작물-질병-데이터셋으로-커스텀-yolo-모델-학습",
    "title": "[cropdoctor] 4. 작물 질병 데이터셋으로 커스텀 YOLO 모델 학습",
    "section": "4. 작물 질병 데이터셋으로 커스텀 YOLO 모델 학습",
    "text": "4. 작물 질병 데이터셋으로 커스텀 YOLO 모델 학습\n\nimport os\nimport zipfile\nimport random \n\nfrom PIL import Image, ImageDraw\nimport json\nimport pickle\nfrom tqdm import tqdm\nimport numpy as np \n\nimport matplotlib.pyplot as plt\nplt.rc(\"font\", family=\"NanumGothic\", size=13) \n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision.transforms as transforms\nimport torchvision.datasets as datasets\nfrom torch.utils.data import DataLoader\nfrom torch.utils.tensorboard import SummaryWriter\nfrom torch.utils.data.sampler import WeightedRandomSampler\nimport time\nfrom tqdm import tqdm\nimport torchvision.transforms as transforms\nfrom torchvision.datasets import ImageFolder\nimport os\nfrom PIL import ImageFile\nImageFile.LOAD_TRUNCATED_IMAGES = True\nfrom imblearn.over_sampling import SMOTE\nimport torchvision.datasets as datasets\nfrom torchvision.transforms import transforms\nimport numpy as np \n\n2023-05-10 13:15:09.260609: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n2023-05-10 13:15:09.314742: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\nTo enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2023-05-10 13:15:10.776130: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n\n\n \n\n00. 데이터셋 폴더구조 설계\n기존의 image classification pytorch 모델의 폴더구조는 아래와 같습니다. 클래스 별 폴더를 생성하고, ImageFolder와 DataLoader 함수를 사용하여 데이터셋을 불러왔습니다.\n\nimage classifiaction (pytorch) : 클래스 별 폴더 생성\ndata\n  |_ training \n      |_ image_class \n          |_ 고추\n          |_ 고추질병1 \n          |_ ... \n\n\n  |_ vadlidation\n      |_ image_class \n          |_ 고추\n          |_ 고추질병1 \n          |_ ...         \n\nyolo : 전체 이미지 모으고, 라벨데이터(라벨, 바운딩박스 정보) 따로 생성\ndatasets\n    |_ training \n        |_ images\n            |_ 사진이름.jpg\n            |_ ...\n\n        |_ labels\n            |_ 사진이름.txt\n            |_ ... \n\n\n    |_ validation \n        |_ images\n            |_ 사진이름.jpg\n            |_ ...\n\n        |_ labels\n            |_ 사진이름.txt\n            |_ ...         \n하지만 yolo는 위와 같이 다른 폴더구조를 사용합니다. images/label 폴더를 각각 타로 생성합니다. 이미지 데이터를 클래스 별로 모으는 것이 아니라, 전체 이미지 데이터를 한 폴더에 모으고, label 폴더에는 이미지 파일과 동일한 이름의 txt 파일을 생성하고, txt 파일 안에 label 정보를 담습니다.\n\n📕 reference\nyolo 학습 ~ 예측 참고 링크\n \n\n데이터셋 폴더 경로 설정\n\n# 경로 설정  \npath_train_img = \"datasets/training/images/\"\npath_train_label = \"datasets/training/labels/\"\n\npath_valid_img = \"datasets/validation/images/\"\npath_valid_label = \"datasets/validation/labels/\"\n\n위에서 설계한 대로 폴더를 생성하고, 그에 맞게 경로를 설정해줍니다.\n\n \n\n\n\n01. 현재 annotation 데이터의 bbox를 yolo bbox 형식으로 변환하기\nyolo bbox 형식 참고 링크\n\n\n레이블 정보 딕셔너리 불러오기\n\n# train\nwith open(\"data_preprocessing/dic_img2label_train_sampling.pickle\",\"rb\") as fr:\n    dic_img2label_train_sampling = pickle.load(fr)\n    \n# validation\nwith open(\"data_preprocessing/dic_img2label_valid_sampling.pickle\",\"rb\") as fr:\n    dic_img2label_valid_sampling = pickle.load(fr)\n\nyolo 학습을 위해 기존의 bbox를 yolo bbox 형식으로 변환해야 합니다. 이를 위해 EDA에서 생성해 놓은 레이블 정보가 담겨져 있는 딕셔너리를 불러옵니다.\n\n\n\nbbox 변환 함수\n\n# 현재 bbox: (x_min, y_min, x_max, y_max) 좌상단, 우하단 꼭짓점 좌표 \n# yolo bbox: (x_center, y_center, width, height)\ndef bbox_to_yolo_bbox(bbox, w, h): \n    # xmin, ymin, xmax, ymax\n    if w < 10: \n        w, h = 3024, 3024 \n        \n    x_center = ((bbox['xbr'] + bbox['xtl']) / 2) / w\n    y_center = ((bbox['ybr'] + bbox['ytl']) / 2) / h\n    width = (bbox['xbr'] - bbox['xtl']) / w\n    height = (bbox['ybr'] - bbox['ytl']) / h\n    return [x_center, y_center, width, height]\n\n\n\n\ntrain datasets\n\nfor img, values in tqdm(dic_img2label_train_sampling.items()):\n    filename = os.path.splitext(img)[0]\n    \n    yolo_bbox = bbox_to_yolo_bbox(values[\"points\"], values[\"size\"][0], values[\"size\"][1])\n    bbox_str = \" \".join([str(b) for b in yolo_bbox])\n    label = values[\"disease\"]\n    result = f\"{label} {bbox_str}\"\n       \n    if result: \n        with open(os.path.join(path_train_label, f\"{filename}.txt\"), \"w\", encoding='utf-8') as f: \n            f.write(result)\n\n100%|██████████| 13915/13915 [00:01<00:00, 12558.49it/s]\n\n\n\n\n\nvalidation datasets\n\nfor img, values in tqdm(dic_img2label_valid_sampling.items()):\n    filename = os.path.splitext(img)[0]\n    \n    yolo_bbox = bbox_to_yolo_bbox(values[\"points\"], values[\"size\"][0], values[\"size\"][1])\n    bbox_str = \" \".join([str(b) for b in yolo_bbox])\n    label = values[\"disease\"]\n    result = f\"{label} {bbox_str}\"\n       \n    if result: \n        with open(os.path.join(path_valid_label, f\"{filename}.txt\"), \"w\", encoding='utf-8') as f: \n            f.write(result)\n\n100%|██████████| 7873/7873 [00:00<00:00, 15471.80it/s]\n\n\ntrain, validation 데이터셋 모두 yolo bbox에 맞게 변환을 해주었고, datasets/training/labels, datasets/validation/labels 폴더에 이미지 별 txt 파일로 생성해주었습니다. 이 때 txt 파일의 내용은 label bbox정보 순서로 작성해줍니다.\n\n\n \n\n\n\n02. 데이터셋 경로가 적힌 txt 파일 생성하기\n\n# 경로에 들어있는 파일 리스트\nlst_train_img = os.listdir(path_train_img)\nlst_valid_img = os.listdir(path_valid_img)\n\n\nlst_train_data = [\"/home/elicer/\" + path_train_img + img for img in lst_train_img]\nlst_valid_data = [\"/home/elicer/\" + path_valid_img + img for img in lst_valid_img]\n\n\n# train.txt\nwith open(\"train.txt\", 'w') as f:\n    f.write('\\n'.join(lst_train_data) + '\\n')\n\n# valid.txt\nwith open(\"valid.txt\", 'w') as f:\n    f.write('\\n'.join(lst_valid_data) + '\\n')\n\n전체 이미지 파일 경로가 담긴 txt 파일도 생성합니다.\n\n\n \n\n\n03. yaml 파일 생성하기\n\nimport yaml\n\n\nyaml_data = {\n    \"names\": [\"Pepper\", \"Pepper anthrax\", \"Pepper white powder bottle\",\n\"Radish\", \"Radish Black-and-white disease\", \"Radish Bacteria-free disease\",\n\"Cabbage\", \"Cabbage black rot disease\", \"Cabbage roe disease\",\n\"Zucchini\", \"Zucchini nosocomial disease\", \"Zucchini white powder disease\",\n\"Bean\", \"Bean fire disease\", \"Bean dot disease\",\n\"Tomato\", \"Tomato leaf blight\",\n\"Pumpkin\", \"Pumpkin roe disease\", \"Pumpkin white powder disease\"],\n    \"nc\":20, \n    \"path\": \"/\",\n    \"train\": \"./datasets/train.txt\",\n    \"val\": \"./datasets/valid.txt\",\n}\n\nwith open(\"custom.yaml\", \"w\") as f: \n    yaml.dump(yaml_data, f)\n\nname에 클래스의 이름을 영문으로 적어 리스트로 넣어주고, nc에 클래스의 개수를 넣어줍니다. 그 다음 train, val에 앞서 생성한 txt 파일의 경로를 넣어주고 yaml파일로 저장합니다.\n\n\n \n\n\n04. train 진행하기\n\n학습을 위해 생성한 파일 정리\n1) images 폴더 안에 전체 이미지 파일\n2) labels 폴더 안에 이미지 파일에 대응되는 레이블 정보 txt 파일\n3) 전체 이미지 파일의 경로가 모두 적힌 txt 파일\n4) 클래스 이름, 개수, 3번에서 생성한 txt 파일 경로를 포함한 yaml 파일\n\n아래의 명령어를 터미널 에서 실행\n\ngit clone https://github.com/ultralytics/yolov5.git\ncd yolov5\npip install -qr requirements.txt\n\n!python train.py --batch 64 --epochs 20 --data ../custom.yaml --device 0 --weights yolov5s.pt --name test\n제가 수행한 학습 환경에서는 1에폭 당 1시간 30분 정도 걸렸습니다. 따라서 마찬가지로 tmux를 사용하여 학습을 진행하였습니다.\nyolo 학습을 수행하면 자동으로 모델 평가 결과를 몇가지 제공해줍니다.\n\n\n실제 vs 예측 Image 비교\n\nlabel\n\n\n\n\nimage.png\n\n\n\npred\n\n\n\n\nimage.png\n\n\n위의 도출된 결과 사진을 보면, pred의 bbox가 오히려 잎을 더 잘 예측한 것으로 보입니다.\n \n\n\nPrecision-Confidence Curve / Precision-Recall Curve / Recall-Confidence Curve 확인\n  \n \n\n\ntensorboard 평가지표 확인\n\n\n\nimage.png\n\n\n\n\n\nimage.png\n\n\n\n\n\nimage.png\n\n\n20 에폭 동안 이상적으로 평가지표는 올라가고 loss값은 줄어들지만, validation의 obj_loss값은 증가하는 추이를 보입니다. 위의 3가지의 curve 그래프나, 이 tensorboard로 미루어 보아, 20 에폭으로는 학습이 덜 된 것 같다는 생각이 듭니다. epoch을 100 이상으로 설정하고 돌리면 더 괜찮은 결과를 보일 수 있을 것이라 예상됩니다.\n\n이렇게 인공지능 기반의 웹서비스 개발 CropDoctor 프로젝트에서 모델 학습 부분을 마쳤습니다. 시간의 여유가 있었다면, 더 많은 모델 학습과 실험을 통해 결정하고 싶은 마음이 있었지만, image classification과 object detection 모델 학습을 비교 학습해봤다는 점에 의의를 두려고 합니다. 결론적으로 웹 서비스에서는 성능도 좋고, 아키텍처가 가벼운 Image Classification의 mobilenetV2 모델로 서빙하기로 결정하였습니다."
  },
  {
    "objectID": "posts/s00.demo.html",
    "href": "posts/s00.demo.html",
    "title": "[stockait] stockait 사용하여 주가 예측 모델 학습하기",
    "section": "",
    "text": "본 글에서는 직접 개발한 파이썬 라이브러리 stockait 사용법을 정리합니다. stockait는 주가 빅데이터 연구를 위한 통합 라이브러리로, 데이터 수집부터 데이터 전처리, 모델 학습 모델 평가, 수익률 계산까지 모든 과정을 이 라이브러리에서 사용할 수 있습니다.\n라이브러리의 전체적인 흐름 및 사용법을 다음의 과정으로 정리하도록 하겠습니다.\n데이터 수집 - 데이터 전처리 - 트레이더 정의 - 트레이더 사용(모델 학습 & 평가) - 수익률 시뮬레이션\n\nimport pandas as pd\nimport os\nimport sys\nimport stockait as sai"
  },
  {
    "objectID": "posts/s01.trader_definition.html#트레이더-정의하기",
    "href": "posts/s01.trader_definition.html#트레이더-정의하기",
    "title": "[stockait] stockait에서 trader의 개념과 사용법",
    "section": "트레이더 정의하기",
    "text": "트레이더 정의하기\nstockait는 모델학습 과정에서 트레이더라는 개념이 나옵니다. 게임 캐릭터가 장비를 장착하는 것 처럼 하나의 트레이더 안에 모델학습에 필요한 데이터셋을 저장하고, 모델의 정보, 주식 매매와 관련된 정보를 저장해놓습니다. 그 후에 캐릭터가 동작을 수행하는 것과 같이 트레이더를 사용하여 모델 학습을 진행하고, 주식 매매 시뮬레이션까지 수행해볼 수 있는 개념입니다.\n다음은 트레이더 기능에 대한 설명입니다.\n \n\n트레이더의 기능\n트레이더는 다음의 세가지 기능을 갖고있습니다.\n\n\n\nimage.png\n\n\n매수 매도 정보를 포함한 트레이더 객체를 정의하며, 데이터셋을 저장합니다. 추가적으로, 트레이더의 이름과 레이블 정보가 저장됩니다.\n \n\n\n첫째, 트레이더 객체 정의\n\n\n\nimage.png\n\n\n\ntrader.buyer: buyer 객체 안에 두개의 하위 매수 객체를 리스트 형태로 포함됩니다. ([conditional_buyer, machine learning_buyer])\ntrader.seller: seller객체 안에 하위 매도 객체를 포함합니다. (Subseller).\n\n\n\n1. conditional_buyer\n데이터셋 필터링 조건으로 매수 결정을 하는 객체입니다.\n\nConditional_buyer.condition: Add a dataset filtering condition to the condition method. (For example, only data with a transaction price (end price x transaction volume) of more than 1 billion won.) condition 메서드에 데이터셋 필터링 조건을 함수로 넣어줍니다. (예를 들어, 거래대금 (종가x거래량)이 1억원 이상인 데이터들)\n\n\n\n\n2. machinelearning_buyer\n머신러닝 모델로 매수 결정을 하는 객체입니다.\n\nmachinelearning_buyer.algorithm: algorithm 메서드에 유저가 정의한 모델을 넣어줍니다. sklearn 패키지나 pytorch, tensorflow 등으로 정의한 딥러닝 모델 등 과 같이 모든 머신러닝 모델을 추가할 수 있습니다.\n\n\n\n\n3. SubSeller\n모든 날짜에 대하여 매도를 결정하는 객체입니다.\n\n \n\n\n\n둘째, 데이터셋 저장\n\n\n\nimage.png\n\n\n트레이더 객체 안에 데이터셋을 저장합니다. train data, test data를 넣어주고, 만약 표준화를 진행했다면 train data scaled, test data scaled 까지 넣어줍니다. 그럼 학습에 필요한 데이터셋들을 생성하여 객체에 위와 같이 저장됩니다.\n \n\n\n셋째, 트레이더의 정보 저장\n\n\n\nimage.png\n\n\n\nTrader.name: 트레이더를 구별하는 이름입니다.\nTrader.label: 종속 변수의 타입을 저장합니다. (regression: reg, classification: class&0.02)\n\nstockait에서 지정한 default 종속변수는 다음 날 종가 변화율 (next_change)입니다.\n따라서 regresion으로 설정하면 종속변수는 next_change,\nclass&0.02 와 같이 설정하면 next_change, 즉 다음 날 종가 변화율이 0.02 이상이면 1, 그렇지 않으면 0 으로 이진분류 해줍니다. (다음날의 종가가 0.02 이상 올랐는지 아닌지)\n\n\n \n\n그 다음으로, 트레이더를 정의하는 예시를 보도록 하겠습니다.\n트레이더를 정의할 때 다음의 과정을 따라 진행하도록 합니다.\n\n처음으로, 트레이더를 담을 빈 리스트를 하나 선언합니다.\n\nlst_trader = [] \n\n다음은 이미 생성되어있는 머신러닝 패키지로 트레이더를 정의하는 예시입니다.\n\n\n\nex1) LightGBM\n\nfrom lightgbm import LGBMClassifier\n\n# conditional_buyer: Object that determines acquisition based on data filtering conditions \nb1_lg = sai.ConditionalBuyer()\n\ndef sampling1(df): # Create a conditional function\n    condition1 = (-0.3 <= df.D0_Change) & (df.D0_Change <= 0.3) # Remove exceptions that exceed upper and lower limits\n    condition2 = df.D0_trading_value >= 1000000000 # condition 1: Transaction amount of more than 1 billion won \n    condition3 = (-0.05 >= df.D0_Change) | (0.05 <= df.D0_Change) # condition 2: Today's stock price change rate is more than 5%\n    condition = condition1 & condition2 & condition3\n    return condition\n\nb1_lg.condition = sampling1  # Define the condition function directly (sampling1) and store it in the condition property \n\n\n# machinelearning_buyer: Object that determines acquisition by machine learning model\nb2_lg = sai.MachinelearningBuyer()\n\n# Save user-defined models to algorithm properties\nscale_pos_weight = round(72/28 , 2)\nparams = {  'random_state' : 42,\n            'scale_pos_weight' : scale_pos_weight,\n            'learning_rate' : 0.1, \n            'num_iterations' : 1000,\n            'max_depth' : 4,\n            'n_jobs' : 30,\n            'boost_from_average' : False,\n            'objective' : 'binary' }\n\nb2_lg.algorithm =  LGBMClassifier( **params )\n\n\n# SubSeller: Object that determines selling all of the following days\nsell_all = sai.SubSeller() \n\n\n# Trader Object   \nt1 = sai.Trader()\nt1.name = 'saiLightGBM' # Trader's name\nt1.label = 'class&0.02' # Set the Trader dependent variable (do not set if it is regression analysis) \nt1.buyer = sai.Buyer([b1_lg, b2_lg]) # [ conditional buyer, machinelearning buyer ] \nt1.seller = sai.Seller(sell_all)\n\nlst_trader.append(t1)\n\n\n\n\nex2) XGBoost\n\nfrom xgboost import XGBClassifier\n\nb1_xgb = sai.ConditionalBuyer() \n\ndef sampling2(df): \n    condition1 = (-0.3 <= df.D0_Change) & (df.D0_Change <= 0.3) \n    condition2 = df.D0_trading_value >= 1000000000 \n    condition3 = (-0.05 >= df.D0_Change) | (0.05 <= df.D0_Change) \n    condition = condition1 & condition2 & condition3\n    return condition\n\nb1_xgb.condition = sampling2\n\n\nb2_xgb = sai.MachinelearningBuyer()  \n\nscale_pos_weight = round(72/28 , 2)\nb2_xgb.algorithm = XGBClassifier(random_state = 42,\n                   n_jobs=30,\n                   scale_pos_weight=scale_pos_weight,\n                   learning_rate=0.1,\n                   max_depth=4,\n                   n_estimators=1000,\n                   )  \n\nsell_all = sai.SubSeller()\n\n\nt2 = sai.Trader()\nt2.name = 'saiXGboost' \nt2.label = 'class&0.02' \nt2.buyer = sai.Buyer([b1_xgb, b2_xgb])\nt2.seller = sai.Seller(sell_all) \n\nlst_trader.append(t2) \n\n\n\n\nex3) LogisticRegression\n\nfrom sklearn.linear_model import LogisticRegression\n\nb1_lr = sai.ConditionalBuyer()\n\ndef sampling3(df):  \n    condition1 = (-0.3 <= df.D0_Change) & (df.D0_Change <= 0.3) \n    condition2 = df.D0_trading_value >= 1000000000 \n    condition3 = (-0.05 >= df.D0_Change) | (0.05 <= df.D0_Change) \n    condition = condition1 & condition2 & condition3\n    return condition\n\nb1_lr.condition = sampling3\n\n\nb2_lr = sai.MachinelearningBuyer()  \n\nb2_lr.algorithm = LogisticRegression()\n\n\nsell_all = sai.SubSeller() \n\n\nt3 = sai.Trader()\nt3.name = 'saiLogisticRegression'  \nt3.label = 'class&0.02' \nt3.buyer = sai.Buyer([b1_lr, b2_lr]) \nt3.seller = sai.Seller(sell_all)\n\nlst_trader.append(t3) \n\n\n\n\nex4) Support Vector Machine\n\nfrom sklearn.svm import SVC\n\nb1_sv = sai.ConditionalBuyer()\n\ndef sampling4(df):  \n    condition1 = (-0.3 <= df.D0_Change) & (df.D0_Change <= 0.3) \n    condition2 = df.D0_trading_value >= 1000000000 \n    condition3 = (-0.05 >= df.D0_Change) | (0.05 <= df.D0_Change) \n    condition = condition1 & condition2 & condition3\n    return condition\n\nb1_sv.condition = sampling4 \n\n\nb2_sv = sai.MachinelearningBuyer()  \n\nb2_sv.algorithm = SVC() \n\n\nsell_all = sai.SubSeller() \n\n\nt4 = sai.Trader()\nt4.name = 'saiSupportVectorMachine'  \nt4.label = 'class&0.02' \nt4.buyer = sai.Buyer([b1_sv, b2_sv]) \nt4.seller = sai.Seller(sell_all)\n\nlst_trader.append(t4) \n\n\n\n\nex5) Decision Tree\n\nfrom sklearn.tree import DecisionTreeClassifier\n\nb1_dt = sai.ConditionalBuyer()\n\ndef sampling5(df):  \n    condition1 = (-0.3 <= df.D0_Change) & (df.D0_Change <= 0.3) \n    condition2 = df.D0_trading_value >= 1000000000 \n    condition3 = (-0.05 >= df.D0_Change) | (0.05 <= df.D0_Change) \n    condition = condition1 & condition2 & condition3\n    return condition\n\nb1_dt.condition = sampling5 \n\n\nb2_dt = sai.MachinelearningBuyer()  \n\nb2_dt.algorithm = DecisionTreeClassifier() \n\n\nsell_all = sai.SubSeller() \n\n\nt5 = sai.Trader()\nt5.name = 'saiDecisionTree'  \nt5.label = 'class&0.02' \nt5.buyer = sai.Buyer([b1_dt, b2_dt]) \nt5.seller = sai.Seller(sell_all)\n\nlst_trader.append(t5) \n\n\n\n\nex6) RandomForest\n\nfrom sklearn.ensemble import RandomForestClassifier\n\nb1_rf = sai.ConditionalBuyer()\n\ndef sampling6(df):  \n    condition1 = (-0.3 <= df.D0_Change) & (df.D0_Change <= 0.3) \n    condition2 = df.D0_trading_value >= 1000000000 \n    condition3 = (-0.05 >= df.D0_Change) | (0.05 <= df.D0_Change) \n    condition = condition1 & condition2 & condition3\n    return condition\n\nb1_rf.condition = sampling6 \n\n\nb2_rf = sai.MachinelearningBuyer()  \n\nb2_rf.algorithm = RandomForestClassifier() \n\n\nsell_all = sai.SubSeller() \n\n\nt6 = sai.Trader()\nt6.name = 'saiDecisionTree'  \nt6.label = 'class&0.02' \nt6.buyer = sai.Buyer([b1_rf, b2_rf]) \nt6.seller = sai.Seller(sell_all)\n\nlst_trader.append(t6) \n\n \n\n다음은 keras를 사용하여 LSTM 모델을 정의하는 딥러닝 모델의 예시입니다. 위의 머신러닝 모델과 다른 점은, data_transform이라는 속성을 하나 더 추가해서 정의해주어야 합니다.\n\n\nex7) LSTM\n⭐ 딥러닝 모델을 사용하기 위해서는 2차원 구조로 데이터셋 변환을 수행하는 transform 함수를 정의하고 data_transform 속성에 넣어줍니다. 아래 예시에서는 1x480 데이터셋을 10x48로 변환시켜주고, 모델 구조를 정의할 때 input_shape을 그에 맞게 (10, 48)로 지정해주었습니다. 사용하는 데이터셋의 구조를 제대로 파악하고, 변환 후의 input shape을 제대로 설정해주는 것이 중요합니다.\n\nfrom tensorflow import keras\n\nb1_ls = sai.ConditionalBuyer()\n\ndef sampling7(df): \n    condition1 = (-0.3 <= df.D0_Change) & (df.D0_Change <= 0.3) \n    condition2 = df.D0_trading_value >= 1000000000 \n    condition3 = (-0.05 >= df.D0_Change) | (0.05 <= df.D0_Change) \n    condition = condition1 & condition2 & condition3\n    return condition\n\nb1_ls.condition = sampling7 \n\n\nb2_ls = sai.MachinelearningBuyer()\n\n# ⭐ User-defined functions (users who want deep learning modeling)\ndef transform(data): # A function that converts into a two-dimensional structure / data: list (lst_time_series)\n    data_2d = []\n    n_col = int(len(data[0]) / 10) \n    for row in data:      \n        data_2d.append([])\n        for i in range(0, len(row), n_col):\n            data_2d[-1].append(row[i:i+n_col])\n    \n    return np.array(data_2d)\n    \n\n# Directly define a two-dimensional structure transformation function (transform) and store it in the data_transform property\nb2_ls.data_transform = transform \n\nmodel = keras.models.Sequential()\nmodel.add(keras.layers.InputLayer(input_shape=(10, 48)))\nmodel.add(keras.layers.LSTM(128, activation='relu', return_sequences=True))\nmodel.add(keras.layers.LSTM(64, activation='relu', return_sequences=True))\nmodel.add(keras.layers.Dropout(0.2))\nmodel.add(keras.layers.LSTM(64, activation='relu', return_sequences=True))\nmodel.add(keras.layers.LSTM(64, activation='relu', return_sequences=True))\nmodel.add(keras.layers.LSTM(64, activation='relu', return_sequences=True))\nmodel.add(keras.layers.LSTM(32, activation='relu', return_sequences=False))\nmodel.add(keras.layers.Dropout(0.2))\nmodel.add(keras.layers.Dense(1, activation='sigmoid'))\n    \nmodel.compile(optimizer=keras.optimizers.Adam(\n    learning_rate=keras.optimizers.schedules.ExponentialDecay(0.05,decay_steps=100000,decay_rate=0.96)), \n    loss=\"binary_crossentropy\",\n    metrics=['accuracy'])\n\nb2_ls.algorithm =  model\n\n\nsell_all = sai.SubSeller() \n\n\nt7 = sai.Trader()\nt7.name = 'saiLSTM' \nt7.label = 'class&0.02' \nt7.buyer = sai.Buyer([b1_ls, b2_ls]) \nt7.seller = sai.Seller(sell_all)\n\nlst_trader.append(t7)\n\n트레이더를 정의하고, 처음에 생성했던 lst_trader 리스트에 각각의 트레이더를 마지막 줄에서 append 해줍니다.\n트레이더를 정의하고, 모든 트레이더들이 모아진 lst_trader를 사용하여 정의한 여러개의 모델로 모델 학습 및 주식 매매를 실험해볼 수 있습니다. stockait 사용하여 여러 모델 학습해보기 글에 예시를 볼 수 있으며, 좋은 참고 자료가 될 것입니다."
  },
  {
    "objectID": "posts/s02.different_models.html",
    "href": "posts/s02.different_models.html",
    "title": "[stockait] stockait 사용하여 주가 예측 머신러닝 모델 실험하기",
    "section": "",
    "text": "import stockait as sai\nimport pandas as pd\n\n본 글은 stockait 라이브러리를 사용하여 같은 방식으로 전처리한 데이터셋으로 각각 다른 머신러닝 모델을 학습하여 어떤 트레이더의 수익률이 가장 높게 나오는지 실험해보는 예제입니다.\n\n \n\n1. 데이터 불러오기\n⭐ 본 글에서 사용되는 주가 데이터셋 전처리는 생략하고, [stockait] stockait 사용하여 주가 예측 모델 학습하기 글과 같은 방식으로 전처리하여 저장한 데이터를 불러와서 사용하겠습니다.\n\ndf_time_series = pd.read_parquet(\"../../../../cha0716/time_series_0305.parquet\")\ndf_time_series_scaled = pd.read_parquet(\"../../../../cha0716/time_series_scaled_0305.parquet\")\n\ndf_time_series = df_time_series[~(df_time_series[\"Code\"].isin([\"33626K\", \"33637k\", '33637K']))]\ndf_time_series_scaled = df_time_series_scaled[~(df_time_series_scaled[\"Code\"].isin([\"33626K\", \"33637k\", '33637K']))]\n\ndf_time_series['Code'] = df_time_series['Code'].astype(str).str.zfill(6)\ndf_time_series_scaled['Code'] = df_time_series_scaled['Code'].astype(str).str.zfill(6)\n\n불러온 데이터셋은 2016년 부터 2021년까지 div-close 방법으로 표준화된 시계열 데이터입니다.\n\ndata = df_time_series # Data Before Scaling\ndata_scaled = df_time_series_scaled # Data After Scaling\n\n# train, test dataset split\ntrain_data = data[(data['Date'] >= '2017-01-01') & (data['Date'] <= '2020-12-31')]\ntest_data = data[(data['Date'] >= '2021-01-01') & (data['Date'] <= '2021-12-31')]\n\n# train, test dataset split (scaled) \ntrain_data_scaled = data_scaled[(data_scaled['Date'] >= '2017-01-01') & (data_scaled['Date'] <= '2020-12-31')]\ntest_data_scaled = data_scaled[(data_scaled['Date'] >= '2021-01-01') & (data_scaled['Date'] <= '2021-12-31')]\n\nprint(train_data.shape, test_data.shape)\nprint(train_data_scaled.shape, test_data_scaled.shape)\n\n(828290, 483) (217159, 483)\n(828290, 483) (217159, 483)\n\n\n2017년 부터 2020년까지는 학습 데이터셋, 2021년은 시험 데이터셋으로 설정하였습니다.\n \n\n\n2. Trader 정의\n그 다음 트레이더를 정의합니다. 본 글에서는 글에서 정의했던 LightGBM, XGBoost, RandomForest, LSTM 모델을 그대로 사용하겠습니다.\n\nlst_trader = [] \n\n\n\n1) LightGBM\n\nfrom lightgbm import LGBMClassifier\n\n# conditional_buyer: Object that determines acquisition based on data filtering conditions \nb1_lg = sai.ConditionalBuyer()\n\ndef sampling1(df): # Create a conditional function\n    condition1 = (-0.3 <= df.D0_Change) & (df.D0_Change <= 0.3) # Remove exceptions that exceed upper and lower limits\n    condition2 = (df.D0_Close * df.D0_Volume) >= 1000000000 # condition 1: Transaction amount of more than 1 billion won \n    condition3 = (-0.05 >= df.D0_Change) | (0.05 <= df.D0_Change) # condition 2: Today's stock price change rate is more than 5%\n    condition = condition1 & condition2 & condition3\n    return condition\n\nb1_lg.condition = sampling1  # Define the condition function directly (sampling1) and store it in the condition property \n\n\n# machinelearning_buyer: Object that determines acquisition by machine learning model\nb2_lg = sai.MachinelearningBuyer()\n\n# Save user-defined models to algorithm properties\nscale_pos_weight = round(72/28 , 2)\nparams = {  'random_state' : 42,\n            'scale_pos_weight' : scale_pos_weight,\n            'learning_rate' : 0.1, \n            'num_iterations' : 1000,\n            'max_depth' : 4,\n            'n_jobs' : 30,\n            'boost_from_average' : False,\n            'objective' : 'binary' }\n\nb2_lg.algorithm =  LGBMClassifier( **params )\n\n\n# SubSeller: Object that determines selling all of the following days\nsell_all = sai.SubSeller() \n\n\n# Trader Object   \nt1 = sai.Trader()\nt1.name = 'saiLightGBM' # Trader's name\nt1.label = 'class&0.02' # Set the Trader dependent variable (do not set if it is regression analysis) \nt1.buyer = sai.Buyer([b1_lg, b2_lg]) # [ conditional buyer, machinelearning buyer ] \nt1.seller = sai.Seller(sell_all)\n\nlst_trader.append(t1)\n\n\n\n\n2) XGBoost\n\nfrom xgboost import XGBClassifier\n\nb1_xgb = sai.ConditionalBuyer() \n\ndef sampling2(df): \n    condition1 = (-0.3 <= df.D0_Change) & (df.D0_Change <= 0.3) \n    condition2 = (df.D0_Close * df.D0_Volume) >= 1000000000 \n    condition3 = (-0.05 >= df.D0_Change) | (0.05 <= df.D0_Change) \n    condition = condition1 & condition2 & condition3\n    return condition\n\nb1_xgb.condition = sampling2\n\n\nb2_xgb = sai.MachinelearningBuyer()  \n\nscale_pos_weight = round(72/28 , 2)\nb2_xgb.algorithm = XGBClassifier(random_state = 42,\n                   n_jobs=30,\n                   scale_pos_weight=scale_pos_weight,\n                   learning_rate=0.1,\n                   max_depth=4,\n                   n_estimators=1000,\n                   )  \n\nsell_all = sai.SubSeller()\n\n\nt2 = sai.Trader()\nt2.name = 'saiXGboost' \nt2.label = 'class&0.02' \nt2.buyer = sai.Buyer([b1_xgb, b2_xgb])\nt2.seller = sai.Seller(sell_all) \n\nlst_trader.append(t2) \n\n\n\n\n3) RandomForest\n\nfrom sklearn.ensemble import RandomForestClassifier\n\nb1_rf = sai.ConditionalBuyer()\n\ndef sampling3(df):  \n    condition1 = (-0.3 <= df.D0_Change) & (df.D0_Change <= 0.3) \n    condition2 = (df.D0_Close * df.D0_Volume) >= 1000000000 \n    condition3 = (-0.05 >= df.D0_Change) | (0.05 <= df.D0_Change) \n    condition = condition1 & condition2 & condition3\n    return condition\n\nb1_rf.condition = sampling3\n\n\nb2_rf = sai.MachinelearningBuyer()  \n\nb2_rf.algorithm = RandomForestClassifier() \n\n\nsell_all = sai.SubSeller() \n\n\nt3 = sai.Trader()\nt3.name = 'saiRandomForest'  \nt3.label = 'class&0.02' \nt3.buyer = sai.Buyer([b1_rf, b2_rf]) \nt3.seller = sai.Seller(sell_all)\n\nlst_trader.append(t3) \n\n\n\n\n4) LSTM\n\nfrom tensorflow import keras\nimport numpy as np\n\nb1_ls = sai.ConditionalBuyer()\n\ndef sampling4(df): \n    condition1 = (-0.3 <= df.D0_Change) & (df.D0_Change <= 0.3) \n    condition2 = (df.D0_Close * df.D0_Volume) >= 1000000000 \n    condition3 = (-0.05 >= df.D0_Change) | (0.05 <= df.D0_Change) \n    condition = condition1 & condition2 & condition3\n    return condition\n\nb1_ls.condition = sampling4\n\n\nb2_ls = sai.MachinelearningBuyer()\n\n# ⭐ User-defined functions (users who want deep learning modeling)\ndef transform(data): # A function that converts into a two-dimensional structure / data: list (lst_time_series)\n    data_2d = []\n    n_col = int(len(data[0]) / 10) \n    for row in data:      \n        data_2d.append([])\n        for i in range(0, len(row), n_col):\n            data_2d[-1].append(row[i:i+n_col])\n    \n    return np.array(data_2d)\n    \n\n# Directly define a two-dimensional structure transformation function (transform) and store it in the data_transform property\nb2_ls.data_transform = transform \n\nmodel = keras.models.Sequential()\nmodel.add(keras.layers.InputLayer(input_shape=(10, 48)))\nmodel.add(keras.layers.LSTM(128, activation='selu', return_sequences=True))\nmodel.add(keras.layers.Dropout(0.2))\nmodel.add(keras.layers.LSTM(64, activation='selu', return_sequences=True))\nmodel.add(keras.layers.Dropout(0.2))\nmodel.add(keras.layers.LSTM(32, activation='selu', return_sequences=False))\nmodel.add(keras.layers.Dropout(0.2))\nmodel.add(keras.layers.Dense(1, activation='sigmoid'))\n    \nmodel.compile(optimizer=keras.optimizers.Adam(\n    learning_rate=keras.optimizers.schedules.ExponentialDecay(0.05,decay_steps=100000,decay_rate=0.96)), \n    loss=\"binary_crossentropy\",\n    metrics=['accuracy'])\n\nb2_ls.algorithm =  model\n\n\nsell_all = sai.SubSeller() \n\n\nt4 = sai.Trader()\nt4.name = 'saiLSTM' \nt4.label = 'class&0.02' \nt4.buyer = sai.Buyer([b1_ls, b2_ls]) \nt4.seller = sai.Seller(sell_all)\n\nlst_trader.append(t4)\n\n2023-04-02 10:53:25.764037: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2023-04-02 10:53:25.885853: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n2023-04-02 10:53:26.354422: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.6/lib64::/usr/local/cuda-11.5/lib64:/usr/local/cuda-11.5/targets/x86_64-linux/lib\n2023-04-02 10:53:26.354479: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.6/lib64::/usr/local/cuda-11.5/lib64:/usr/local/cuda-11.5/targets/x86_64-linux/lib\n2023-04-02 10:53:26.354484: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n2023-04-02 10:53:27.287623: E tensorflow/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n2023-04-02 10:53:27.287685: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (ahnbi2): /proc/driver/nvidia/version does not exist\n2023-04-02 10:53:27.288577: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n\n\n네개의 모델을 정의하였고 각각의 모델을 모두 lst_trader 안에 넣었습니다.\n \n\n\n\n3. Trader(Model) 학슴 및 평가\n\n1) 트레이더에 데이터셋 저장하기\n트레이더 안에 데이터셋을 저장합니다.\n\nsai.save_dataset(lst_trader, train_data, test_data, train_data_scaled, test_data_scaled)\n\n== saiLightGBM ==\n== train_code_date: (828290, 2),  test_code_date: (217159, 2) ==\n== trainX: (828290, 480),  testX: (217159, 480) ==\n== trainX_scaled: (828290, 480),  testX_scaled: (217159, 480) ==\n== trainY: (828290,),  testY: (217159,) ==\n== trainY_classification: (828290,),  testY_classification: (217159,) ==\n\n== saiXGboost ==\n== train_code_date: (828290, 2),  test_code_date: (217159, 2) ==\n== trainX: (828290, 480),  testX: (217159, 480) ==\n== trainX_scaled: (828290, 480),  testX_scaled: (217159, 480) ==\n== trainY: (828290,),  testY: (217159,) ==\n== trainY_classification: (828290,),  testY_classification: (217159,) ==\n\n== saiRandomForest ==\n== train_code_date: (828290, 2),  test_code_date: (217159, 2) ==\n== trainX: (828290, 480),  testX: (217159, 480) ==\n== trainX_scaled: (828290, 480),  testX_scaled: (217159, 480) ==\n== trainY: (828290,),  testY: (217159,) ==\n== trainY_classification: (828290,),  testY_classification: (217159,) ==\n\n== saiLSTM ==\n== train_code_date: (828290, 2),  test_code_date: (217159, 2) ==\n== trainX: (828290, 480),  testX: (217159, 480) ==\n== trainX_scaled: (828290, 480),  testX_scaled: (217159, 480) ==\n== trainY: (828290,),  testY: (217159,) ==\n== trainY_classification: (828290,),  testY_classification: (217159,) ==\n\n\n\n\n\n\n2) 모델 학습\n각 트레이더에 정의되어있는 머신러닝 모델을 학습합니다.\n\nsai.trader_train(lst_trader) \n\n== saiLightGBM Model Fitting Completed ==\n== saiXGboost Model Fitting Completed ==\n== saiRandomForest Model Fitting Completed ==\n1268/1268 [==============================] - 18s 13ms/step - loss: 257227161600.0000 - accuracy: 0.6267\n== saiLSTM Model Fitting Completed ==\n\n\n\n\n\n3) 모델 평가 및 임계값 설정\n\n\n모델 평가\n네개의 모델에 대하여 threshold 별 평가지표를 시각화 하고, 매수를 위한 임계값 설정을 고려합니다.\n\nsai.get_eval_by_threshold(lst_trader)\n\n380/380 [==============================] - 2s 4ms/step\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n임계값 설정\n위에서 판단한 threshold를 순서대로 lst_threshold에 넣어주고, histogram을 그려 수익성 검증을 할 수 있습니다. 예시로 0.8, 0.8, 0.6, 0.8 로 설정해주었고, 변경해가며 수익성 검증 실험을 지속적으로 해볼 수 있습니다.\n\nsai.set_threshold(lst_trader, lst_threshold=[0.8, 0.8, 0.6, 0.8], histogram=True)\n\nError: local variable 'threshold' referenced before assignment\n380/380 [==============================] - 2s 4ms/step\n\n\n<Figure size 1152x720 with 0 Axes>\n\n\n<Figure size 1152x720 with 0 Axes>\n\n\n\n\n\n<Figure size 1152x720 with 0 Axes>\n\n\n<Figure size 1152x720 with 0 Axes>\n\n\n\n\n\n<Figure size 1152x720 with 0 Axes>\n\n\n<Figure size 1152x720 with 0 Axes>\n\n\n\n\n\n<Figure size 1152x720 with 0 Axes>\n\n\n<Figure size 1152x720 with 0 Axes>\n\n\n\n\n\n \n\n\n\n\n4. Back-Testing\n\n\n1) 매매일지 작성\n각각의 트레이더에서 모든 날짜에 대하여 매수 및 매도 기록을 생성합니다.\n\ndf_signal_all = sai.decision(lst_trader, dtype='test')\ndf_signal_all\n\n217159it [00:06, 36183.33it/s]\n217159it [00:05, 36418.57it/s]\n\n\n== saiLightGBM completed ==\n\n\n217159it [00:06, 35330.36it/s]\n217159it [00:06, 35767.62it/s]\n\n\n== saiXGboost completed ==\n\n\n217159it [00:06, 35462.79it/s]\n217159it [00:06, 35646.76it/s]\n\n\n== saiRandomForest completed ==\n6787/6787 [==============================] - 30s 4ms/step\n\n\n217159it [00:05, 37334.97it/s]\n217159it [00:05, 37169.70it/s]\n\n\n== saiLSTM completed ==\n\n\n\n\n\n\n  \n    \n      \n      Trader_id\n      Date\n      Code\n      +(buy)/-(sell)\n      Amount\n      Close\n    \n  \n  \n    \n      0\n      saiLightGBM\n      2021-01-04\n      000020\n      +\n      0.0\n      19100.0\n    \n    \n      1\n      saiLightGBM\n      2021-01-05\n      000020\n      +\n      0.0\n      19400.0\n    \n    \n      2\n      saiLightGBM\n      2021-01-06\n      000020\n      +\n      0.0\n      19700.0\n    \n    \n      3\n      saiLightGBM\n      2021-01-07\n      000020\n      +\n      0.0\n      19700.0\n    \n    \n      4\n      saiLightGBM\n      2021-01-08\n      000020\n      +\n      0.0\n      19100.0\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      217154\n      saiLSTM\n      2021-12-24\n      009900\n      -\n      1.0\n      30600.0\n    \n    \n      217155\n      saiLSTM\n      2021-12-27\n      009900\n      -\n      1.0\n      29900.0\n    \n    \n      217156\n      saiLSTM\n      2021-12-28\n      009900\n      -\n      1.0\n      29400.0\n    \n    \n      217157\n      saiLSTM\n      2021-12-29\n      009900\n      -\n      1.0\n      29850.0\n    \n    \n      217158\n      saiLSTM\n      2021-12-30\n      009900\n      -\n      1.0\n      30100.0\n    \n  \n\n1737272 rows × 6 columns\n\n\n\n\n\n\n2) 수익률 계산 시뮬레이션\n위의 매매일지를 기반으로 각각의 트레이더의 모든 날짜에 대한 수익률을 계산합니다.\n\ndf_history_all = sai.simulation(df_signal_all, init_budget=10000000, init_stock={}, fee=0.01)\ndf_history_all\n\n100%|███████████████████████████████████████████████████████████████████████████████████████████████▌| 247/248 [00:07<00:00, 33.88it/s]\n\n\n== saiLSTM completed ==\n\n\n100%|███████████████████████████████████████████████████████████████████████████████████████████████▌| 247/248 [00:07<00:00, 33.80it/s]\n\n\n== saiLightGBM completed ==\n\n\n100%|███████████████████████████████████████████████████████████████████████████████████████████████▌| 247/248 [00:07<00:00, 33.88it/s]\n\n\n== saiRandomForest completed ==\n\n\n100%|███████████████████████████████████████████████████████████████████████████████████████████████▌| 247/248 [00:07<00:00, 33.95it/s]\n\n\n== saiXGboost completed ==\n\n\n\n\n\n\n\n\n\n  \n    \n      \n      Trader_id\n      Sell_date\n      Budget\n      Yield\n      Stock\n    \n  \n  \n    \n      0\n      saiLSTM\n      2021-01-04\n      10000000\n      0.000000\n      {}\n    \n    \n      1\n      saiLSTM\n      2021-01-05\n      10042066\n      0.420662\n      {'000100': 3, '000660': 2, '001120': 10, '0013...\n    \n    \n      2\n      saiLSTM\n      2021-01-06\n      9793093\n      -2.069066\n      {'000120': 1, '001250': 154, '001430': 27, '00...\n    \n    \n      3\n      saiLSTM\n      2021-01-07\n      9830300\n      -1.696992\n      {'000220': 59, '001200': 107, '001250': 205, '...\n    \n    \n      4\n      saiLSTM\n      2021-01-08\n      9808160\n      -1.918394\n      {'000540': 105, '001200': 67, '001380': 131, '...\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      243\n      saiXGboost\n      2021-12-24\n      6110500\n      -38.894995\n      {'025620': 315}\n    \n    \n      244\n      saiXGboost\n      2021-12-27\n      6110500\n      -38.894995\n      {}\n    \n    \n      245\n      saiXGboost\n      2021-12-28\n      6110500\n      -38.894995\n      {}\n    \n    \n      246\n      saiXGboost\n      2021-12-29\n      6110500\n      -38.894995\n      {}\n    \n    \n      247\n      saiXGboost\n      2021-12-30\n      6110500\n      -38.894995\n      {}\n    \n  \n\n992 rows × 5 columns\n\n\n\n\n\n\n3) Leader Board\n트레이더의 최종 수익률 결과를 내림차순 정렬하여 데이터프레임으로 리더보드를 생성합니다.\n\nsai.leaderboard(df_history_all)\n\n\n\n\n\n  \n    \n      \n      Trader_id\n      Yield\n    \n  \n  \n    \n      0\n      saiLightGBM\n      44.423725\n    \n    \n      1\n      saiRandomForest\n      29.597223\n    \n    \n      2\n      saiXGboost\n      -38.894995\n    \n    \n      3\n      saiLSTM\n      -96.950032\n    \n  \n\n\n\n\n본 실험에서는 LightGBM의 수익률이 가장 높게 나왔으며, LSTM의 수익률이 가장 낮게 나왔습니다.\n\n\n\n4) 수익률 결과 시각화\n트레이더 별로 모든 날짜에 대한 수익률 추이를 확인할 수 있습니다.\n\nsai.yield_plot(df_history_all)"
  },
  {
    "objectID": "posts/s03.different_scaling_method.html",
    "href": "posts/s03.different_scaling_method.html",
    "title": "[stockait] stockait 사용하여 주가 데이터 표준화 실험하기",
    "section": "",
    "text": "import pandas as pd\nimport stockait as sai\n\n본 글은 같은 트레이더의 조건일 때 네개의 다른 표준화 방법으로 전처리한 데이터셋 중 어떤 데이터셋의 성능이 가장 높게 나오는지 비교 실험을 수행하는 글입니다.\n \n\n1. 데이터 수집\n실험은 한국의 코스피 시장에서 500개의 종목만을 사용해서 주행하도록 하겠습니다.\n\nlst_tickers = sai.get_tickers(markets=['KOSPI'])\nprint(len(lst_tickers), lst_tickers[:5])\n\n920 ['095570', '006840', '282330', '027410', '138930']\n\n\n\nraw_data = sai.load_data(date=['2016-01-01', '2021-12-31'], tickers=lst_tickers[:600])\nprint(raw_data.shape)\nraw_data.head()\n\n100%|█████████████████████████████████████████████████████████████████████████████████████| 600/600 [00:38<00:00, 15.43it/s]\n\n\n(821266, 7)\n\n\n\n\n\n\n  \n    \n      \n      Code\n      Date\n      Open\n      High\n      Low\n      Close\n      Volume\n    \n  \n  \n    \n      0\n      000020\n      2016-01-04\n      8130\n      8150\n      7920\n      8140\n      281440\n    \n    \n      1\n      000020\n      2016-01-05\n      8040\n      8250\n      8000\n      8190\n      243179\n    \n    \n      2\n      000020\n      2016-01-06\n      8200\n      8590\n      8110\n      8550\n      609906\n    \n    \n      3\n      000020\n      2016-01-07\n      8470\n      8690\n      8190\n      8380\n      704752\n    \n    \n      4\n      000020\n      2016-01-08\n      8210\n      8900\n      8130\n      8770\n      802330\n    \n  \n\n\n\n\n\n\n\n2. 데이터 전처리\n\n1) 보조지표 추가\n첫번째로, 보조지표를 추가합니다.\n\ncheck_index = ['MA5', 'MA20', 'MA60','MA120', \n             'next_change','CMF','VPT','VMAP', \"ADI\",\n             'BHB','BLB','KCH','KCL','KCM','DCH','DCL','DCM','UI',\n             'SMA','EMA','WMA','MACD','VIneg','VIpos','TRIX','MI','CCI','DPO','KST','Ichimoku','ParabolicSAR','STC',\n             'RSI','SRSI','TSI','UO','SR','WR','AO','ROC','PPO','PVO']\n\ncheck_df = sai.add_index(data=raw_data, index_list=check_index)\ncheck_df\n\n100%|████████████████████████████████████████████████████████████████████████████████████████████████| 595/595 [09:39<00:00,  1.03it/s]\n\n\n\n\n\n\n  \n    \n      \n      Code\n      Date\n      Open\n      High\n      Low\n      Close\n      Volume\n      Change\n      MA5\n      MA20\n      ...\n      RSI\n      SRSI\n      TSI\n      UO\n      SR\n      WR\n      AO\n      ROC\n      PPO\n      PVO\n    \n  \n  \n    \n      0\n      000020\n      2016-06-29\n      9850\n      10100\n      9700\n      9750\n      352292\n      -0.001025\n      9544.0\n      10210.00\n      ...\n      45.763505\n      0.529126\n      -8.116135\n      0.0\n      41.176471\n      -58.823529\n      -827.794118\n      -3.940887\n      -1.408719\n      -11.019576\n    \n    \n      1\n      000020\n      2016-06-30\n      9850\n      10400\n      9760\n      10100\n      466248\n      0.035897\n      9618.0\n      10195.00\n      ...\n      51.232344\n      0.875363\n      -6.553028\n      0.0\n      54.901961\n      -45.098039\n      -765.088235\n      -2.415459\n      -1.124410\n      -9.119594\n    \n    \n      2\n      000020\n      2016-07-01\n      10200\n      10200\n      9960\n      9960\n      208228\n      -0.013861\n      9794.0\n      10148.00\n      ...\n      49.099659\n      0.767924\n      -5.911240\n      0.0\n      49.411765\n      -50.588235\n      -624.205882\n      -4.230769\n      -1.001426\n      -12.558989\n    \n    \n      3\n      000020\n      2016-07-04\n      10000\n      10400\n      9900\n      10400\n      275210\n      0.044177\n      9994.0\n      10135.50\n      ...\n      55.385557\n      1.000000\n      -3.312304\n      0.0\n      66.666667\n      -33.333333\n      -427.205882\n      -0.952381\n      -0.541344\n      -13.975141\n    \n    \n      4\n      000020\n      2016-07-05\n      10400\n      10450\n      10200\n      10350\n      156010\n      -0.004808\n      10112.0\n      10118.00\n      ...\n      54.560981\n      0.961700\n      -1.483696\n      0.0\n      64.705882\n      -35.294118\n      -266.529412\n      1.970443\n      -0.216142\n      -17.711174\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      750398\n      5307W1\n      2021-12-23\n      3420\n      3480\n      3370\n      3425\n      391929\n      0.007353\n      3440.0\n      3535.00\n      ...\n      40.390231\n      0.159714\n      -8.758815\n      0.0\n      20.000000\n      -80.000000\n      -126.044118\n      -5.647383\n      -0.810232\n      -11.033540\n    \n    \n      750399\n      5307W1\n      2021-12-24\n      3465\n      3520\n      3420\n      3500\n      313474\n      0.021898\n      3438.0\n      3529.50\n      ...\n      47.968313\n      0.589811\n      -8.179822\n      0.0\n      43.076923\n      -56.923077\n      -138.058824\n      -3.047091\n      -0.740535\n      -10.762865\n    \n    \n      750400\n      5307W1\n      2021-12-27\n      3515\n      3515\n      3465\n      3475\n      265569\n      -0.007143\n      3446.0\n      3527.25\n      ...\n      45.874782\n      0.494226\n      -8.285385\n      0.0\n      35.384615\n      -64.615385\n      -134.838235\n      -4.005525\n      -0.734518\n      -11.598254\n    \n    \n      750401\n      5307W1\n      2021-12-28\n      3500\n      3520\n      3455\n      3515\n      359215\n      0.011511\n      3463.0\n      3532.75\n      ...\n      49.660421\n      0.719680\n      -7.274530\n      0.0\n      52.542373\n      -47.457627\n      -114.720588\n      -2.361111\n      -0.630225\n      -9.883604\n    \n    \n      750402\n      5307W1\n      2021-12-29\n      3520\n      3650\n      3490\n      3615\n      436463\n      0.02845\n      3506.0\n      3539.75\n      ...\n      57.637528\n      1.000000\n      -3.768305\n      0.0\n      86.440678\n      -13.559322\n      -86.279412\n      -0.138122\n      -0.313929\n      -6.618431\n    \n  \n\n750403 rows × 50 columns\n\n\n\n\n\n\n2) 데이터 표준화\n다음으로 stockait에서 제공하는 네가지의 표준화 방법을 사용하여 데이터를 표준화 합니다.\n\n\nminmax\n\nscaled_minmax = sai.scaling(data=check_df, scaler_type=\"minmax\")\nscaled_minmax.head()\n\n100%|████████████████████████████████████████████████████████████████████████████████████████████████| 586/586 [00:58<00:00, 10.09it/s]\n\n\n\n\n\n\n  \n    \n      \n      Code\n      Date\n      Open\n      High\n      Low\n      Close\n      Volume\n      Change\n      MA5\n      MA20\n      ...\n      RSI\n      SRSI\n      TSI\n      UO\n      SR\n      WR\n      AO\n      ROC\n      PPO\n      PVO\n    \n  \n  \n    \n      0\n      000020\n      2016-06-29\n      0.194499\n      0.168374\n      0.201232\n      9750\n      352292\n      -0.001025\n      0.187465\n      0.222265\n      ...\n      45.763505\n      0.529126\n      -8.116135\n      0.0\n      41.176471\n      -58.823529\n      -827.794118\n      -3.940887\n      -1.408719\n      -11.019576\n    \n    \n      1\n      000020\n      2016-06-30\n      0.194499\n      0.17862\n      0.203696\n      10100\n      466248\n      0.035897\n      0.19065\n      0.221465\n      ...\n      51.232344\n      0.875363\n      -6.553028\n      0.0\n      54.901961\n      -45.098039\n      -765.088235\n      -2.415459\n      -1.124410\n      -9.119594\n    \n    \n      2\n      000020\n      2016-07-01\n      0.208251\n      0.17179\n      0.21191\n      9960\n      208228\n      -0.013861\n      0.198227\n      0.218958\n      ...\n      49.099659\n      0.767924\n      -5.911240\n      0.0\n      49.411765\n      -50.588235\n      -624.205882\n      -4.230769\n      -1.001426\n      -12.558989\n    \n    \n      3\n      000020\n      2016-07-04\n      0.200393\n      0.17862\n      0.209446\n      10400\n      275210\n      0.044177\n      0.206836\n      0.218291\n      ...\n      55.385557\n      1.000000\n      -3.312304\n      0.0\n      66.666667\n      -33.333333\n      -427.205882\n      -0.952381\n      -0.541344\n      -13.975141\n    \n    \n      4\n      000020\n      2016-07-05\n      0.21611\n      0.180328\n      0.221766\n      10350\n      156010\n      -0.004808\n      0.211915\n      0.217358\n      ...\n      54.560981\n      0.961700\n      -1.483696\n      0.0\n      64.705882\n      -35.294118\n      -266.529412\n      1.970443\n      -0.216142\n      -17.711174\n    \n  \n\n5 rows × 50 columns\n\n\n\n\n\n\nstandard\n\nscaled_standard = sai.scaling(data=check_df, scaler_type=\"standard\")\nscaled_standard.head()\n\n100%|████████████████████████████████████████████████████████████████████████████████████████████████| 586/586 [00:26<00:00, 21.84it/s]\n\n\n\n\n\n\n  \n    \n      \n      Code\n      Date\n      Open\n      High\n      Low\n      Close\n      Volume\n      Change\n      MA5\n      MA20\n      ...\n      RSI\n      SRSI\n      TSI\n      UO\n      SR\n      WR\n      AO\n      ROC\n      PPO\n      PVO\n    \n  \n  \n    \n      0\n      000020\n      2016-06-29\n      -0.426667\n      -0.40741\n      -0.422378\n      9750\n      352292\n      -0.001025\n      -0.497811\n      -0.336067\n      ...\n      45.763505\n      0.529126\n      -8.116135\n      0.0\n      41.176471\n      -58.823529\n      -827.794118\n      -3.940887\n      -1.408719\n      -11.019576\n    \n    \n      1\n      000020\n      2016-06-30\n      -0.426667\n      -0.339931\n      -0.407372\n      10100\n      466248\n      0.035897\n      -0.479964\n      -0.339741\n      ...\n      51.232344\n      0.875363\n      -6.553028\n      0.0\n      54.901961\n      -45.098039\n      -765.088235\n      -2.415459\n      -1.124410\n      -9.119594\n    \n    \n      2\n      000020\n      2016-07-01\n      -0.343357\n      -0.384917\n      -0.357352\n      9960\n      208228\n      -0.013861\n      -0.437518\n      -0.351253\n      ...\n      49.099659\n      0.767924\n      -5.911240\n      0.0\n      49.411765\n      -50.588235\n      -624.205882\n      -4.230769\n      -1.001426\n      -12.558989\n    \n    \n      3\n      000020\n      2016-07-04\n      -0.390963\n      -0.339931\n      -0.372358\n      10400\n      275210\n      0.044177\n      -0.389284\n      -0.354314\n      ...\n      55.385557\n      1.000000\n      -3.312304\n      0.0\n      66.666667\n      -33.333333\n      -427.205882\n      -0.952381\n      -0.541344\n      -13.975141\n    \n    \n      4\n      000020\n      2016-07-05\n      -0.295751\n      -0.328684\n      -0.297327\n      10350\n      156010\n      -0.004808\n      -0.360826\n      -0.358600\n      ...\n      54.560981\n      0.961700\n      -1.483696\n      0.0\n      64.705882\n      -35.294118\n      -266.529412\n      1.970443\n      -0.216142\n      -17.711174\n    \n  \n\n5 rows × 50 columns\n\n\n\n\n\n\nrobust\n\nscaled_robust = sai.scaling(data=check_df, scaler_type=\"robust\")\nscaled_robust.head()\n\n100%|████████████████████████████████████████████████████████████████████████████████████████████████| 586/586 [00:44<00:00, 13.26it/s]\n\n\n\n\n\n\n  \n    \n      \n      Code\n      Date\n      Open\n      High\n      Low\n      Close\n      Volume\n      Change\n      MA5\n      MA20\n      ...\n      RSI\n      SRSI\n      TSI\n      UO\n      SR\n      WR\n      AO\n      ROC\n      PPO\n      PVO\n    \n  \n  \n    \n      0\n      000020\n      2016-06-29\n      0.016653\n      0.060905\n      -0.009898\n      9750\n      352292\n      -0.001025\n      -0.037510\n      0.080375\n      ...\n      45.763505\n      0.529126\n      -8.116135\n      0.0\n      41.176471\n      -58.823529\n      -827.794118\n      -3.940887\n      -1.408719\n      -11.019576\n    \n    \n      1\n      000020\n      2016-06-30\n      0.016653\n      0.114006\n      0.000723\n      10100\n      466248\n      0.035897\n      -0.024412\n      0.077720\n      ...\n      51.232344\n      0.875363\n      -6.553028\n      0.0\n      54.901961\n      -45.098039\n      -765.088235\n      -2.415459\n      -1.124410\n      -9.119594\n    \n    \n      2\n      000020\n      2016-07-01\n      0.078605\n      0.078605\n      0.036124\n      9960\n      208228\n      -0.013861\n      0.006741\n      0.069401\n      ...\n      49.099659\n      0.767924\n      -5.911240\n      0.0\n      49.411765\n      -50.588235\n      -624.205882\n      -4.230769\n      -1.001426\n      -12.558989\n    \n    \n      3\n      000020\n      2016-07-04\n      0.043204\n      0.114006\n      0.025504\n      10400\n      275210\n      0.044177\n      0.042142\n      0.067188\n      ...\n      55.385557\n      1.000000\n      -3.312304\n      0.0\n      66.666667\n      -33.333333\n      -427.205882\n      -0.952381\n      -0.541344\n      -13.975141\n    \n    \n      4\n      000020\n      2016-07-05\n      0.114006\n      0.122857\n      0.078605\n      10350\n      156010\n      -0.004808\n      0.063029\n      0.064091\n      ...\n      54.560981\n      0.961700\n      -1.483696\n      0.0\n      64.705882\n      -35.294118\n      -266.529412\n      1.970443\n      -0.216142\n      -17.711174\n    \n  \n\n5 rows × 50 columns\n\n\n\n\n\n\ndiv-close\n\nscaled_div_close = sai.scaling(data=check_df, scaler_type=\"div-close\")\nscaled_div_close.head()\n\n100%|████████████████████████████████████████████████████████████████████████████████████████████████| 586/586 [00:26<00:00, 21.94it/s]\n\n\n\n\n\n\n  \n    \n      \n      Code\n      Date\n      Open\n      High\n      Low\n      Close\n      Volume\n      Change\n      MA5\n      MA20\n      ...\n      RSI\n      SRSI\n      TSI\n      UO\n      SR\n      WR\n      AO\n      ROC\n      PPO\n      PVO\n    \n  \n  \n    \n      0\n      000020\n      2016-06-29\n      1.010256\n      1.035897\n      0.994872\n      9750\n      352292\n      -0.001025\n      0.978872\n      1.047179\n      ...\n      45.763505\n      0.529126\n      -8.116135\n      0.0\n      41.176471\n      -58.823529\n      -827.794118\n      -3.940887\n      -1.408719\n      -11.019576\n    \n    \n      1\n      000020\n      2016-06-30\n      1.010256\n      1.066667\n      1.001026\n      10100\n      466248\n      0.035897\n      0.986462\n      1.045641\n      ...\n      51.232344\n      0.875363\n      -6.553028\n      0.0\n      54.901961\n      -45.098039\n      -765.088235\n      -2.415459\n      -1.124410\n      -9.119594\n    \n    \n      2\n      000020\n      2016-07-01\n      1.009901\n      1.009901\n      0.986139\n      9960\n      208228\n      -0.013861\n      0.969703\n      1.004752\n      ...\n      49.099659\n      0.767924\n      -5.911240\n      0.0\n      49.411765\n      -50.588235\n      -624.205882\n      -4.230769\n      -1.001426\n      -12.558989\n    \n    \n      3\n      000020\n      2016-07-04\n      1.004016\n      1.044177\n      0.993976\n      10400\n      275210\n      0.044177\n      1.003414\n      1.017620\n      ...\n      55.385557\n      1.000000\n      -3.312304\n      0.0\n      66.666667\n      -33.333333\n      -427.205882\n      -0.952381\n      -0.541344\n      -13.975141\n    \n    \n      4\n      000020\n      2016-07-05\n      1.0\n      1.004808\n      0.980769\n      10350\n      156010\n      -0.004808\n      0.972308\n      0.972885\n      ...\n      54.560981\n      0.961700\n      -1.483696\n      0.0\n      64.705882\n      -35.294118\n      -266.529412\n      1.970443\n      -0.216142\n      -17.711174\n    \n  \n\n5 rows × 50 columns\n\n\n\n\n\n\n\n3) 시계열 데이터 변환\n하나의 행에 10일치(D-9 ~ D0)의 데이터가 담기도록 데이터셋을 변환합니다.\n\n# original dataset\ndf_time_series = sai.time_series(check_df, day=10)\n\n# scaled dataset \ndf_time_series_minmax = sai.time_series(scaled_minmax, day=10)\ndf_time_series_standard = sai.time_series(scaled_standard, day=10)\ndf_time_series_robust = sai.time_series(scaled_robust, day=10)\ndf_time_series_div_close = sai.time_series(scaled_div_close, day=10)\n\n100%|████████████████████████████████████████████████████████████████████████████████████████████████| 586/586 [02:19<00:00,  4.21it/s]\n100%|████████████████████████████████████████████████████████████████████████████████████████████████| 586/586 [02:25<00:00,  4.04it/s]\n100%|████████████████████████████████████████████████████████████████████████████████████████████████| 586/586 [02:18<00:00,  4.22it/s]\n100%|████████████████████████████████████████████████████████████████████████████████████████████████| 586/586 [02:18<00:00,  4.23it/s]\n100%|████████████████████████████████████████████████████████████████████████████████████████████████| 586/586 [02:18<00:00,  4.23it/s]\n\n\n \n\n\n\n3. 트레이더 정의\n\nlst_trader = []\n\n\nfrom lightgbm import LGBMClassifier\n\n# conditional_buyer: Object that determines acquisition based on data filtering conditions \nb1_lg = sai.ConditionalBuyer()\n\ndef sampling1(df): # Create a conditional function\n    condition1 = (-0.3 <= df.D0_Change) & (df.D0_Change <= 0.3) # Remove exceptions that exceed upper and lower limits\n    condition2 = (df.D0_Close * df.D0_Volume) >= 1000000000 # condition 1: Transaction amount of more than 1 billion won \n    condition3 = (-0.05 >= df.D0_Change) | (0.05 <= df.D0_Change) # condition 2: Today's stock price change rate is more than 5%\n    condition = condition1 & condition2 & condition3\n    return condition\n\nb1_lg.condition = sampling1  # Define the condition function directly (sampling1) and store it in the condition property \n\n\n# machinelearning_buyer: Object that determines acquisition by machine learning model\nb2_lg = sai.MachinelearningBuyer()\n\n# Save user-defined models to algorithm properties\nscale_pos_weight = round(72/28 , 2)\nparams = {  'random_state' : 42,\n            'scale_pos_weight' : scale_pos_weight,\n            'learning_rate' : 0.1, \n            'num_iterations' : 1000,\n            'max_depth' : 4,\n            'n_jobs' : 30,\n            'boost_from_average' : False,\n            'objective' : 'binary' }\n\nb2_lg.algorithm =  LGBMClassifier( **params )\n\n\n# SubSeller: Object that determines selling all of the following days\nsell_all = sai.SubSeller() \n\n트레이더는 같은 조건으로 생성되어야 하므로 정의를 한 번 해주고, 트레이더를 구분하기 위한 이름만 다르게 하여 lst_trader에 넣어줍니다.\n\ntrader_name = [\"trader_minmax\", \"trader_standard\", \"trader_robust\", \"trader_div-close\"]\nfor name in trader_name:    \n    trader = sai.Trader()\n    trader.name = name # Trader's name\n    trader.label = 'class&0.02' # Set the Trader dependent variable (do not set if it is regression analysis) \n    trader.buyer = sai.Buyer([b1_lg, b2_lg]) # [ conditional buyer, machinelearning buyer ] \n    trader.seller = sai.Seller(sell_all)\n    \n    lst_trader.append(trader)\n\n \n\n\n4. 트레이더 (모델) 학습 및 평가\n\n1) 트레이더 객체에 데이터셋 저장\n\ndf_time_series = df_time_series[~(df_time_series[\"Code\"].isin([\"33626K\", \"33637k\", '33637K']))]\ndf_time_series_minmax = df_time_series_minmax[~(df_time_series_minmax[\"Code\"].isin([\"33626K\", \"33637k\", '33637K']))]\ndf_time_series_standard = df_time_series_standard[~(df_time_series_standard[\"Code\"].isin([\"33626K\", \"33637k\", '33637K']))]\ndf_time_series_robust = df_time_series_robust[~(df_time_series_robust[\"Code\"].isin([\"33626K\", \"33637k\", '33637K']))]\ndf_time_series_div_close = df_time_series_div_close[~(df_time_series_div_close[\"Code\"].isin([\"33626K\", \"33637k\", '33637K']))]\n\n# train, test dataset split\ntrain_data = df_time_series[(df_time_series['Date'] >= '2017-01-01') & (df_time_series['Date'] <= '2020-12-31')]\ntest_data = df_time_series[(df_time_series['Date'] >= '2021-01-01') & (df_time_series['Date'] <= '2021-12-31')]\n\n# train, test dataset split (scaled) \ntrain_data_minmax = df_time_series_minmax[(df_time_series_minmax['Date'] >= '2017-01-01') & (df_time_series_minmax['Date'] <= '2020-12-31')]\ntest_data_minmax = df_time_series_minmax[(df_time_series_minmax['Date'] >= '2021-01-01') & (df_time_series_minmax['Date'] <= '2021-12-31')]\n\ntrain_data_standard = df_time_series_standard[(df_time_series_standard['Date'] >= '2017-01-01') & (df_time_series_standard['Date'] <= '2020-12-31')]\ntest_data_standard = df_time_series_standard[(df_time_series_standard['Date'] >= '2021-01-01') & (df_time_series_standard['Date'] <= '2021-12-31')]\n\ntrain_data_robust = df_time_series_robust[(df_time_series_robust['Date'] >= '2017-01-01') & (df_time_series_robust['Date'] <= '2020-12-31')]\ntest_data_robust = df_time_series_robust[(df_time_series_robust['Date'] >= '2021-01-01') & (df_time_series_robust['Date'] <= '2021-12-31')]\n\ntrain_data_div_close = df_time_series_div_close[(df_time_series_div_close['Date'] >= '2017-01-01') & (df_time_series_div_close['Date'] <= '2020-12-31')]\ntest_data_div_close = df_time_series_div_close[(df_time_series_div_close['Date'] >= '2021-01-01') & (df_time_series_div_close['Date'] <= '2021-12-31')]\n\n\n\nlst_dataset_scaled = [[train_data_minmax, test_data_minmax], \n                      [train_data_standard, test_data_standard],\n                     [train_data_robust, test_data_robust],\n                     [train_data_div_close, test_data_div_close]]\n\nfor i in range(4): \n    lst_trader_one = [lst_trader[i]]\n    sai.save_dataset(lst_trader_one, train_data, test_data, lst_dataset_scaled[i][0], lst_dataset_scaled[i][1])\n\n== trader_minmax ==\n== train_code_date: (542390, 2),  test_code_date: (139968, 2) ==\n== trainX: (542390, 470),  testX: (139968, 470) ==\n== trainX_scaled: (542390, 470),  testX_scaled: (139968, 470) ==\n== trainY: (542390,),  testY: (139968,) ==\n== trainY_classification: (542390,),  testY_classification: (139968,) ==\n\n== trader_standard ==\n== train_code_date: (542390, 2),  test_code_date: (139968, 2) ==\n== trainX: (542390, 470),  testX: (139968, 470) ==\n== trainX_scaled: (542390, 470),  testX_scaled: (139968, 470) ==\n== trainY: (542390,),  testY: (139968,) ==\n== trainY_classification: (542390,),  testY_classification: (139968,) ==\n\n== trader_robust ==\n== train_code_date: (542390, 2),  test_code_date: (139968, 2) ==\n== trainX: (542390, 470),  testX: (139968, 470) ==\n== trainX_scaled: (542390, 470),  testX_scaled: (139968, 470) ==\n== trainY: (542390,),  testY: (139968,) ==\n== trainY_classification: (542390,),  testY_classification: (139968,) ==\n\n== trader_div-close ==\n== train_code_date: (542390, 2),  test_code_date: (139968, 2) ==\n== trainX: (542390, 470),  testX: (139968, 470) ==\n== trainX_scaled: (542390, 470),  testX_scaled: (139968, 470) ==\n== trainY: (542390,),  testY: (139968,) ==\n== trainY_classification: (542390,),  testY_classification: (139968,) ==\n\n\n\n\n\n\n2) 모델 학습\n\nsai.trader_train(lst_trader) \n\n== trader_minmax Model Fitting Completed ==\n== trader_standard Model Fitting Completed ==\n== trader_robust Model Fitting Completed ==\n== trader_div-close Model Fitting Completed ==\n\n\n\n\n\n3) 모델 평가 및 임계값 설정\n\n모델 평가\n\nsai.get_eval_by_threshold(lst_trader)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n임계값 설정\n\nsai.set_threshold(lst_trader, lst_threshold=[0.8, 0.8, 0.8, 0.8], histogram=True)\n\nError: local variable 'threshold' referenced before assignment\n\n\n<Figure size 1152x720 with 0 Axes>\n\n\n<Figure size 1152x720 with 0 Axes>\n\n\n\n\n\n<Figure size 1152x720 with 0 Axes>\n\n\n<Figure size 1152x720 with 0 Axes>\n\n\n\n\n\n<Figure size 1152x720 with 0 Axes>\n\n\n<Figure size 1152x720 with 0 Axes>\n\n\n\n\n\n<Figure size 1152x720 with 0 Axes>\n\n\n<Figure size 1152x720 with 0 Axes>\n\n\n\n\n\n \n\n\n\n\n4. BackTesting\n\n\n1) 매매일지 작성\n\ndf_signal_all = sai.decision(lst_trader, dtype='test')\ndf_signal_all\n\n139968it [00:03, 36309.18it/s]\n139968it [00:03, 35560.93it/s]\n\n\n== trader_minmax completed ==\n\n\n139968it [00:03, 36491.79it/s]\n139968it [00:03, 36350.87it/s]\n\n\n== trader_standard completed ==\n\n\n139968it [00:03, 35696.38it/s]\n139968it [00:03, 36391.64it/s]\n\n\n== trader_robust completed ==\n\n\n139968it [00:03, 36208.76it/s]\n139968it [00:03, 35618.92it/s]\n\n\n== trader_div-close completed ==\n\n\n\n\n\n\n  \n    \n      \n      Trader_id\n      Date\n      Code\n      +(buy)/-(sell)\n      Amount\n      Close\n    \n  \n  \n    \n      0\n      trader_minmax\n      2021-01-04\n      000020\n      +\n      0.0\n      19100.0\n    \n    \n      1\n      trader_minmax\n      2021-01-05\n      000020\n      +\n      0.0\n      19400.0\n    \n    \n      2\n      trader_minmax\n      2021-01-06\n      000020\n      +\n      0.0\n      19700.0\n    \n    \n      3\n      trader_minmax\n      2021-01-07\n      000020\n      +\n      0.0\n      19700.0\n    \n    \n      4\n      trader_minmax\n      2021-01-08\n      000020\n      +\n      0.0\n      19100.0\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      139963\n      trader_div-close\n      2021-12-22\n      5307W1\n      -\n      1.0\n      3400.0\n    \n    \n      139964\n      trader_div-close\n      2021-12-23\n      5307W1\n      -\n      1.0\n      3425.0\n    \n    \n      139965\n      trader_div-close\n      2021-12-24\n      5307W1\n      -\n      1.0\n      3500.0\n    \n    \n      139966\n      trader_div-close\n      2021-12-27\n      5307W1\n      -\n      1.0\n      3475.0\n    \n    \n      139967\n      trader_div-close\n      2021-12-28\n      5307W1\n      -\n      1.0\n      3515.0\n    \n  \n\n1119744 rows × 6 columns\n\n\n\n\n\n\n2) Simulation: 수익률 계산\n\ndf_history_all = sai.simulation(df_signal_all, init_budget=10000000, init_stock={}, fee=0.01)\ndf_history_all\n\n100%|███████████████████████████████████████████████████████████████████████████████████████████████▌| 245/246 [00:10<00:00, 24.31it/s]\n\n\n== trader_div-close completed ==\n\n\n100%|███████████████████████████████████████████████████████████████████████████████████████████████▌| 245/246 [00:10<00:00, 24.29it/s]\n\n\n== trader_minmax completed ==\n\n\n100%|███████████████████████████████████████████████████████████████████████████████████████████████▌| 245/246 [00:10<00:00, 24.32it/s]\n\n\n== trader_robust completed ==\n\n\n100%|███████████████████████████████████████████████████████████████████████████████████████████████▌| 245/246 [00:10<00:00, 24.29it/s]\n\n\n== trader_standard completed ==\n\n\n\n\n\n\n\n\n\n  \n    \n      \n      Trader_id\n      Sell_date\n      Budget\n      Yield\n      Stock\n    \n  \n  \n    \n      0\n      trader_div-close\n      2021-01-04\n      10000000\n      0.000000\n      {}\n    \n    \n      1\n      trader_div-close\n      2021-01-05\n      10860611\n      8.606110\n      {'006340': 7142}\n    \n    \n      2\n      trader_div-close\n      2021-01-06\n      10860611\n      8.606110\n      {}\n    \n    \n      3\n      trader_div-close\n      2021-01-07\n      10860611\n      8.606110\n      {}\n    \n    \n      4\n      trader_div-close\n      2021-01-08\n      10860611\n      8.606110\n      {}\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      241\n      trader_standard\n      2021-12-22\n      8866099\n      -11.339006\n      {}\n    \n    \n      242\n      trader_standard\n      2021-12-23\n      8866099\n      -11.339006\n      {}\n    \n    \n      243\n      trader_standard\n      2021-12-24\n      8866099\n      -11.339006\n      {}\n    \n    \n      244\n      trader_standard\n      2021-12-27\n      8866099\n      -11.339006\n      {}\n    \n    \n      245\n      trader_standard\n      2021-12-28\n      8866099\n      -11.339006\n      {}\n    \n  \n\n984 rows × 5 columns\n\n\n\n\n\n\n3) 리더보드\n\nsai.leaderboard(df_history_all)\n\n\n\n\n\n  \n    \n      \n      Trader_id\n      Yield\n    \n  \n  \n    \n      0\n      trader_div-close\n      69.888981\n    \n    \n      1\n      trader_standard\n      -11.339006\n    \n    \n      2\n      trader_robust\n      -22.930525\n    \n    \n      3\n      trader_minmax\n      -26.364585\n    \n  \n\n\n\n\n\n\n\n4) 시각화 결과\n\nsai.yield_plot(df_history_all)\n\n\n\n\ndiv-close 표준화 방법이 다른 방법들보다 수익률이 상당히 높게 나온 것을 확인할 수 있습니다."
  },
  {
    "objectID": "posts/use_pil.html",
    "href": "posts/use_pil.html",
    "title": "PIL을 사용한 전통적인 이미지 처리 방법",
    "section": "",
    "text": "전통적인 이미지 처리 방법\n딥러닝을 사용한 이미지 학습 기법 전에, 여러 수많은 이미지 처리 기법들이 발전되어 왔습니다. 이들은 현재는 딥러닝 모델 학습을 위한 데이터 가공에 사용되고 있습니다.\n전통적인 이미지 처리 기법에는 대표적으로 다음과 같이 3가지가 존재합니다. - 형태변환\n\n색상변환\n필터 변환\n\n본 글에서는 위의 세가지 전통적인 이미지 처리 방법에 대해서 알아보고, 직접 실습해보도록 하겠습니다.\n \n\n\nPIL (Pillow) 란?\nPIL(Python Imaging Library)은 파이썬에서 openCV와 함께 자주 사용되는 이미지 처리 라이브러리입니다. 기존 PIL 라이브러리 개발은 중단되었지만, 오픈소스를 복제한 Pillow 라는 이름으로 현재까지 개발되고 있습니다.\n설치\npip install Pillow\n라이브러리 import\n\nfrom PIL import Image\n\n\n \n\n\n1. 이미지 확인하기\n\n이미지 불러오기\n\nimg = Image.open(\"pil_test.jfif\")\n\nimg 변수에는 PIL의 Image 클래스 객체가 저장됩니다. from PIL import Image에서 불러온 Image와, PIL의 Image 클래스는 다른 존재입니다. 지금부터 사용하는 메서드는 모두 PIL의 Image 클래스에 구현된 함수임에 유의하도록 합니다.\n\n\n\n이미지 시각화\n\nimport matplotlib.pyplot as plt \n\nplt.imshow(img)\n\n<matplotlib.image.AxesImage at 0x7f78cc261dc0>\n\n\n\n\n\n이미지 시각화에는 matplotlib의 imshow 메서드를 사용합니다. 이미지는 근육을 자랑하는 짱구로 준비해보았습니다.\n\n\n\n이미지 정보 확인\n\nprint(f\"Size: {img.size}, Mode: {img.mode}\")\n\nSize: (583, 587), Mode: RGB\n\n\n이미지의 사이즈는 Image 객체안의 size 속성에, 컬러는 mode 속성에 저장되어있습니다. 이 짱구 사진의 크기는 가로 583, 세로 587 픽셀로 이루어져 있고, 각 픽셀은 RGB 채널을 가지고 있음을 확인하였습니다.\n \n\n\n\n2. 형태 변환\n형태 변환은 말그대로 이미지의 형태를 변환하는 것입니다. 그 예시로, 잘라내기 (crop), 회전 (rotate), 크기 변경 (resize) 이 있습니다.\n\n2.1 이미지 잘라내기 (crop)\n잘라내기 (crop)은 전체 이미지 중 특정 부분만 잘라내는 작업입니다. crop 메서드를 통해 구현할 수 있으며, 이미지 좌표계에 따라, 4개 좌표를 튜플 형태로 입력하면 됩니다.\n따라서 이미지의 좌표계에 대하여 이해하고 있어야 합니다. 해당 이미지 좌표계는 원점 (0, 0)이 좌측 최상단에 존재합니다. 그렇기 때문에 x축은 원점을 기준으로 오른쪽으로, y축은 원점을 기준으로 아래쪽으로 값이 커진다고 생각하면 됩니다.\ncrop 메서드에 들어갈 튜플은 (x1, y1, x2, y2) 의 형태로 들어가게되는데, 크롭 할 네모박스 기준으로 좌측 상단 좌표 (x1, y1), 우측 하단 좌표 (x2, y2)를 의미합니다.\n\nplt.imshow(img.crop((10, 50, 460, 350)))\n\n<matplotlib.image.AxesImage at 0x7f78b4f110d0>\n\n\n\n\n\n짱구의 얼굴만 잘라낸 결과입니다.\n\n\n\n2.2 이미지 회전 (rotate)\n이미지 회전은 이미지를 시계방향이나 반시계 방향으로 일정 각도만큼 돌리는 것입니다. rotate 메서드에 회전하고자 하는 각도를 넣으면, 해당 각도만큼 반시계 방향으로 회전한 결과가 나오게됩니다.\n\nplt.imshow(img.rotate(30))\n\n<matplotlib.image.AxesImage at 0x7f78b4ebb130>\n\n\n\n\n\n\nplt.imshow(img.rotate(60, expand=True))\n\n<matplotlib.image.AxesImage at 0x7f78b52d7d00>\n\n\n\n\n\n이미지가 잘리는 것을 막기 위해 expand=True를 사용할 수 있습니다.\n\nplt.imshow(img.rotate(270))\n\n<matplotlib.image.AxesImage at 0x7f78b51aecd0>\n\n\n\n\n\n시계방향으로 돌리고 싶다면, 돌리고싶은 각도만큼 360에서 수를 빼주면 됩니다.\n\n\n\n2.3 이미지 크기 및 비율 변환\n이미지의 가로, 세로 길이를 변화시키는 작업입니다. resize 메서드에 바꾸고자 하는 가로, 세로 픽셀 길이를 튜플로 넣어주면 됩니다.\n\nimg_resized = img.resize((150, 300))\nplt.imshow(img_resized)\n\n<matplotlib.image.AxesImage at 0x7f78b49ac0a0>\n\n\n\n\n\n\n\n\n2.4 전단 변환 (Shearing)\n원래는 사각형의 형태였던 이미지를 평행사변형 꼴로 만드는 변환입니다. 여기에는 transform 메서드를 사용합니다.\nImage.transform(size, method, data=None, resample=Resampling.NEAREST, fill=1, fillcolor=None)\n\nsize: 출력될 이미지의 크기\nmethod: 변환의 종류를 지정 (전단 변환에는 아핀변환[Affine Transform]을 사용해야 하므로 Image.AFFINE을 지정합니다)\ndata: 적절한 값을 넣어주어야 함. (선형대수학이 필요함)\n\n공식문서\n\nplt.imshow(img.transform((int(img.size[0] * 1.5), img.size[1]), Image.AFFINE, (1, -0.5, 0, 0, 1, 0)))\n\n<matplotlib.image.AxesImage at 0x7f78b49d4fa0>\n\n\n\n\n\n\nplt.imshow(img.transform((int(img.size[0] * 1.2), img.size[1]), Image.AFFINE, (1, -0.2, 0, 0, 1, 0)))\n\n<matplotlib.image.AxesImage at 0x7f78b4b317f0>\n\n\n\n\n\n두 가지 방법의 차이점은 첫번째 파라미터 (size) 의 첫번째 값에 곱해주는 값과 세번째 파라미터 (data)의 첫번째 값 입니다. size에서 곱해주는 값은출력 크기를 맞춰주는 장치일 뿐이고, data파라미터의 두번째 값이 전단 변환의 정도, 즉 평행사변형의 기울기를 의미합니다.\n \n\n\n\n3. 색상 변환\n색상 변환에는 대표적으로 밝기(Brightness) 변화, 대조(Contrast)변화, 흑백 (Grayscale)변화 등이 있습니다. 여기에는 Pillow의 ImageEnhance라는 모듈을 사용합니다.\n\nfrom PIL import ImageEnhance \n\n\n\n3.1 밝기 변화\n\nbright_enhancer = ImageEnhance.Brightness(img)\n\n먼저, ImageEnhance모듈의 Brightness 클래스를 불러옵니다. 그러면 img 객체에 밝기 조절을 할 수 있는 Enhancer 객체가 만들어집니다. 이 객체에 enhance라는이름의 메서드를 사용합니다. 얼마나 밝기 조절을 할지, 배수를 넣어주면 됩니다.\n\nplt.imshow(bright_enhancer.enhance(2))\n\n<matplotlib.image.AxesImage at 0x7f78b503aac0>\n\n\n\n\n\n밝기를 2배 올려준 이미지가 나온 것을 확인할 수 있습니다.\n\nplt.imshow(bright_enhancer.enhance(0.5))\n\n<matplotlib.image.AxesImage at 0x7f78b4a92b50>\n\n\n\n\n\n이번엔 0.5배로 밝게, 즉, 두배 어둡게 만들어 준 예제입니다.\n\n\n\n3.2 대조 변화\n대조 변화는 ImageEnhance 모듈에서 Contrast 클래스를 사용하며, 사용법은 밝기 변화와 동일합니다.\n\ncontrast_enhancer = ImageEnhance.Contrast(img)\nplt.imshow(contrast_enhancer.enhance(2))\n\n<matplotlib.image.AxesImage at 0x7f78b4a8c9d0>\n\n\n\n\n\n대조를 2배 강하게 만들었습니다.\n\nplt.imshow(contrast_enhancer.enhance(0.5))\n\n<matplotlib.image.AxesImage at 0x7f78b48a5490>\n\n\n\n\n\n대조를 2배 약하게 만들었습니다.\n\n\n\n3.3 흑백 변화\n이미지 컬러 여부를 mode 속성에 RGB 임이 저장되어있었는데, 이 mode를 흑백으로 변경해주는 작업입니다.\n공식문서 에서 모드들을 확인할 수 있습니다.\n\nimg_gray = img.convert(\"L\")\nprint(\"흑백사진 모드: \", img_gray.mode)\nplt.imshow(img_gray, cmap=plt.get_cmap(\"gray\"))\n\n흑백사진 모드:  L\n\n\n<matplotlib.image.AxesImage at 0x7f78b4520fd0>\n\n\n\n\n\n \n\n\n\n4. 필터 변환\n필터는 포토샵이나 스마트폰에서 사진 보정을 위해 적용하는 필터와 비슷하다고 생각하면 됩니다. 대표적으로 샤프닝(Sharpening), 블러 (Blur), 경계선 탐지 (Edge Detection) 이 있습니다. filter 메서드에서 적용할 필터 종류를 파라미터로 넣어주어야 합니다. 이는 ImageFilter라는 별도 모듈에 구현되어 있습니다.\n공식문서 에서 필터의 종류를 확인할 수 있습니다.\n\n4.1 샤프닝 (Sharpening)\n이미지의 질갑을 날카롭게 만들어주는 작업입니다. 원본 사진이 흐릿할 경우 어느 정도의 화질 개선의 효과를 얻을 수 있지만, 과하게 적용하면 이미지의 자글자글한 노이즈가 부각될 수 있습니다.\nPillow의 ImageFilter 모듈의 SHARPEN 을 동태 샤프닝을 수행할 수 있습니다.\n\nfrom PIL import ImageFilter\n\nplt.imshow(img.filter(ImageFilter.SHARPEN))\n\n<matplotlib.image.AxesImage at 0x7f78b44836a0>\n\n\n\n\n\n샤프닝 전과 큰 차이가 없어보이므로 이럴 때는 여러 번 사용해볼 수도 있습니다.\n\nimg_sharpen = img.filter(ImageFilter.SHARPEN)\nimg_sharpen = img_sharpen.filter(ImageFilter.SHARPEN)\nimg_sharpen = img_sharpen.filter(ImageFilter.SHARPEN)\nplt.imshow(img_sharpen)\n\n<matplotlib.image.AxesImage at 0x7f78b4a3feb0>\n\n\n\n\n\n\n\n\n4.2 블러 (Blur)\n샤프닝과 반대의 개념으로, 이미지를 흐릿하게 만들어주는 작업입니다. 예시로, 배경에 블러 처리를 하여 사진에 찍힌 대상물을 부각시키는 작업에서 흔히 사용합니다. 스마트폰 카메라의 인물모드로, 배경에 자동으로 블러를 입혀주는 기능과 같습니다.\n블러는 ImageFilter의 BLUR를 사용하면 됩니다. 사용법은 샤프닝과 동일하게 진행하면 됩니다.\n\nimg_blur = img.filter(ImageFilter.BLUR)\nimg_blur = img_blur.filter(ImageFilter.BLUR)\nimg_blur = img_blur.filter(ImageFilter.BLUR)\nplt.imshow(img_blur)\n\n<matplotlib.image.AxesImage at 0x7f78b4afad60>\n\n\n\n\n\n\n\n\n5.3 경계선 감지\n경계선 감지는 이미지의 경계선을 찾아주는 작업입니다. 경계선은 이미지 내에서 색의 변화가 급격한 선 이라고 해석할 수 있습니다.\nImageFilter의 FIND_EDGES를 통해 경계선을 찾아내도록 합니다.\n\nplt.imshow(img.filter(ImageFilter.FIND_EDGES))\n\n<matplotlib.image.AxesImage at 0x7f78b43c3eb0>\n\n\n\n\n\n검정 선으로 그려진 캐릭터라 그런지 경계선이 더 잘 감지되는 것을 확인할 수 있습니다.\n \n본 글은 Elice 이미지 처리 - [이론] PIL 알아보기 강의를 참고하여 정리한 글입니다."
  },
  {
    "objectID": "posts/상권정보분석.html",
    "href": "posts/상권정보분석.html",
    "title": "상권정보 시각화 분석",
    "section": "",
    "text": "데이터 출처 : 공공데이터포털: 소상공인시장진흥공단_상가(상권)정보\n\n이번 데이터 시각화 분석에서는 공공데이터포털의 상권정보 데이터를 사용하여 서울시에 있는 상권 업종 분류 별로 살펴보며 서울시 상권업종 현황을 분석해보고, 그 중에서도 두 가지 분류(학원, 베이커리)를 집중적으로 분석해보도록 하겠습니다.\n* 전공 수업시간에 진행한 내용을 토대로 가설 설정과 해석, 시각화 디자인을 재구성한 글입니다.\n\n\n\nimport sys\nprint('python', sys.version)\n\nimport numpy as np\nprint('numpy', np.__version__)\n\nimport pandas as pd\nprint('pandas', pd.__version__)\n\nimport matplotlib as mpl\nprint('matplotlib', mpl.__version__)\n\nimport matplotlib.pyplot as plt\nplt.rc(\"font\", family=\"Malgun Gothic\", size=15) \nplt.rc(\"axes\", unicode_minus=False) # x,y축 (-)부호 표시\n\n# 레티나 디스플레이로 폰트가 선명하게 표시되도록 합니다.\nfrom IPython.display import set_matplotlib_formats\nset_matplotlib_formats(\"retina\")\n\nimport seaborn as sns\nprint('pandas', sns.__version__)\n\nfrom matplotlib.ticker import MaxNLocator\n\n# 결과 확인을 용이하게 하기 위한 코드\nfrom IPython.core.interactiveshell import InteractiveShell\nInteractiveShell.ast_node_interactivity = 'all'\n\n# dataframe 39 column 까지 표시\npd.options.display.max_columns=39\n\npython 3.7.3 (default, Mar 27 2019, 17:13:21) [MSC v.1915 64 bit (AMD64)]\nnumpy 1.21.6\npandas 1.3.5\nmatplotlib 3.3.2\npandas 0.12.1\n\n\n\n\n\n\ndf = pd.read_csv(\"11w/data/상가업소정보_201912_01_small.csv\", sep='|')\n\ndf.head(3)\n\n\n\n\n\n  \n    \n      \n      상가업소번호\n      상호명\n      지점명\n      상권업종대분류코드\n      상권업종대분류명\n      상권업종중분류코드\n      상권업종중분류명\n      상권업종소분류코드\n      상권업종소분류명\n      표준산업분류코드\n      표준산업분류명\n      시도코드\n      시도명\n      시군구코드\n      시군구명\n      행정동코드\n      행정동명\n      법정동코드\n      법정동명\n      지번코드\n      대지구분코드\n      대지구분명\n      지번본번지\n      지번부번지\n      지번주소\n      도로명코드\n      도로명\n      건물본번지\n      건물부번지\n      건물관리번호\n      건물명\n      도로명주소\n      구우편번호\n      신우편번호\n      동정보\n      층정보\n      호정보\n      경도\n      위도\n    \n  \n  \n    \n      0\n      21817342\n      기아자동차\n      중랑지점\n      D\n      소매\n      D23\n      자동차/자동차용품\n      D23A01\n      자동차판매\n      G45110\n      자동차 신품 판매업\n      11\n      서울특별시\n      11260\n      중랑구\n      1126060000\n      중화1동\n      1126010300\n      중화동\n      1126010300202860025\n      1\n      대지\n      286\n      25.0\n      서울특별시 중랑구 중화동 286-25\n      112603000001\n      서울특별시 중랑구 동일로\n      802\n      NaN\n      1126010300102860025013386\n      대신빌딩\n      서울특별시 중랑구 동일로 802\n      131120\n      2051.0\n      NaN\n      NaN\n      NaN\n      127.079691\n      37.602078\n    \n    \n      1\n      20614940\n      도미노피자\n      용산점\n      Q\n      음식\n      Q07\n      패스트푸드\n      Q07A04\n      패스트푸드\n      I56199\n      그외 기타 음식점업\n      11\n      서울특별시\n      11170\n      용산구\n      1117056000\n      원효로1동\n      1117011300\n      원효로2가\n      1117011300200030004\n      1\n      대지\n      3\n      4.0\n      서울특별시 용산구 원효로2가 3-4\n      111703102007\n      서울특별시 용산구 원효로\n      210\n      1.0\n      1117011300100030004018579\n      NaN\n      서울특별시 용산구 원효로 210-1\n      140847\n      4368.0\n      NaN\n      1\n      NaN\n      126.965501\n      37.537384\n    \n    \n      2\n      16108153\n      케이아이에프앤비\n      NaN\n      D\n      소매\n      D01\n      음/식료품소매\n      D01A01\n      식료품점\n      G47219\n      기타 식료품 소매업\n      26\n      부산광역시\n      26110\n      중구\n      2611056000\n      부평동\n      2611012300\n      부평동1가\n      2611012300200290079\n      1\n      대지\n      29\n      79.0\n      부산광역시 중구 부평동1가 29-79\n      261104175232\n      부산광역시 중구 중구로29번길\n      22\n      NaN\n      2611012300100290079003926\n      NaN\n      부산광역시 중구 중구로29번길 22\n      600804\n      48978.0\n      NaN\n      2\n      NaN\n      129.026690\n      35.100566\n    \n  \n\n\n\n\n \n\n\n\n\n\n\nprint(\"== 크기 확인 ==\")\ndf.shape\n\nprint()\nprint(\"== 컬럼 확인 ==\")\ndf.columns\n\nprint()\nprint(\"== 정보 확인 ==\")\ndf.info()\n\n== 크기 확인 ==\n\n\n(150000, 39)\n\n\n\n== 컬럼 확인 ==\n\n\nIndex(['상가업소번호', '상호명', '지점명', '상권업종대분류코드', '상권업종대분류명', '상권업종중분류코드',\n       '상권업종중분류명', '상권업종소분류코드', '상권업종소분류명', '표준산업분류코드', '표준산업분류명', '시도코드',\n       '시도명', '시군구코드', '시군구명', '행정동코드', '행정동명', '법정동코드', '법정동명', '지번코드',\n       '대지구분코드', '대지구분명', '지번본번지', '지번부번지', '지번주소', '도로명코드', '도로명', '건물본번지',\n       '건물부번지', '건물관리번호', '건물명', '도로명주소', '구우편번호', '신우편번호', '동정보', '층정보',\n       '호정보', '경도', '위도'],\n      dtype='object')\n\n\n\n== 정보 확인 ==\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 150000 entries, 0 to 149999\nData columns (total 39 columns):\n #   Column     Non-Null Count   Dtype  \n---  ------     --------------   -----  \n 0   상가업소번호     150000 non-null  int64  \n 1   상호명        150000 non-null  object \n 2   지점명        19939 non-null   object \n 3   상권업종대분류코드  150000 non-null  object \n 4   상권업종대분류명   150000 non-null  object \n 5   상권업종중분류코드  150000 non-null  object \n 6   상권업종중분류명   150000 non-null  object \n 7   상권업종소분류코드  150000 non-null  object \n 8   상권업종소분류명   150000 non-null  object \n 9   표준산업분류코드   141065 non-null  object \n 10  표준산업분류명    141065 non-null  object \n 11  시도코드       150000 non-null  int64  \n 12  시도명        150000 non-null  object \n 13  시군구코드      150000 non-null  int64  \n 14  시군구명       150000 non-null  object \n 15  행정동코드      150000 non-null  int64  \n 16  행정동명       150000 non-null  object \n 17  법정동코드      150000 non-null  int64  \n 18  법정동명       150000 non-null  object \n 19  지번코드       150000 non-null  int64  \n 20  대지구분코드     150000 non-null  int64  \n 21  대지구분명      150000 non-null  object \n 22  지번본번지      150000 non-null  int64  \n 23  지번부번지      124172 non-null  float64\n 24  지번주소       150000 non-null  object \n 25  도로명코드      150000 non-null  int64  \n 26  도로명        150000 non-null  object \n 27  건물본번지      150000 non-null  int64  \n 28  건물부번지      18999 non-null   float64\n 29  건물관리번호     150000 non-null  object \n 30  건물명        69352 non-null   object \n 31  도로명주소      150000 non-null  object \n 32  구우편번호      150000 non-null  int64  \n 33  신우편번호      149996 non-null  float64\n 34  동정보        13346 non-null   object \n 35  층정보        90788 non-null   object \n 36  호정보        22282 non-null   object \n 37  경도         150000 non-null  float64\n 38  위도         150000 non-null  float64\ndtypes: float64(5), int64(11), object(23)\nmemory usage: 44.6+ MB\n\n\n데이터를 확인 해보니 결측값 있는 컬럼을 확인할 수 있습니다.\n\n\n\n\n위에서 확인한 결측값을 더 자세히 보기 위해 missingo 라이브러리를 사용하여 시각화해보겠습니다.\n라이브러리 설치 conda install -c conda-forge missingno\n\n기본\n\n\nimport missingno as msno\n\n_=msno.matrix(df)\n\n\n\n\n\n\n\n\n\ndf.describe()\n\n\n\n\n\n  \n    \n      \n      상가업소번호\n      시도코드\n      시군구코드\n      행정동코드\n      법정동코드\n      지번코드\n      대지구분코드\n      지번본번지\n      지번부번지\n      도로명코드\n      건물본번지\n      건물부번지\n      구우편번호\n      신우편번호\n      경도\n      위도\n    \n  \n  \n    \n      count\n      1.500000e+05\n      150000.000000\n      150000.000000\n      1.500000e+05\n      1.500000e+05\n      1.500000e+05\n      150000.000000\n      150000.000000\n      124172.000000\n      1.500000e+05\n      150000.000000\n      18999.000000\n      150000.000000\n      149996.000000\n      150000.000000\n      150000.000000\n    \n    \n      mean\n      2.041990e+07\n      15.332800\n      15748.405900\n      1.574901e+09\n      1.574852e+09\n      1.574853e+18\n      1.001320\n      468.930780\n      33.179960\n      1.574876e+11\n      153.743900\n      7.015159\n      273384.199573\n      17495.024287\n      127.593626\n      36.856878\n    \n    \n      std\n      5.213909e+06\n      6.798467\n      6756.859055\n      6.756845e+08\n      6.756859e+08\n      6.756858e+17\n      0.036308\n      486.121473\n      105.183785\n      6.756853e+10\n      277.414013\n      9.877446\n      215380.462007\n      19396.705618\n      0.940031\n      1.080322\n    \n    \n      min\n      2.895874e+06\n      11.000000\n      11110.000000\n      1.111052e+09\n      1.111010e+09\n      1.111010e+18\n      1.000000\n      1.000000\n      1.000000\n      1.111020e+11\n      0.000000\n      1.000000\n      100011.000000\n      1000.000000\n      126.768169\n      35.010463\n    \n    \n      25%\n      1.606223e+07\n      11.000000\n      11320.000000\n      1.132069e+09\n      1.132011e+09\n      1.132011e+18\n      1.000000\n      108.000000\n      3.000000\n      1.132041e+11\n      20.000000\n      1.000000\n      134851.000000\n      4386.000000\n      126.966849\n      35.215700\n    \n    \n      50%\n      2.210410e+07\n      11.000000\n      11620.000000\n      1.162058e+09\n      1.162010e+09\n      1.162010e+18\n      1.000000\n      333.000000\n      10.000000\n      1.162030e+11\n      50.000000\n      3.000000\n      142874.000000\n      6522.000000\n      127.047053\n      37.511174\n    \n    \n      75%\n      2.477166e+07\n      26.000000\n      26200.000000\n      2.620065e+09\n      2.620012e+09\n      2.620012e+18\n      1.000000\n      679.000000\n      25.000000\n      2.620042e+11\n      171.000000\n      9.000000\n      604040.000000\n      46563.000000\n      128.986009\n      37.560275\n    \n    \n      max\n      2.852486e+07\n      26.000000\n      26710.000000\n      2.671033e+09\n      2.671033e+09\n      2.671033e+18\n      2.000000\n      9993.000000\n      3784.000000\n      2.671042e+11\n      3318.000000\n      198.000000\n      619963.000000\n      49527.000000\n      129.286869\n      37.688821\n    \n  \n\n\n\n\n \n\n\n\n\n\n\n\nprint(\"== 결측치 ==\")\nprint(df.isnull().sum())\n\nprint()\nprint(\"== 결측치가 많은 컬럼 ==\")\nnot_use_col = df.isnull().sum().sort_values(ascending=False).head(9).index \nprint(not_use_col)\n\n== 결측치 ==\n상가업소번호            0\n상호명               0\n지점명          130061\n상권업종대분류코드         0\n상권업종대분류명          0\n상권업종중분류코드         0\n상권업종중분류명          0\n상권업종소분류코드         0\n상권업종소분류명          0\n표준산업분류코드       8935\n표준산업분류명        8935\n시도코드              0\n시도명               0\n시군구코드             0\n시군구명              0\n행정동코드             0\n행정동명              0\n법정동코드             0\n법정동명              0\n지번코드              0\n대지구분코드            0\n대지구분명             0\n지번본번지             0\n지번부번지         25828\n지번주소              0\n도로명코드             0\n도로명               0\n건물본번지             0\n건물부번지        131001\n건물관리번호            0\n건물명           80648\n도로명주소             0\n구우편번호             0\n신우편번호             4\n동정보          136654\n층정보           59212\n호정보          127718\n경도                0\n위도                0\ndtype: int64\n\n== 결측치가 많은 컬럼 ==\nIndex(['동정보', '건물부번지', '지점명', '호정보', '건물명', '층정보', '지번부번지', '표준산업분류명',\n       '표준산업분류코드'],\n      dtype='object')\n\n\n\ndf.shape\ndf = df.drop(columns=not_use_col)\ndf.shape\n\n(150000, 39)\n\n\n(150000, 30)\n\n\n결측치가 너무 많은 컬럼은 분석에 사용할 수 없으므로 결측값 개수 상위 9개 컬럼을 추출하여 제거해주었습니다.\n\n\n\n\ndf[\"시도명\"].unique()\n\narray(['서울특별시', '부산광역시'], dtype=object)\n\n\n서울시의 데이터만 분석을 진행하기 위해 서울특별시만 따로 추출하여 저장하도록 합니다.\n\ndf = df[df[\"시도명\"] == \"서울특별시\"]\n\n# 확인 \ndf[\"시도명\"].unique()\n\narray(['서울특별시'], dtype=object)\n\n\n \n\n\n\n\n1 ) 서울시 상권업종분류 통계\n2 ) 서울시 음식점 분석\n3 ) 서울시 학원 분석\n4 ) 서울시 베이커리 입점분석\n\n\n\n상권업종대분류명 컬럼을 사용하여 서울시에 어떤 업종이 있고, 어떤 분류의 업종이 많이 존재하는지 통계를 내보도록 합니다.\n\n\n\nsr_order = df['상권업종대분류명'].value_counts()\n\n_=plt.figure(figsize=(10, 5))\n\n_=plt.title(\"서울시 상권 업종 현황\", fontsize=20, pad=10)\nbars=sns.countplot(data=df, x=\"상권업종대분류명\", palette=sns.color_palette(\"viridis_r\"), order=sr_order.index)\n\nfor i, b in enumerate(bars.patches):\n    _=plt.text(b.get_x()+b.get_width()*(1/2),b.get_height()+500, \\\n            round(b.get_height()),ha='center',fontsize=15, color='black')\n\n_=plt.ylim(0, 55000)\n_=plt.xticks(rotation=45)\n\n\n\n\n상권업종대분류명으로 barplot을 그려보았습니다. 서울시에는 음식 업종이 가장 많고, 그 다음으로는 비슷하게 소매의 수가 많습니다.\n\n\n\n\n\ndf[\"시군구명\"].value_counts().head(4)\n\n강남구    12368\n서초구     6575\n광진구     5757\n중구      5538\nName: 시군구명, dtype: int64\n\n\n서울시 자치구 중 상권업종이 많은 4가지 자치구(강남구, 중구, 서초구, 강서구)만 뽑아 각 지역별로 어떤 업종이 많이 차지하는지 시각화 해보도록 하겠습니다.\n\ndf_gn = df[df['시군구명']=='강남구']\ndf_j = df[df['시군구명']=='중구']\ndf_s = df[df['시군구명']=='서초구']\ndf_gs = df[df['시군구명']=='강서구']\n\nfig = plt.figure(figsize=(15, 10))\naxes = fig.subplots(2, 2).flatten() \n\n\nfor idx, df_r in enumerate([df_gn, df_j, df_s, df_gs]):\n    _=sns.countplot(data=df_r, x=\"상권업종대분류명\", \n                    palette=sns.color_palette(\"viridis_r\"), \n                    order=df_r[\"상권업종대분류명\"].value_counts().index, \n                    ax=axes[idx])\n\n    _=axes[idx].set_xticklabels(labels=df_r[\"상권업종대분류명\"].value_counts().index, rotation=45)\n\n    _=axes[idx].set_title(f\"{df_r['시군구명'].unique()[0]} 상권 업종 현황\", fontsize=20, pad=10)\n\nfig.tight_layout()\n\n\n\n\n대부분 비슷한 형태를 띄는데, 중구는 조금 다르게 나타났습니다. 음식보다 소매 업종이 월등히 높았고, 학문/교육 업종은 다른 지역에 비해 현저히 낮은 것으로 보여집니다.\n\n\n\n\n\n서울시의 교육관련 업종 데이터만 추출하여 어떤 지역에 학원이 많이 들어서있는지, 어느 곳에 학원가가 활발한지 분석합니다.\n\ndf_academy = df[df[\"상권업종대분류명\"] == \"학문/교육\"].copy()\n\n# 확인\ndf_academy[\"상권업종대분류명\"].unique()\n\narray(['학문/교육'], dtype=object)\n\n\n먼저 상권업종대분류명이 학문/교육인 데이터들만 추출하여 df_academy에 넣어주었습니다.\n\n\n\n상호명 top 10개 확인하기\n\n\ndf_academy[\"상호명\"].value_counts().head(10)\n\n점프셈교실      442\n해법수학        15\n해법영어교실       9\n무지개어린이집      8\n뮤엠영어         7\n윤선생영어교실      7\n새싹어린이집       6\n숲속어린이집       6\n행복한어린이집      6\n삼성영어         5\nName: 상호명, dtype: int64\n\n\n서울시의 상호명은 점프셈교실이라는 상호명이 압도적으로 많습니다.\n\n서울시 자치구별 학문/교육 업종 개수 확인하기\n\n\nsr_order = df_academy['시군구명'].value_counts()\n\n_=plt.figure(figsize=(15, 7))\n\n_=plt.title(\"서울시 자치구별 학문/교육 업종 현황\", fontsize=20, pad=10)\nbars=sns.countplot(data=df_academy, x=\"시군구명\", palette=sns.color_palette(\"twilight_shifted\"), order=sr_order.index)\n\nfor i, b in enumerate(bars.patches):\n    _=plt.text(b.get_x()+b.get_width()*(1/2),b.get_height()+10, \\\n            round(b.get_height()),ha='center',fontsize=15, color='black')\n\n_=plt.ylim(0, 1150)\n_=plt.xticks(rotation=45)\n\n\n\n\n학문/교육 업종은 강남구가 가장 많은 비율을 차지하고 있습니다. 그 다음으로는 서초구, 광진구, 송파구 등 비등비등한 개수로 점차 줄어듭니다.\n\n상권업종 중분류 별 개수 확인하기\n\n\nsr_order = df_academy['상권업종중분류명'].value_counts()\n\n_=plt.figure(figsize=(11, 7))\n\n_=plt.title(\"서울시 상권업종중분류 별 개수\", fontsize=20, pad=10)\nbars=sns.countplot(data=df_academy, y=\"상권업종중분류명\", palette=sns.color_palette(\"twilight_shifted\"), order=sr_order.index)\n\nfor i, b in enumerate(bars.patches):\n    _=plt.text(b.get_width()+70,b.get_y()+b.get_height()*(1/2), \\\n            round(b.get_width()),ha='center',fontsize=15, color='black')\n\n_=plt.xlim(0, 2150)\n\n\n\n\n중분류 업종으로 나눠봤을 때는, 보습교습입시가 가장 많은 것으로 나타났습니다.\n\n상권업종 소분류명 top 30개 개수 확인하기\n\n\nsr_order = df_academy['상권업종소분류명'].value_counts().head(30)\n\n_=plt.figure(figsize=(20, 7))\n\n_=plt.title(\"서울시 상권업종 소분류명 (top 30)\", fontsize=20, pad=10)\nbars=sns.countplot(data=df_academy, x=\"상권업종소분류명\", palette=sns.color_palette(\"twilight_shifted\"), order=sr_order.index)\n\nfor i, b in enumerate(bars.patches):\n    _=plt.text(b.get_x()+b.get_width()*(1/2),b.get_height()+10, \\\n            round(b.get_height()),ha='center',fontsize=15, color='black')\n\n_=plt.ylim(0, 2100)\n_=plt.xticks(rotation=60)\n\n\n\n\n소분류명별 시각화를 진행해보니, 학원의 입시와 종합이 가장 많았으며, 어린이집도 높은 비율을 차지하고있습니다.\n\n구별, 중분류별 상점개수 파악\n\n\nfig=plt.figure(figsize=(15,5),dpi=100)\n\n_=plt.title(\"구별, 중분류별 상점개수\", fontsize=20, pad=10)\n\n_=sns.countplot(data=df_academy, x='시군구명',hue='상권업종중분류명', palette=sns.color_palette(\"twilight_shifted\"))\n\n_=plt.xticks(rotation=60)\n_=plt.legend(bbox_to_anchor=(1.02, 1), loc=2)\n\n\n\n\n서울시의 자치구 별로 중분류별 상점개수 시각화를 해보았습니다. 특히 강남구에 학원-보습교습입시가 뚜렷하게 높이 솟아있는 것을 볼 수 있습니다. 또, 대체적으로 학원-보습교습입시의 상점 개수가 가장 많은 것으로 보입니다.\n\n자치구별 가장 많은 소분류명 상점과 그 개수 시각화\n\n\n# 자치구별 상권업종 소분류명의 개수 \nacademy_count_s=df_academy.groupby(['시군구명','상권업종소분류명'])['상호명'].count()\n\ndf2=academy_count_s.reset_index()\ndf2=df2.rename(columns={'상호명':'상호수'})\n\nmax_num_index=df2.groupby(['시군구명'])['상호수'].idxmax()\ndf3=df2.loc[max_num_index]\n\ndf3['ind']=df3['시군구명']+'('+df5['상권업종소분류명'] + ')'\ndf3.head()\ndf3=df3.set_index('ind')\ndf3=df3.drop(columns=['시군구명','상권업종소분류명'])\ndf3.head()\n\n\n\n\n\n  \n    \n      \n      시군구명\n      상권업종소분류명\n      상호수\n      ind\n    \n  \n  \n    \n      46\n      강남구\n      학원-입시\n      333\n      강남구(학원-입시)\n    \n    \n      81\n      강동구\n      학원-입시\n      82\n      강동구(학원-입시)\n    \n    \n      99\n      강북구\n      학원-입시\n      28\n      강북구(학원-입시)\n    \n    \n      126\n      강서구\n      학원-입시\n      106\n      강서구(학원-입시)\n    \n    \n      149\n      관악구\n      학원-입시\n      59\n      관악구(학원-입시)\n    \n  \n\n\n\n\n\n\n\n\n  \n    \n      \n      상호수\n    \n    \n      ind\n      \n    \n  \n  \n    \n      강남구(학원-입시)\n      333\n    \n    \n      강동구(학원-입시)\n      82\n    \n    \n      강북구(학원-입시)\n      28\n    \n    \n      강서구(학원-입시)\n      106\n    \n    \n      관악구(학원-입시)\n      59\n    \n  \n\n\n\n\n\n_=plt.figure(figsize=(12, 9))\n\n_=plt.title(\"서울시 상권업종중분류 별 개수\", fontsize=20, pad=10)\nbars=sns.barplot(data=df3.reset_index(), \n                   x=\"상호수\",\n                   y='ind',\n                   palette=sns.color_palette(\"flare\"))\n\nfor i, b in enumerate(bars.patches):\n    _=plt.text(b.get_width()+10,b.get_y()+b.get_height()*(1/2), \\\n            round(b.get_width()),ha='center',fontsize=15, color='gray')\n\n_=plt.xlim(0, 360)\n\n\n\n\n대체적으로 학원-입시 소분류가 가장 많습니다. 다른 학원 (종합, 외국어/어학)도 있고, 어린이집이 가장 많은 자치구도 존재합니다.\n\n\n\n\n\ng = df_academy.groupby([\"상권업종소분류명\", \"시군구명\"])[\"상호명\"].count()\n\nsr_order = g.loc[\"학원-입시\"].sort_values(ascending=False)\n\n_=plt.figure(figsize=(11, 7))\n\n_=plt.title(\"서울시 상권업종중분류 별 개수\", fontsize=20, pad=10)\nbars=sns.countplot(data=df_academy[df_academy[\"상권업종소분류명\"]==\"학원-입시\"], \n                   y=\"시군구명\", \n                   palette=sns.color_palette(\"flare\"), \n                   order=sr_order.index)\n\nfor i, b in enumerate(bars.patches):\n    _=plt.text(b.get_width()+10,b.get_y()+b.get_height()*(1/2), \\\n            round(b.get_width()),ha='center',fontsize=15, color='gray')\n\n_=plt.xlim(0, 370)\n\n\n\n\n학원-입시 상점 개수를 자치구별로 통계낸 시각화입니다. 강남구가 다른 지역의 2배 이상은 되는 양입니다.\n\n\n\n\n상권업종 소분류명별 상점 개수에서 1, 3위를 차지했던 학원-입시와 어린이집을 지도에 그려보겠습니다.\n\nscatterplot\n\n\n# 어린이집과 학원-입시를 비교해 봅니다.\n_=plt.figure(figsize=(10, 7))\n\n_=plt.title(\"학원-입시 vs 어린이집\", fontsize=20, pad=10)\n\n_=sns.scatterplot(data=df_academy.loc[df_academy[\"상권업종소분류명\"].isin([\"어린이집\", \"학원-입시\"])],\n                x=\"경도\", y=\"위도\", hue=\"상권업종소분류명\")\n\n\n\n\n먼저, 전체 서울시 지역에 대해서 학원/입시 와 어린이집을 scatter plot으로 그려보았습니다. 산점도에 위도와 경도를 그려봄으로써 대략적인 분포도를 확인할 수 있습니다.\n\nFolium\n\n\ndf_academy = df.loc[df[\"시군구명\"].isin([\"도봉구\",\"성동구\"]) & (df[\"상권업종대분류명\"] == \"학문/교육\")].copy()\ndf_m = df_academy.loc[df_academy[\"상권업종소분류명\"].isin([\"어린이집\", \"학원-입시\"])]\n\nlat = df_m[\"위도\"].mean()\nlong = df_m[\"경도\"].mean()\nm = folium.Map(location=[lat, long], zoom_start=12)\n\ndf_m_ex=df_m.loc[df_m['상권업종소분류명']=='학원-입시']\nfor i in df_m_ex.index:\n    t1 = df_m.loc[i, \"상호명\"] +\"(\"+ df_m.loc[i, \"도로명주소\"]+\")\"\n    lat = df_m.loc[i, \"위도\"]\n    long = df_m.loc[i, \"경도\"]\n    \n    _=folium.Marker([lat, long], tooltip=t1, radius=2, icon=folium.Icon(color=\"green\")).add_to(m)\n    \ndf_m_ch=df_m.loc[df_m['상권업종소분류명']=='어린이집']\nfor i in df_m_ch.index:\n    t1 = df_m.loc[i, \"상호명\"] +\"(\"+ df_m.loc[i, \"도로명주소\"]+\")\"\n    lat = df_m.loc[i, \"위도\"]\n    long = df_m.loc[i, \"경도\"]\n    \n    _=folium.Marker([lat, long], tooltip=t1, radius=2, icon=folium.Icon(color=\"red\")).add_to(m)\n\nm\n\nMake this Notebook Trusted to load map: File -> Trust Notebook\n\n\nfolium 라이브러리를 이용하여, 도봉구와 성동구에 대해서만 어린이집과 학원-입시 상점을 시각화 해보았습니다. 초록색이 학원-입시, 빨간색이 어린이집입니다. 두가지 상점의 분포도 파악이나, 실제 위치정보까지 알 수 있는 유용한 라이브러리입니다.\n\n여기까지 공공데이터포털의 소상공인시장진흥공단_상가(상권)정보 데이터를 사용하여 시각화해보았습니다. 시각화 연습용으로 데이터의 개수를 줄여서 진행하였는데, 전체 데이터를 가지고 여러 주제로 분석이 가능해 보입니다. 이 글에서는 교육 업종 현황을 파악해보면서, 어느 지역에 입시 학원이 많은지에 대한 정보를 얻을 수 있었습니다. 또 다른 특정 상권업종분류 업종을 사용하여, 입점분석이나 해당 업종의 현황 파악, 업종이 발달한 지역 등을 분석해 보는 것도 흥미로울 것 같습니다."
  },
  {
    "objectID": "posts/서울시_1인가구_시각화.html",
    "href": "posts/서울시_1인가구_시각화.html",
    "title": "서울시 1인가구 시각화 분석",
    "section": "",
    "text": "본 글은 데이터분석 기반 웹 서비스 개발 프로젝트의 데이터분석 내용입니다. 서울시 1인가구 데이터를 활용하여 인사이트를 도출해내어, 1인가구의 문제점을 파악하고, 해결방안을 제시하는 과정으로 웹서비스를 기획합니다."
  },
  {
    "objectID": "posts/서울시_1인가구_시각화.html#section",
    "href": "posts/서울시_1인가구_시각화.html#section",
    "title": "서울시 1인가구 시각화 분석",
    "section": "",
    "text": "* 커뮤니케이션이 적은 집단 측정 기준: 전화/문자 수 발신 대상자 수, 전화/문자 수 발신 건 수, SNS 사용량 기준\n* 외출이 매우 많은집단 측정 기준: 1인가구 대상 근로소득이 3천만원 초과이고, 휴일의 이동건수와 이동거리가 크고, 휴일의 추정거주지 체류시간이 적은 사람 기준 (휴일 이동경향이 높은 대상자를 구분하기 위함)"
  },
  {
    "objectID": "posts/전국_아파트_분양가_분석.html",
    "href": "posts/전국_아파트_분양가_분석.html",
    "title": "전국 아파트 분양가격 시각화 분석",
    "section": "",
    "text": "분양가란 처음 분양을 시작할 때의 가격을 말합니다. 이번 데이터 분석에서는 2013년부터 2019년도까지의 전국 신규 민간 아파트 분양가격 데이터를 사용하여 아파트 분양가를 연도, 지역 별로 분석해보면서 부동산 가격 변동 추세를 알아봅니다.\n* 인프런, 전공 수업시간에 진행한 내용을 토대로 가설 설정과 해석, 시각화 디자인을 재구성한 글입니다.\n\n\n\nimport sys\nprint('python', sys.version)\n\nimport numpy as np\nprint('numpy', np.__version__)\n\nimport pandas as pd\nprint('pandas', pd.__version__)\n\nimport matplotlib as mpl\nprint('matplotlib', mpl.__version__)\n\nimport matplotlib.pyplot as plt\n\nimport seaborn as sns\nprint('pandas', sns.__version__)\n\nimport matplotlib.pyplot as plt\nplt.rc(\"font\", family=\"Malgun Gothic\", size=15) \n\n# 결과 확인을 용이하게 하기 위한 코드\nfrom IPython.core.interactiveshell import InteractiveShell\nInteractiveShell.ast_node_interactivity = 'all'\n\n# 경고 메시지 무시.\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\npython 3.7.3 (default, Mar 27 2019, 17:13:21) [MSC v.1915 64 bit (AMD64)]\nnumpy 1.21.6\npandas 1.2.0\nmatplotlib 3.3.2\npandas 0.11.1\n\n\n \n\n\n\n데이터: [공공데이터 포털] 주택도시보증공사_전국 평균 분양가격\n\n# 1) 2015.10 ~ 2019.12 \ndf_last = pd.read_csv(\"3w/data/전국평균 분양가격 (2015년10월~2019년12월).csv\", encoding=\"cp949\") # 한글이 깨지지 않게 하려면 인코딩을 해줘야 한다.\n\n# 2) 2013.12 ~ 2015.8 \ndf_first = pd.read_csv(\"3w/data/전국평균 분양가격 (2013년12월~2015년8월).csv\", encoding=\"cp949\") # 한글이 깨지지 않게 하려면 인코딩을 해줘야 한다.\n\n\nprint(\"== shape ==\")\nprint(\"df_last\", df_last.shape)\nprint(\"df_first\", df_first.shape)\n\nprint()\nprint(\"== head ==\")\ndf_last.head() \ndf_first.head()\n\n== shape ==\ndf_last (4335, 5)\ndf_first (17, 22)\n\n== head ==\n\n\n\n\n\n\n  \n    \n      \n      지역명\n      규모구분\n      연도\n      월\n      분양가격(㎡)\n    \n  \n  \n    \n      0\n      서울\n      전체\n      2015\n      10\n      5841\n    \n    \n      1\n      서울\n      전용면적 60㎡이하\n      2015\n      10\n      5652\n    \n    \n      2\n      서울\n      전용면적 60㎡초과 85㎡이하\n      2015\n      10\n      5882\n    \n    \n      3\n      서울\n      전용면적 85㎡초과 102㎡이하\n      2015\n      10\n      5721\n    \n    \n      4\n      서울\n      전용면적 102㎡초과\n      2015\n      10\n      5879\n    \n  \n\n\n\n\n\n\n\n\n  \n    \n      \n      지역\n      2013년12월\n      2014년1월\n      2014년2월\n      2014년3월\n      2014년4월\n      2014년5월\n      2014년6월\n      2014년7월\n      2014년8월\n      ...\n      2014년11월\n      2014년12월\n      2015년1월\n      2015년2월\n      2015년3월\n      2015년4월\n      2015년5월\n      2015년6월\n      2015년7월\n      2015년8월\n    \n  \n  \n    \n      0\n      서울\n      18189\n      17925\n      17925\n      18016\n      18098\n      19446\n      18867\n      18742\n      19274\n      ...\n      20242\n      20269\n      20670\n      20670\n      19415\n      18842\n      18367\n      18374\n      18152\n      18443\n    \n    \n      1\n      부산\n      8111\n      8111\n      9078\n      8965\n      9402\n      9501\n      9453\n      9457\n      9411\n      ...\n      9208\n      9208\n      9204\n      9235\n      9279\n      9327\n      9345\n      9515\n      9559\n      9581\n    \n    \n      2\n      대구\n      8080\n      8080\n      8077\n      8101\n      8267\n      8274\n      8360\n      8360\n      8370\n      ...\n      8439\n      8253\n      8327\n      8416\n      8441\n      8446\n      8568\n      8542\n      8542\n      8795\n    \n    \n      3\n      인천\n      10204\n      10204\n      10408\n      10408\n      10000\n      9844\n      10058\n      9974\n      9973\n      ...\n      10020\n      10020\n      10017\n      9876\n      9876\n      9938\n      10551\n      10443\n      10443\n      10449\n    \n    \n      4\n      광주\n      6098\n      7326\n      7611\n      7346\n      7346\n      7523\n      7659\n      7612\n      7622\n      ...\n      7752\n      7748\n      7752\n      7756\n      7861\n      7914\n      7877\n      7881\n      8089\n      8231\n    \n  \n\n5 rows × 22 columns\n\n\n\n2013 ~ 2015년 데이터와 2015 ~ 2019년 데이터가 서로 다른 형태로 되어있는 것을 볼 수 있습니다. 먼저, 같은 형태로 전처리 한 후에 데이터셋을 병합하여 분석을 진행하도록 하겠습니다.\n \n\n\n\n\n\n규모구분 컬럼에 전용면적이라는 문구가 공통적으로 들어가고, 규모구분 보다 전용 면적이 더 직관적이므로 새로운 ‘전용면적’ 이라는 컬럼을 생성합니다. 또, 기존 규모구분의 데이터에서 전용면적, 초과, 이하 와 같은 문구를 빼고 간결하게 만듭니다.\n\ndf_last[\"전용면적\"] = df_last[\"규모구분\"].str.replace(\"전용면적\", \"\")\ndf_last[\"전용면적\"] = df_last[\"전용면적\"].str.replace(\"초과\", \"~\")\ndf_last[\"전용면적\"] = df_last[\"전용면적\"].str.replace(\"이하\", \"\")\ndf_last[\"전용면적\"] = df_last[\"전용면적\"].str.replace(\" \", \"\")\ndf_last.head()\n\n\n\n\n\n  \n    \n      \n      지역명\n      규모구분\n      연도\n      월\n      분양가격(㎡)\n      분양가격\n      평당분양가격\n      전용면적\n    \n  \n  \n    \n      0\n      서울\n      전체\n      2015\n      10\n      5841\n      5841.0\n      19275.3\n      전체\n    \n    \n      1\n      서울\n      전용면적 60㎡이하\n      2015\n      10\n      5652\n      5652.0\n      18651.6\n      60㎡\n    \n    \n      2\n      서울\n      전용면적 60㎡초과 85㎡이하\n      2015\n      10\n      5882\n      5882.0\n      19410.6\n      60㎡~85㎡\n    \n    \n      3\n      서울\n      전용면적 85㎡초과 102㎡이하\n      2015\n      10\n      5721\n      5721.0\n      18879.3\n      85㎡~102㎡\n    \n    \n      4\n      서울\n      전용면적 102㎡초과\n      2015\n      10\n      5879\n      5879.0\n      19400.7\n      102㎡~\n    \n  \n\n\n\n\n\n\n\n\n\n\ndf_last 데이터에서 분양가격이 object 데이터 타입으로 되어있습니다. 계산을 위해서는 float형으로 바꿔주어야 합니다.\n\ndef to_float(x): \n    if x!=x: # 결측값 검사\n        return np.nan \n    \n    x = x.replace(\",\", \"\")\n    if x.isdigit(): \n        return float(x) \n    \n    return np.nan \n\ndf_last[\"분양가격\"] = df_last['분양가격(㎡)'].map(to_float)\ndf_last[\"분양가격\"].dtypes\n\ndtype('float64')\n\n\n\n\n\n\n먼저, df_first 데이터는 평당 분양가격 기준으로 이루어져있는데, df_last 데이터는 ㎡당 분양가격으로 들어가있기 때문에 분양 가격을 평당 기준으로 보기 위해 3.3을 곱해서 df_last 데이터에 평당분양가격 컬럼을 생성하도록 합니다.\n\ndf_last['평당분양가격'] = df_last['분양가격'] * 3.3 \ndf_last.head()\n\n\n\n\n\n  \n    \n      \n      지역명\n      규모구분\n      연도\n      월\n      분양가격(㎡)\n      분양가격\n      평당분양가격\n      전용면적\n    \n  \n  \n    \n      0\n      서울\n      전체\n      2015\n      10\n      5841\n      5841.0\n      19275.3\n      전체\n    \n    \n      1\n      서울\n      전용면적 60㎡이하\n      2015\n      10\n      5652\n      5652.0\n      18651.6\n      60㎡\n    \n    \n      2\n      서울\n      전용면적 60㎡초과 85㎡이하\n      2015\n      10\n      5882\n      5882.0\n      19410.6\n      60㎡~85㎡\n    \n    \n      3\n      서울\n      전용면적 85㎡초과 102㎡이하\n      2015\n      10\n      5721\n      5721.0\n      18879.3\n      85㎡~102㎡\n    \n    \n      4\n      서울\n      전용면적 102㎡초과\n      2015\n      10\n      5879\n      5879.0\n      19400.7\n      102㎡~\n    \n  \n\n\n\n\n\n\n\n\ndf_last와 같은 형태가 되도록 melt를 사용하여 변경해줍니다. 컬럼의 이름도 같게 만들어주도록 합니다.\n\ndf_first_melt = df_first.melt(id_vars='지역', var_name=\"날짜\", value_name='평당분양가격').rename(columns={\"지역\": \"지역명\"})\ndf_first_melt.head()\n\n\n\n\n\n  \n    \n      \n      지역명\n      날짜\n      평당분양가격\n    \n  \n  \n    \n      0\n      서울\n      2013년12월\n      18189\n    \n    \n      1\n      부산\n      2013년12월\n      8111\n    \n    \n      2\n      대구\n      2013년12월\n      8080\n    \n    \n      3\n      인천\n      2013년12월\n      10204\n    \n    \n      4\n      광주\n      2013년12월\n      6098\n    \n  \n\n\n\n\n\n\n\n\n\ndef parse_year(x):\n    year = x.split(\"년\")[0]\n    return int(year) \n\ndef parse_month(x):\n    year = x.split(\"년\")[1].replace(\"월\", \"\")\n    return int(year) \n\ndf_first_melt['연도'] = df_first_melt['날짜'].map(parse_year)\ndf_first_melt['월'] = df_first_melt['날짜'].map(parse_month)\ndf_first_melt.head()\n\n\n\n\n\n  \n    \n      \n      지역명\n      날짜\n      평당분양가격\n      연도\n      월\n    \n  \n  \n    \n      0\n      서울\n      2013년12월\n      18189\n      2013\n      12\n    \n    \n      1\n      부산\n      2013년12월\n      8111\n      2013\n      12\n    \n    \n      2\n      대구\n      2013년12월\n      8080\n      2013\n      12\n    \n    \n      3\n      인천\n      2013년12월\n      10204\n      2013\n      12\n    \n    \n      4\n      광주\n      2013년12월\n      6098\n      2013\n      12\n    \n  \n\n\n\n\n\n\n\n\ndf_last와 df_fist_melt 에 공통적으로 있는 column 추출하기\n\ncols = df_last.columns.intersection(df_first_melt.columns) \ncols\n\nIndex(['지역명', '연도', '월', '평당분양가격'], dtype='object')\n\n\n공통적으로 있는 컬럼만 뽑아 df_first_prepare, df_last_prepare 생성\n\ndf_first_prepare = df_first_melt[cols]\ndf_first.head() \n\ndf_last_prepare = df_last.loc[df_last['전용면적']=='전체', cols].copy() \ndf_last_prepare.head() \n\n\n\n\n\n  \n    \n      \n      지역\n      2013년12월\n      2014년1월\n      2014년2월\n      2014년3월\n      2014년4월\n      2014년5월\n      2014년6월\n      2014년7월\n      2014년8월\n      ...\n      2014년11월\n      2014년12월\n      2015년1월\n      2015년2월\n      2015년3월\n      2015년4월\n      2015년5월\n      2015년6월\n      2015년7월\n      2015년8월\n    \n  \n  \n    \n      0\n      서울\n      18189\n      17925\n      17925\n      18016\n      18098\n      19446\n      18867\n      18742\n      19274\n      ...\n      20242\n      20269\n      20670\n      20670\n      19415\n      18842\n      18367\n      18374\n      18152\n      18443\n    \n    \n      1\n      부산\n      8111\n      8111\n      9078\n      8965\n      9402\n      9501\n      9453\n      9457\n      9411\n      ...\n      9208\n      9208\n      9204\n      9235\n      9279\n      9327\n      9345\n      9515\n      9559\n      9581\n    \n    \n      2\n      대구\n      8080\n      8080\n      8077\n      8101\n      8267\n      8274\n      8360\n      8360\n      8370\n      ...\n      8439\n      8253\n      8327\n      8416\n      8441\n      8446\n      8568\n      8542\n      8542\n      8795\n    \n    \n      3\n      인천\n      10204\n      10204\n      10408\n      10408\n      10000\n      9844\n      10058\n      9974\n      9973\n      ...\n      10020\n      10020\n      10017\n      9876\n      9876\n      9938\n      10551\n      10443\n      10443\n      10449\n    \n    \n      4\n      광주\n      6098\n      7326\n      7611\n      7346\n      7346\n      7523\n      7659\n      7612\n      7622\n      ...\n      7752\n      7748\n      7752\n      7756\n      7861\n      7914\n      7877\n      7881\n      8089\n      8231\n    \n  \n\n5 rows × 22 columns\n\n\n\n\n\n\n\n  \n    \n      \n      지역명\n      연도\n      월\n      평당분양가격\n    \n  \n  \n    \n      0\n      서울\n      2015\n      10\n      19275.3\n    \n    \n      5\n      인천\n      2015\n      10\n      10437.9\n    \n    \n      10\n      경기\n      2015\n      10\n      10355.4\n    \n    \n      15\n      부산\n      2015\n      10\n      10269.6\n    \n    \n      20\n      대구\n      2015\n      10\n      8850.6\n    \n  \n\n\n\n\ndf_first_prepare + df_last_prepare 합치기\n\ndf = pd.concat([df_first_prepare, df_last_prepare])\ndf.head()\n\n\n\n\n\n  \n    \n      \n      지역명\n      연도\n      월\n      평당분양가격\n    \n  \n  \n    \n      0\n      서울\n      2013\n      12\n      18189.0\n    \n    \n      1\n      부산\n      2013\n      12\n      8111.0\n    \n    \n      2\n      대구\n      2013\n      12\n      8080.0\n    \n    \n      3\n      인천\n      2013\n      12\n      10204.0\n    \n    \n      4\n      광주\n      2013\n      12\n      6098.0\n    \n  \n\n\n\n\n2013년도부터 2019년도까지의 데이터를 같은 형태로 모두 합쳤습니다. 이제 본격적으로 전국의 아파트 분양가 시각화 분석을 진행해보도록 하겠습니다.\n \n\n\n\n\n\n\nprint(\"df.shape: \", df.shape)\nprint()\nprint(\"df.info(): \", df.info())\nprint()\nprint(\"== 결측값 ==\")\nprint(df.isnull().sum())\n\ndf.shape:  (1224, 4)\n\n<class 'pandas.core.frame.DataFrame'>\nInt64Index: 1224 entries, 0 to 4330\nData columns (total 4 columns):\n #   Column  Non-Null Count  Dtype  \n---  ------  --------------  -----  \n 0   지역명     1224 non-null   object \n 1   연도      1224 non-null   int64  \n 2   월       1224 non-null   int64  \n 3   평당분양가격  1215 non-null   float64\ndtypes: float64(1), int64(2), object(1)\nmemory usage: 47.8+ KB\ndf.info():  None\n\n== 결측값 ==\n지역명       0\n연도        0\n월         0\n평당분양가격    9\ndtype: int64\n\n\n평당분양가격에 결측값이 9개 있는 것을 확인하였습니다. 시각화 정보전달의 목적이므로 결측값 처리는 진행하지 않고, 어떤 데이터에 결측값이 있는지 알아보겠습니다.\n\ndf[df['평당분양가격'].isnull()]\n\n\n\n\n\n  \n    \n      \n      지역명\n      연도\n      월\n      평당분양가격\n    \n  \n  \n    \n      3265\n      울산\n      2018\n      12\n      NaN\n    \n    \n      3350\n      울산\n      2019\n      1\n      NaN\n    \n    \n      3435\n      울산\n      2019\n      2\n      NaN\n    \n    \n      3520\n      울산\n      2019\n      3\n      NaN\n    \n    \n      3605\n      울산\n      2019\n      4\n      NaN\n    \n    \n      3690\n      울산\n      2019\n      5\n      NaN\n    \n    \n      3775\n      울산\n      2019\n      6\n      NaN\n    \n    \n      3860\n      울산\n      2019\n      7\n      NaN\n    \n    \n      3945\n      울산\n      2019\n      8\n      NaN\n    \n  \n\n\n\n\n울산 지역에서 2018년도부터 데이터가 존재하지 않는 것으로 보입니다. 이 데이터셋(2013~2019)에서 울산 지역의 최신 데이터는 없는 것을 감안하고 분석을 진행하도록 합니다.\n \n\n\n\n1 ) 연도별 분양가격 동향 - 지속적으로 분양가격이 증가 할 것이다.\n2 ) 지역별 분양가격 - 수도권 근방의 분양가격이 높을 것이다.\n3 ) 지역별 연도별 분양가격 - 특정 지역의 연도별 분양가격이 증가할 것이다.\n먼저 연도별로 분양가격을 분석해보고 분양가격의 추이를 살펴봅니다. 그 다음 지역 별로 분양가격을 분석하여 각 지역의 평균 분양가격을 파악하고, 어느 지역의 분양가격이 높고 낮은지 확인합니다. 마지막으로 지역별 연도별로 시각화를 진행하고, 특정 지역의 어느 연도에서 증가 추세를 띄는지, 동향이 어떻게 되는지 분석합니다.\n \n\n\n\n\n\n\n\n연도 별 평당분양가격의 평균을 그려봅니다.\n\nbarplot + pointplot\n\n\n_=plt.figure(figsize=(8, 5))\n\n_=plt.title(\"연도별 평당 분양가격 평균 (단위: 천원)\", fontsize=20, pad=10)\nbars=sns.barplot(data=df, x='연도', y='평당분양가격', ci=None, palette=sns.color_palette(\"Pastel1\"))\n_=sns.pointplot(data=df, x=\"연도\", y=\"평당분양가격\", ci=None, color='#bfaeae')\n\nfor i, b in enumerate(bars.patches):\n    _=plt.text(b.get_x()+b.get_width()*(1/2),b.get_height()+500, \\\n            round(b.get_height()),ha='center',fontsize=15, color='grey')\n\n_=plt.ylim(0, 13500)\n\n\n\n\n2013년도부터 2019년도까지 평당 분양가격의 평균을 시각화해보았더니 지속적으로 증가하는 추세를 보였습니다.\n\nboxplot\n\n\n_=plt.figure(figsize=(8, 5))\n\n_=plt.title(\"연도별 평당 분양가격 (단위: 천원)\", fontsize=20, pad=10)\n_=sns.boxplot(data=df, x='연도', y='평당분양가격', palette=sns.color_palette(\"Pastel1\"))\n\n\n\n\nboxplot으로는 전체 값의 분포와 이상치를 확인할 수 있습니다. 연도별로 IQR 방식의 이상치로 판단되는 값이 조금씩 있는 것으로 보여집니다. 그리고 중앙값이나 사분위수 등 모두 연도별로 증가하는 것을 볼 수 있습니다.\n\nviolinplot\n\n\n_=plt.figure(figsize=(8, 5))\n\n_=plt.title(\"연도별 평당 분양가격 (단위: 천원)\", fontsize=20, pad=10)\n_=sns.violinplot(data=df, x='연도', y='평당분양가격', palette=sns.color_palette(\"Pastel1\"))\n\n\n\n\nviolinplot을 사용하여 연도별 분양가격의 빈도를 파악할 수 있고, 마찬가지로 연도별로 증가하는 추세를 보이고 있음을 확인할 수 있습니다.\n\n\n\n\n이번엔 연도별 월별 평당분양가격의 평균을 시각화하여 달마다 어떤 추세가 있는지 확인해보겠습니다.\n\n_=plt.figure(figsize=(15, 7))\n\n_=plt.title(\"연도별 월별 평당 분양가격 평균\", fontsize=20, pad=10)\n_=sns.barplot(data=df, x='연도', y='평당분양가격', hue='월', ci=None, palette=sns.color_palette(\"Accent\"))\n\n_=plt.legend(loc=\"upper left\")\n\n_=plt.ylim(0, 13000)\n\n\n\n\n연도별 월별로 평당 분양가격의 평균 시각화를 그려보았을 때, 대체적으로 연초보다는 연말에 분양가격이 높은 것으로 보입니다.\n\n\n\n\n\n\n\n\nbaplot\n\n\n_=plt.figure(figsize=(17, 6))\n\n_=plt.title(\"지역별 평당 분양가격 평균 (단위: 천원)\", fontsize=20, pad=10)\nbars=sns.barplot(data=df, x='지역명', y='평당분양가격', ci=None, palette=sns.color_palette(\"turbo\"))\n\nfor i, b in enumerate(bars.patches):\n    _=plt.text(b.get_x()+b.get_width()*(1/2),b.get_height()+500, \\\n            round(b.get_height()),ha='center',fontsize=15, color='#706363')\n\n_=plt.ylim(0, 25000)\n\n\n\n\n서울의 분양가가 압도적으로 높은 것을 확인할 수 있습니다. 더 직관적으로 지역별 집값을 확인해보기 위해 내림차순으로 sorting 하여 그려보도록 하겠습니다.\n\n_=plt.figure(figsize=(17, 6))\n\n# order에 사용 할 시리즈 생성 \nsr1_order = df.groupby(\"지역명\")[\"평당분양가격\"].mean().sort_values(ascending=False)\n\n_=plt.title(\"지역별 평당 분양가격 평균 (단위: 천원)\", fontsize=20, pad=10)\nbars=sns.barplot(data=df, x='지역명', y='평당분양가격', ci=None, palette=sns.color_palette(\"turbo\"), order=sr1_order.index)\n\nfor i, b in enumerate(bars.patches):\n    _=plt.text(b.get_x()+b.get_width()*(1/2),b.get_height()+500, \\\n            round(b.get_height()),ha='center',fontsize=15, color='#706363')\n\n_=plt.ylim(0, 25000)\n\n\n\n\n서울-경기-부산-인천 등의 순으로 분양가가 높고, 전남-전북-충북 지역의 분양가가 가장 낮은 것으로 나타납니다.\n\nboxplot & violinplot\n\n\nfig = plt.figure(figsize=(17, 15))\nax1, ax2 = fig.subplots(2, 1)\n\n_=plt.suptitle(\"지역별 평당 분양가격 (단위: 천원)\", fontsize=25)\n\n_=sns.boxplot(data=df, x='지역명', y='평당분양가격', palette=sns.color_palette(\"turbo\"), ax=ax1)\n_=sns.violinplot(data=df, x='지역명', y='평당분양가격', palette=sns.color_palette(\"turbo\"), ax=ax2)\n\n_=ax1.set_title(\"boxplot\", pad=10, fontsize=20)\n_=ax2.set_title(\"violinplot\", pad=10, fontsize=20)\n\nfig.tight_layout()\n\n\n\n\n지역별로 시각화를 해보니, 이상치는 얼마 나타나지 않았습니다. 연도별 시각화에서 나온 이상치는 서울의 분양가였던 것으로 해석할 수 있습니다. 뒤에서 연도별 지역별로 그려보고 직접 확인해보도록 하겠습니다.\n\n\n\n\n\n\n\n\nheatmap\n\n\n# 지역별 연도별 pivot table 생성 \ndf_region_year = df.pivot_table(index=\"연도\", columns='지역명', values='평당분양가격').round().astype(int)\n\nfig = plt.figure(figsize=(20, 17), dpi=100)\nax1, ax2 = fig.subplots(2, 1)\n\n_=sns.heatmap(df_region_year, annot=True, fmt=\".0f\", cmap=\"Purples\", ax=ax1)\n_=sns.heatmap(df_region_year.T, annot=True, fmt=\".0f\", cmap=\"Purples\", ax=ax2)\n\n_=ax1.set_title(\"지역별 연도별 평당 분양가격 평균\", fontsize=20, pad=10)\n_=ax2.set_title(\"연도별 지역별 평당 분양가격 평균\", fontsize=20, pad=10)\n\nfig.tight_layout()\n\n\n\n\n위에서 연도 별 boxplot을 그려봤을 때 이상치로 나온 값들이 서울 분양가인 것으로 보입니다. 서울이 대체적으로 다른 지역의 두배 이상의 값인 것을 확인할 수 있습니다.\n\nlineplot\n\n\n_=plt.figure(figsize=(13, 8))\n\n_=sns.lineplot(data=df, x=\"연도\", y=\"평당분양가격\", hue=\"지역명\", ci=None, marker='o')\n_=plt.legend(bbox_to_anchor=(1.02, 1), loc=2)\n\n_=plt.title(\"연도별 지역별 평당 분양가격 (단위: 천원)\", pad=10, fontsize=20)\n\n\n\n\nlineplot으로 확인해 봐도 서울의 분양가격이 월등히 높은 것을 볼 수 있습니다. 또, 증가 폭도 높은 것으로 확인됩니다. 따라서 다음으로는 서울 지역만 따로 뽑아 시각화 해보도록 하겠습니다.\n\n\n\n\n\ndf_seoul = df[df['지역명']=='서울'].copy()\ndf_seoul.shape\n\n(72, 4)\n\n\n서울 지역만 따로 뽑아 df_seoul 변수에 넣어주었습니다.\n\nbarplot\n\n\n_=plt.figure(figsize=(10, 5))\n\n_=plt.title(\"연도별 평당분양가격 평균 (서울)\", fontsize=20, pad=10)\n\nbars=sns.barplot(data=df_seoul, x=\"연도\", y=\"평당분양가격\", ci=None, palette=sns.color_palette(\"magma_r\"))\n\nfor i, b in enumerate(bars.patches):\n    _=plt.text(b.get_x()+b.get_width()*(1/2),b.get_height()+500, \\\n            round(b.get_height()),ha='center',fontsize=15, color='#706363')\n\n_=plt.ylim(0, 29900)\n\n\n\n\n연도 별로 증가하는 추세를 보입니다.\n\nboxplot\n\n\n_=plt.figure(figsize=(10, 5))\n\n_=plt.title(\"연도별 평당분양가격 (서울)\", fontsize=20, pad=10)\n\nbars=sns.boxplot(data=df_seoul, x=\"연도\", y=\"평당분양가격\", palette=sns.color_palette(\"magma_r\"))\n\n\n\n\n서울지역의 연도별 boxplot을 그려보았을 때, 이상치는 존재하지 않습니다. 또, 값을 살펴보면, 위에서 전체 지역의 연도별 평당분양가격의 boxplot에서 이상치로 나왔던 값들이 서울 지역의 값들인 것을 직접 확인할 수 있습니다.\n \n\n\n\n\n\n4번에서 설정했던 가설을 토대로 시각화 분석 결과를 요약합니다.\n1 ) 연도별 분양가격 동향 - 지속적으로 분양가격이 증가한다.\n2 ) 지역별 분양가격 - 수도권 근방, (서울, 경기도, ..) 광역시 (대구, ..)의 분양가격이 높다. 특히 서울은 다른 지역의 2배 정도의 분양가임을 확인하였다.\n3 ) 지역별 연도별 분양가격 - 대체적으로 모든 지역에서 연도마다 지속적으로 분양가가 증가해왔다.\n\n전국의 분양가격 데이터로 시각화 분석을 진행하였습니다. 분양가격의 동향을 연도별, 지역별로 확인하였고, 어느 지역의 분양가가 높은지 정보를 얻을 수 있었습니다. 시각화 분석 과정에서 전체 지역에서는 이상치로 나왔던 값들이 서울 지역의 보편적인 값이라는 점을 확인하였고, 이를 통해 이상치를 섣불리 삭제해서는 안 되고, 값을 분석해 본 후에 처리해야 한다는 것을 배웠습니다."
  }
]