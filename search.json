[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "blog",
    "section": "",
    "text": "[cropdoctor] 4. ì‘ë¬¼ ì§ˆë³‘ ë°ì´í„°ì…‹ìœ¼ë¡œ ì»¤ìŠ¤í…€ YOLO ëª¨ë¸ í•™ìŠµ\n\n\n\n\n\n\n\nproject\n\n\nelice\n\n\n\n\n\n\n\n\n\n\n\nMay 7, 2023\n\n\nagsu\n\n\n\n\n\n\n  \n\n\n\n\n[cropdoctor] 3. ì‘ë¬¼ ì§ˆë³‘ ì§„ë‹¨ ëª¨ë¸ í•™ìŠµ ë° í‰ê°€ (mobilenetV2)\n\n\n\n\n\n\n\nproject\n\n\nelice\n\n\n\n\n\n\n\n\n\n\n\nMay 3, 2023\n\n\nagsu\n\n\n\n\n\n\n  \n\n\n\n\n[cropdoctor] 2. tar í™•ì¥ì ëª¨ë¸ docker image ë¶ˆëŸ¬ì˜¤ê¸°\n\n\n\n\n\n\n\nproject\n\n\nelice\n\n\n\n\n\n\n\n\n\n\n\nApr 30, 2023\n\n\nagsu\n\n\n\n\n\n\n  \n\n\n\n\n[cropdoctor] 1. ë…¸ì§€ì‘ë¬¼ ë°ì´í„° EDA\n\n\n\n\n\n\n\nproject\n\n\nelice\n\n\n\n\n\n\n\n\n\n\n\nApr 28, 2023\n\n\nagsu\n\n\n\n\n\n\n  \n\n\n\n\nPILì„ ì‚¬ìš©í•œ ì „í†µì ì¸ ì´ë¯¸ì§€ ì²˜ë¦¬ ë°©ë²•\n\n\n\n\n\n\n\nImage\n\n\n\n\n\n\n\n\n\n\n\nApr 4, 2023\n\n\nagsu\n\n\n\n\n\n\n  \n\n\n\n\nì„œìš¸ì‹œ 1ì¸ê°€êµ¬ ì‹œê°í™” ë¶„ì„\n\n\n\n\n\n\n\nvisualization\n\n\n\n\n\n\n\n\n\n\n\nMar 24, 2023\n\n\nagsu\n\n\n\n\n\n\n  \n\n\n\n\n[stockait] stockait ì‚¬ìš©í•˜ì—¬ ì£¼ê°€ ë°ì´í„° í‘œì¤€í™” ì‹¤í—˜í•˜ê¸°\n\n\n\n\n\n\n\nproject\n\n\nstockait\n\n\n\n\n\n\n\n\n\n\n\nFeb 15, 2023\n\n\nagsu\n\n\n\n\n\n\n  \n\n\n\n\n[stockait] stockait ì‚¬ìš©í•˜ì—¬ ì£¼ê°€ ì˜ˆì¸¡ ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ ì‹¤í—˜í•˜ê¸°\n\n\n\n\n\n\n\nproject\n\n\nstockait\n\n\n\n\n\n\n\n\n\n\n\nFeb 11, 2023\n\n\nagsu\n\n\n\n\n\n\n  \n\n\n\n\n[stockait] stockaitì—ì„œ traderì˜ ê°œë…ê³¼ ì‚¬ìš©ë²•\n\n\n\n\n\n\n\nproject\n\n\nstockait\n\n\n\n\n\n\n\n\n\n\n\nFeb 5, 2023\n\n\nagsu\n\n\n\n\n\n\n  \n\n\n\n\n[stockait] stockait ì‚¬ìš©í•˜ì—¬ ì£¼ê°€ ì˜ˆì¸¡ ëª¨ë¸ í•™ìŠµí•˜ê¸°\n\n\n\n\n\n\n\nproject\n\n\nstockait\n\n\n\n\n\n\n\n\n\n\n\nFeb 2, 2023\n\n\nagsu\n\n\n\n\n\n\n  \n\n\n\n\n[stock prediction] 3.3 í´ëŸ¬ìŠ¤í„° íƒìƒ‰ì„ í†µí•œ ì£¼ê°€ìƒìŠ¹ íŒ¨í„´ ê²€ì¶œ\n\n\n\n\n\n\n\nproject\n\n\nstock prediction\n\n\n\n\n\n\n\n\n\n\n\nJul 25, 2022\n\n\nagsu\n\n\n\n\n\n\n  \n\n\n\n\n[stock prediction] 3.2 t-SNEë¥¼ ì‚¬ìš©í•œ ì£¼ê°€ë°ì´í„° 2ì°¨ì› ì‹œê°í™”\n\n\n\n\n\n\n\nproject\n\n\nstock prediction\n\n\n\n\n\n\n\n\n\n\n\nJul 22, 2022\n\n\nagsu\n\n\n\n\n\n\n  \n\n\n\n\n[stock prediction] 3.1. ì„¤ëª…ê°€ëŠ¥ AI (XAI), SHAP value\n\n\n\n\n\n\n\nproject\n\n\nstock prediction\n\n\n\n\n\n\n\n\n\n\n\nJul 18, 2022\n\n\nagsu\n\n\n\n\n\n\n  \n\n\n\n\n[stock prediction] 2.3 CCIë¥¼ ì´ìš©í•œ ì£¼ê°€ ë°ì´í„° í•„í„°ë§\n\n\n\n\n\n\n\nproject\n\n\nstock prediction\n\n\n\n\n\n\n\n\n\n\n\nJul 15, 2022\n\n\nagsu\n\n\n\n\n\n\n  \n\n\n\n\n[stock prediction] 2.2 ì£¼ê°€ ë°ì´í„° ìŠ¤ì¼€ì¼ë§\n\n\n\n\n\n\n\nproject\n\n\nstock prediction\n\n\n\n\n\n\n\n\n\n\n\nJul 11, 2022\n\n\nagsu\n\n\n\n\n\n\n  \n\n\n\n\n[stock prediction] 2.1 ì£¼ê°€ ë°ì´í„°ì…‹ ë³´ì¡°ì§€í‘œ ì¶”ê°€\n\n\n\n\n\n\n\nproject\n\n\nstock prediction\n\n\n\n\n\n\n\n\n\n\n\nJul 9, 2022\n\n\nagsu\n\n\n\n\n\n\n  \n\n\n\n\n[stock prediction] 1.2 ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ ë¹„êµ\n\n\n\n\n\n\n\nproject\n\n\nstock prediction\n\n\n\n\n\n\n\n\n\n\n\nJul 8, 2022\n\n\nagsu\n\n\n\n\n\n\n  \n\n\n\n\n[stock prediction] 1.1 ë¨¸ì‹ ëŸ¬ë‹ì„ ìœ„í•œ ì£¼ê°€ ë°ì´í„°ì…‹ ìƒì„±\n\n\n\n\n\n\n\nproject\n\n\nstock prediction\n\n\n\n\n\n\n\n\n\n\n\nJul 6, 2022\n\n\nagsu\n\n\n\n\n\n\n  \n\n\n\n\nìƒê¶Œì •ë³´ ì‹œê°í™” ë¶„ì„\n\n\n\n\n\n\n\nlecture/practice\n\n\nvisualization\n\n\n\n\n\n\n\n\n\n\n\nMay 7, 2022\n\n\nagsu\n\n\n\n\n\n\n  \n\n\n\n\nì „êµ­ ì•„íŒŒíŠ¸ ë¶„ì–‘ê°€ê²© ì‹œê°í™” ë¶„ì„\n\n\n\n\n\n\n\nlecture/practice\n\n\nvisualization\n\n\n\n\n\n\n\n\n\n\n\nApr 19, 2022\n\n\nagsu\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/01.stock_dataset.html",
    "href": "posts/01.stock_dataset.html",
    "title": "[stock prediction] 1.1 ë¨¸ì‹ ëŸ¬ë‹ì„ ìœ„í•œ ì£¼ê°€ ë°ì´í„°ì…‹ ìƒì„±",
    "section": "",
    "text": "ë³¸ ê¸€ì—ì„œëŠ” ì‹œê°€, ì €ê°€, ê³ ê°€, ì¢…ê°€, ê±°ë˜ëŸ‰, ë³€í™”ìœ¨ë¡œ êµ¬ì„±ë˜ì–´ìˆëŠ” ì£¼ê°€ ë°ì´í„°ì…‹ì„ ë¨¸ì‹ ëŸ¬ë‹ í•™ìŠµì„ ìœ„í•œ í˜•íƒœë¡œ ì „ì²˜ë¦¬í•œë‹¤. ë˜í•œ, ì´ ì „ì²˜ë¦¬ ê³¼ì •ì—ì„œ ì²˜ìŒì— ì‹œë„í•˜ì˜€ë˜ ë°©ë²•ìœ¼ë¡œë¶€í„° ì ì°¨ íš¨ìœ¨ì ìœ¼ë¡œ ê°œì„ í•˜ì—¬ ì²˜ë¦¬ ì‹œê°„ì„ ì¤„ì´ëŠ” ë°©ë²•ì˜ ê³¼ì •ì„ ë‹´ì•˜ë‹¤.\n\n\n\në³¸ ì—°êµ¬ì—ì„œ ë¨¸ì‹ ëŸ¬ë‹ í•™ìŠµì„ ìœ„í•œ ë°ì´í„°ì…‹ì€ ë‹¤ìŒê³¼ ê°™ì´ ì •ì˜í•˜ì˜€ë‹¤.\n\nì¡°ê±´: ì „ì²´ ì•½ 2000ê°œ ì¢…ëª© ì¤‘ 2018ë…„ ë¶€í„° ì¡´ì†í•˜ì˜€ë˜ ê¸°ì—… ì¤‘ ê±°ë˜ëŒ€ê¸ˆì´ 1000ì–µ ì´ìƒ ë°œìƒí•œ ë‚ ì§œ (ê±°ë˜ëŒ€ê¸ˆì€ ì¶”í›„ì— ìˆ˜ì • ì˜ˆì •)\në…ë¦½ë³€ìˆ˜ ì •ì˜: ì¡°ê±´ ì—ì„œ ì„ ë³„í•œ íŠ¹ì • ë‚ ì§œë¥¼ D0ë¼ê³  í–ˆì„ ë•Œ, D-9, D-8, â€¦, D0, ì´ 10ì¼ ì¹˜ì˜ Open, High, Low, Close, ê±°ë˜ëŒ€ê¸ˆ(trading_value)\nì¢…ì†ë³€ìˆ˜ ì •ì˜: ì¡°ê±´ì— ë¶€í•©í•˜ëŠ” íŠ¹ì • ë‚ ì§œ(D0) ëŒ€ë¹„ ë‹¤ìŒë‚ (D+1) ë‚ ì§œì˜ ì¢…ê°€(close)ê°€ 2%ì´ìƒ ìƒìŠ¹í•˜ë©´ 1, ìƒìŠ¹í•˜ì§€ ì•Šìœ¼ë©´ 0\ní•™ìŠµ&ì‹œí—˜ ë°ì´í„°ì…‹ ì •ì˜\n\ntrain dataset: 2018ë…„ 1ì›” 2ì¼ - 2020ë…„ 12ì›” 31ì¼ (2ë…„)\ntest dataset: 2021ë…„ 1ì›” 2ì¼ ~ 2021ë…„ 6ì›” 31ì¼ (6ê°œì›”)\n\n\n\n\n\n\në°ì´í„°ì…‹ ìƒì„±ì„ 3ë‹¨ê³„ì˜ ê³¼ì •ìœ¼ë¡œ ë‚˜ëˆ„ì–´ ì§„í–‰í–ˆë‹¤.\n\nê³¼ì œ I: ì¢…ëª© ë¦¬ìŠ¤íŠ¸ ìƒì„±-2018ë…„ ë¶€í„° ì¡´ì†í•˜ì˜€ë˜ ê¸°ì—…(ìƒì¥ì¼ 2018ë…„ 1ì›” 1ì¼ ì´ì „ ê¸°ì—…) ì„ ë³„\nê³¼ì œ II: ê³¼ì œI ì˜ ê¸°ì—… ì¤‘ ê±°ë˜ëŒ€ê¸ˆì´ 1000ì–µ ì´ìƒ ë°œìƒí–ˆë˜ íŠ¹ì • ë‚ ì§œ ì„ ë³„ (ê±°ë˜ëŒ€ê¸ˆ=ê±°ë˜ëŸ‰Xì¢…ê°€ ë¡œ ê³„ì‚°)\nê³¼ì œ III: ê³¼ì œII ì˜ ë‚ ì§œì— ëŒ€í•´ ìµœì¢… ë¨¸ì‹ ëŸ¬ë‹ ë°ì´í„° ìƒì„±í•˜ì—¬ csv íŒŒì¼ë¡œ ì €ì¥\n\n\nì£¼ê°€ ë°ì´í„°ì…‹ ìƒì„± ì ˆì°¨ëŠ” df2list - dictionary - MultiProcessing - MySQL ìˆœì„œë¡œ ì§„í–‰í•œë‹¤. 4ê°€ì§€ ì ˆì°¨ë¥¼ í†µí•´ ë°ì´í„°ì…‹ì´ ë‹¬ë¼ì§€ëŠ” ê²ƒì€ ì•„ë‹ˆê³ , ì†ë„ë¥¼ ê°œì„ ì‹œí‚¤ëŠ” ë°©í–¥ìœ¼ë¡œì˜ íš¨ìœ¨ì ì¸ ì½”ë“œë¥¼ ì§œê¸° ìœ„í•œ í›ˆë ¨ ê³¼ì •ì´ë‹¤. ëŒ€ê·œëª¨ ë°ì´í„°ì…‹ì„ ë‹¤ë£¨ê¸° ìœ„í•´ì„œëŠ” íš¨ìœ¨ì ì¸ ì½”ë”©ì„ í†µí•´ ì†ë„ë¥¼ ê°œì„ í•˜ëŠ” ê²ƒë„ ì¤‘ìš”í•œ ì¼ì´ë‹¤. íŠ¹íˆ ì´ë²ˆì— ì‚¬ìš©í•˜ëŠ” ì£¼ê°€ ë°ì´í„°ì…‹ì€ ì–‘ì´ ë§ì„ ë¿ë§Œ ì•„ë‹ˆë¼, fdr ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•˜ì—¬ ì™¸ë¶€ì—ì„œ ë¶ˆëŸ¬ì™€ì•¼í•˜ê¸° ë•Œë¬¸ì— ë°ì´í„°ì…‹ì„ êµ¬ì„±í•˜ëŠ” ê²ƒë§Œ í•´ë„ ì†ë„ê°€ ìƒë‹¹íˆ ëŠë¦¬ë‹¤. ì´ëŸ¬í•œ ë¬¸ì œì ì„ í•´ê²°í•˜ê³ ì ë³¸ í¬ìŠ¤íŒ…ì—ì„œëŠ” 4ë‹¨ê³„ì— ê±¸ì¹œ ë°ì´í„°ì…‹ ìƒì„± ê³¼ì •ì„ ë³´ì—¬ì¤€ë‹¤.\n\n\n\n\n\n(0) Finance Data Readerë¥¼ ì´ìš©í•œ ì£¼ê°€ ë°ì´í„°ì…‹ - DataFrame\n(1) Finance Data Readerë¥¼ ì´ìš©í•œ ì£¼ê°€ ë°ì´í„°ì…‹ - df2list\n(2) Finance Data Readerë¥¼ ì´ìš©í•œ ì£¼ê°€ ë°ì´í„°ì…‹ - dictionary\n(3) Finance Data Readerë¥¼ ì´ìš©í•œ ì£¼ê°€ ë°ì´í„°ì…‹ - MultiProcessing\n(4) Finance Data Readerë¥¼ ì´ìš©í•œ ì£¼ê°€ ë°ì´í„°ì…‹ - MySQL\n(5) ìµœì¢… ë¨¸ì‹ ëŸ¬ë‹ ë°ì´í„°ì…‹\n\n\n\n\n\n# finance datareader ì„¤ì¹˜ \n# ! pip install -U finance-datareader\n\n\nimport pandas as pd\nimport os\nfrom tqdm import tqdm\nimport FinanceDataReader as fdr\nimport time\n\n# ê²½ê³  ë©”ì‹œì§€ ë¬´ì‹œ \nimport warnings\nwarnings.filterwarnings(action='ignore')\n\n \n\n\n\n\nì†ë„ ë¹„êµë¥¼ ìœ„í•´ ê¸°ì¡´ ë°ì´í„°ì˜ íƒ€ì…ì¸ DataFrameì„ ì‚¬ìš©í•˜ì—¬ ë°ì´í„°ì…‹ì„ ìƒì„±í•œë‹¤.\nê³¼ì œ I\n\ndf = pd.read_html('http://kind.krx.co.kr/corpgeneral/corpList.do?method=download', header=0)[0]\n\n# íšŒì‚¬ëª…, ì¢…ëª©ì½”ë“œ, ìƒì¥ì¼ ì»¬ëŸ¼ë§Œ ì‚¬ìš© \ndf_code = df[['íšŒì‚¬ëª…', 'ì¢…ëª©ì½”ë“œ', 'ìƒì¥ì¼']]\n\n# ì¢…ëª©ì½”ë“œë¥¼ 6ìë¦¬ë¡œ ë§ì¶° ì¤€ë‹¤. \ndf_code['ì¢…ëª©ì½”ë“œ'] = df_code['ì¢…ëª©ì½”ë“œ'].apply(lambda x : str(x).zfill(6))\ndisplay(df_code.head(3))\nprint()\n\n# ìƒì¥ì¼ì´ 2018ë…„ 1ì›” 1ì¼ ì´ì „ì¸ ì¢…ëª©ì½”ë“œ ì„ ë³„ \nstart_time = time.time()\nlst_code = df_code.loc[df_code['ìƒì¥ì¼'] < '2018-01-01', 'ì¢…ëª©ì½”ë“œ'].to_list()\nprint(\"ê±¸ë¦° ì‹œê°„: \", time.time() - start_time)\nprint()\nprint('ìƒì¥ì¼ 2018-01-01 ì´ì „ ì¢…ëª©ì½”ë“œ: ', lst_code[:5])\nprint()\nprint(f'ì´ {len(df_code)} ê°œì˜ ì¢…ëª© ì¤‘ {len(lst_code)} ê°œì˜ ì¢…ëª© ì„ ë³„')\n\n\n\n\n\n  \n    \n      \n      íšŒì‚¬ëª…\n      ì¢…ëª©ì½”ë“œ\n      ìƒì¥ì¼\n    \n  \n  \n    \n      0\n      DL\n      000210\n      1976-02-02\n    \n    \n      1\n      DRBë™ì¼\n      004840\n      1976-05-21\n    \n    \n      2\n      DSR\n      155660\n      2013-05-15\n    \n  \n\n\n\n\n\nê±¸ë¦° ì‹œê°„:  0.0006058216094970703\n\nìƒì¥ì¼ 2018-01-01 ì´ì „ ì¢…ëª©ì½”ë“œ:  ['000210', '004840', '155660', '078930', '001390']\n\nì´ 2536 ê°œì˜ ì¢…ëª© ì¤‘ 1972 ê°œì˜ ì¢…ëª© ì„ ë³„\n\n\nê³¼ì œ II\n\nstock_dict = {}\nfor code in tqdm(lst_code): \n    stock = fdr.DataReader(code, start='20180101', end='20201231')\n    stock['trading'] = stock['Volume'] * stock['Close'] # ê±°ë˜ëŒ€ê¸ˆ ì»¬ëŸ¼ ì¶”ê°€\n    \n    if sum(stock['trading'] >= 100000000000) >= 1: # ê±°ë˜ëŒ€ê¸ˆì´ 1000ì–µ ì´ìƒì¸ ë°ì´í„°ê°€ í•˜ë‚˜ ì´ìƒ ì¡´ì¬í•˜ë©´\n        stock_dict[code] = stock[stock['trading'] >= 100000000000].index # index == Date \n\nprint(f'ì´ {len(lst_code)} ê°œì˜ ì¢…ëª© ì¤‘ {len(stock_dict)} ê°œì˜ ì¢…ëª© ì‚¬ìš©')\n\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1977/1977 [02:58<00:00, 11.08it/s]\n\n\nì´ 1977 ê°œì˜ ì¢…ëª© ì¤‘ 799 ê°œì˜ ì¢…ëª© ì‚¬ìš©\n\n\n\n\n\në¨¼ì €, fdrDataReader íŒ¨í‚¤ì§€ë¥¼ ì‚¬ìš©í•˜ì—¬ ê±°ë˜ëŒ€ê¸ˆì´ 1000ì–µ ì´ìƒì¸ ë°ì´í„°ì˜ ë‚ ì§œë¥¼ stock_dictì— stock_dict[code] = [ë‚ ì§œ,â€¦] ì˜ í˜•ì‹ìœ¼ë¡œ ë„£ì–´ì£¼ì—ˆë‹¤.\n\n# ì„ ë³„ëœ ì¢…ëª©ê³¼ ë‚ ì§œë¥¼ lst_code_dateì— ë„£ì–´ì¤€ë‹¤. \nlst_code_date = []\nfor code in tqdm(stock_dict): \n    for date in (stock_dict[code]):\n        lst_code_date.append([code, date])\n        \nprint(f'ì„ ë³„ëœ ë‚ ì§œëŠ” ì´ {len(lst_code_date)}ê°œ')\n\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 799/799 [00:00<00:00, 13125.89it/s]\n\n\nì„ ë³„ëœ ë‚ ì§œëŠ” ì´ 14182ê°œ\n\n\n\n\n\nlst_code_dateì— [ì½”ë“œ, ë‚ ì§œ] í˜•ì‹ìœ¼ë¡œ ì¶”ê°€í•œë‹¤.\nê³¼ì œ III\n\ndata_dict = {'code': [], 'd0': [], 'info': [], 'up': []}\nfor code, date in tqdm(lst_code_date):\n    start_date = '20171201' # 2018ë…„ ì´ˆë°˜ ë‚ ì§œê°€ D0ë¼ë©´ 2017ë…„ ë°ì´í„° í•„ìš” (D-9~D-1)\n    end_date = '20210130' # 2020ë…„ í›„ë°˜ ë‚ ì§œê°€ D0ë¼ë©´ 2021ë…„ ë°ì´í„° í•„ìš” (D+1) \n    stock = fdr.DataReader(code, start = start_date, end = end_date)\n    stock.reset_index(inplace=True) # 'Date' index -> column \n    \n    D9_index = stock[stock['Date'] == str(date)].index[0] - 9 # D-9 ë‚ ì§œì˜ ì¸ë±ìŠ¤  \n    next_index = stock[stock['Date'] == str(date)].index[0] + 1 # D+1 ë‚ ì§œì˜ ì¸ë±ìŠ¤  \n        \n    # ì¢…ëª©ì½”ë“œ (code)\n    data_dict['code'].append(code) \n    \n    # ê¸°ì¤€ì¼ (d0)\n    data_dict['d0'].append(date)\n    \n    # D-9 ~ D+1, ì´ 11ì¼ì¹˜ì˜ sub stock DataFrame ìƒì„± \n    sub_stock = stock.iloc[D9_index:next_index+1]\n    sub_stock['trading'] = sub_stock['Close'] * sub_stock['Volume'] # ê±°ë˜ëŒ€ê¸ˆ ì»¬ëŸ¼ ì¶”ê°€ \n   \n    \n    # 10ì¼ ê°„ì˜ ë°ì´í„° (info)\n    info_list = []\n    for i in range(10):        \n        info_list.append(sub_stock.iloc[i, [1, 2, 3, 4, -1]].to_list())\n    remove_list=['[', ']']\n    for i in range(2): \n        info_list = f'{info_list}'.replace(remove_list[i], '')\n    data_dict['info'].append(info_list)    \n        \n        \n    # D+1 ì¢…ê°€ 2% ìƒìŠ¹ ì—¬ë¶€ (up)\n    up = sub_stock.iloc[-2]['Close'] + 0.02 * sub_stock.iloc[-2]['Close']\n    \n    if sub_stock.iloc[-1]['Close'] >= up: \n        data_dict['up'].append(1)\n    else: \n        data_dict['up'].append(0)\n\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 14182/14182 [17:08<00:00, 13.78it/s]\n\n\n\ndf_result = pd.DataFrame(data_dict)\ndisplay(df_result.head()) \n\n# ìµœì¢… ê²°ê³¼ ë°ì´í„°ì…‹ txt íŒŒì¼ ì €ì¥ \ndf_result.to_csv(\"assignment3.txt\")\nprint(f'ìƒì„±ëœ ë°ì´í„°ì˜ ê°œìˆ˜ëŠ” {len(pd.read_csv(\"assignment3.txt\"))} ê°œ')\n\n\n\n\n\n  \n    \n      \n      code\n      d0\n      info\n      up\n    \n  \n  \n    \n      0\n      000210\n      2018-01-26\n      78343, 78614, 76987, 77892, 9590608284, 77801,...\n      0\n    \n    \n      1\n      000210\n      2018-08-08\n      68855, 69397, 67590, 69036, 6067435968, 69487,...\n      0\n    \n    \n      2\n      000210\n      2020-04-02\n      44819, 52951, 44322, 51145, 15615437965, 47439...\n      0\n    \n    \n      3\n      000210\n      2020-09-11\n      80783, 84487, 78524, 78524, 61554885076, 79608...\n      0\n    \n    \n      4\n      000210\n      2020-12-11\n      76174, 76174, 72289, 72289, 76043112348, 72831...\n      0\n    \n  \n\n\n\n\nìƒì„±ëœ ë°ì´í„°ì˜ ê°œìˆ˜ëŠ” 14182 ê°œ\n\n\n\nìµœì¢…ì ìœ¼ë¡œ ë¨¸ì‹ ëŸ¬ë‹ í•™ìŠµì„ ìœ„í•œ ë°ì´í„°ì…‹ì„ ì²˜ë¦¬í•˜ëŠ” ì½”ë“œëŠ” ì´ 17ë¶„ì´ ê±¸ë ¸ë‹¤. ì—°êµ¬ ì´ˆë°˜ì— ê°€ì¥ ì²« ë²ˆì§¸ë¡œ ì‘ì„±í•œ ì½”ë“œì¸ë°, ì§€ê¸ˆ ë³´ë‹ˆ í™•ì‹¤íˆ ë¹„íš¨ìœ¨ì ìœ¼ë¡œ ì‘ì„±í–ˆë˜ ë¶€ë¶„ì´ ë§ì€ ê²ƒ ê°™ë‹¤.\n\nê°œì„ í•´ì•¼í•  ì‚¬í•­\n\nì†ë„ ê°œì„ : ê³¼ì œIII ì€ ì•½ 17ë¶„ì´ ê±¸ë¦° ê²ƒì„ ë³´ì•„ ì†ë„ ë©´ì—ì„œ ìƒë‹¹íˆ ë¹„íš¨ìœ¨ì ì´ì—ˆë‹¤. (ê±°ë˜ëŒ€ê¸ˆì„ 10ì–µìœ¼ë¡œ ì„¤ì •í–ˆì„ ë•Œ 1% ì§„í–‰ì— 5ë¶„ì´ ê±¸ë ¸ìŠµë‹ˆë‹¤. ê·¸ëŸ¼ ì´ 500ë¶„ì´ ê±¸ë¦´ ê²ƒìœ¼ë¡œ ì˜ˆìƒí•  ìˆ˜ ìˆë‹¤.)\n# 10ì¼ ê°„ì˜ ë°ì´í„° ë¶€ë¶„: info_listì— ë°ì´í„°ë¥¼ ì¶”ê°€í•˜ëŠ” ê³¼ì •ì—ì„œ ì–µì§€ë¡œ í¬ë§·ì„ ë§ì¶”ê¸° ìœ„í•´ ë¶ˆí•„ìš”í•œ forë¬¸ì´ ë“¤ì–´ê°”ë‹¤.\n# D+1 ì¢…ê°€ 2% ìƒìŠ¹ ì—¬ë¶€ ë¶€ë¶„: ì „ ë‚  ëŒ€ë¹„ 2% ìƒìŠ¹ìœ¨ì„ ì§ì ‘ ê³„ì‚°í•´ ì£¼ì—ˆëŠ”ë°, ì´ë¯¸ ë³€í™”ìœ¨ì´ ê³„ì‚° ë˜ì–´ ìˆëŠ” changeë¼ëŠ” ì»¬ëŸ¼ì„ ì‚¬ìš©í•˜ëŠ” ê²ƒìœ¼ë¡œ ëŒ€ì²´í•œë‹¤.\ntxt íŒŒì¼ ì €ì¥: DataFrameìœ¼ë¡œ ìƒì„±ì„ í•œ í›„ì— txt íŒŒì¼ë¡œ ì €ì¥í–ˆëŠ”ë°, ë°ì´í„° ìƒì„± ì‹œ íŒŒì¼ ì…ì¶œë ¥ write()ë¥¼ ì‚¬ìš©í•´ì„œ ë°”ë¡œ txt íŒŒì¼ë¡œ ì €ì¥í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ ë³€ê²½í•œë‹¤.\n\n\n \n\n\n\nì²«ë²ˆì§¸ ë°©ë²•ì€ DataFrame ì‚¬ìš©ì„ ì§€ì–‘í•˜ê³ , pythonì˜ ê¸°ë³¸ ë°ì´í„° íƒ€ì…ì¸ listë¡œ ë°”ê¾¸ì–´ ì‚¬ìš©í•˜ëŠ” ë°©ë²•ì…ë‹ˆë‹¤. ì´ë¡œì¨ column ì¤‘ì‹¬ì˜ ì—°ì‚°ì„ row ì¤‘ì‹¬ì˜ ì—°ì‚°ìœ¼ë¡œ ë°”ê¾¼ë‹¤. ì´ ë°©ë²•ì—ì„œëŠ” ì†ë„ ê°œì„  ë³´ë‹¤ëŠ” (0)ë²ˆ ë°©ë²•ì˜ ì½”ë“œì—ì„œ íš¨ìœ¨ì ì´ì§€ ëª»í–ˆë˜ ë¶€ë¶„ì„ ê³ ì¹˜ê³  ê¹”ë”í•œ ì½”ë“œë¡œ ë³´ì™„í•œë‹¤.\nê³¼ì œ I\n\n# (0) ë°©ë²•ì—ì„œ ì‚¬ìš©í–ˆë˜ df_code ë°ì´í„° í”„ë ˆì„ ì‚¬ìš© \ndisplay(df_code.head(2))\n\n# ğŸŒŸ dataframe -> list \nlst_stock = df_code.values.tolist()\nprint(lst_stock[:2])\nprint()\n\n\nlst_code = [] # ì„ ë³„ ëœ ì½”ë“œë¥¼ ë‹´ì„ ë¦¬ìŠ¤íŠ¸ \nstart_time = time.time()\nfor row in lst_stock:\n    code, date = row[1], row[2]\n    if date <= '2018-01-01':\n        lst_code.append(code)\nprint(\"ê±¸ë¦° ì‹œê°„: \", time.time() - start_time)\nprint()\n\n        \nprint('ìƒì¥ì¼ 2018-01-01 ì´ì „ ì¢…ëª©ì½”ë“œ: ', lst_code[:4])\nprint()\nprint(f'ì´ {len(df_code)} ê°œì˜ ì¢…ëª© ì¤‘ {len(lst_code)} ê°œì˜ ì¢…ëª© ì„ ë³„')\n\n\n\n\n\n  \n    \n      \n      íšŒì‚¬ëª…\n      ì¢…ëª©ì½”ë“œ\n      ìƒì¥ì¼\n    \n  \n  \n    \n      0\n      DL\n      000210\n      1976-02-02\n    \n    \n      1\n      DRBë™ì¼\n      004840\n      1976-05-21\n    \n  \n\n\n\n\n[['DL', '000210', '1976-02-02'], ['DRBë™ì¼', '004840', '1976-05-21']]\n\nê±¸ë¦° ì‹œê°„:  0.0004563331604003906\n\nìƒì¥ì¼ 2018-01-01 ì´ì „ ì¢…ëª©ì½”ë“œ:  ['000210', '004840', '155660', '078930']\n\nì´ 2536 ê°œì˜ ì¢…ëª© ì¤‘ 1972 ê°œì˜ ì¢…ëª© ì„ ë³„\n\n\nê³¼ì œ II\n\nlst_code_date = []\nfor code in tqdm(lst_code):\n    stock = fdr.DataReader(code, start='20180102', end='20201231')\n    stock.reset_index(inplace=True)\n    \n    # ğŸŒŸ dataframe -> list \n    lst_stock = stock.values.tolist()\n    \n    for row in lst_stock: \n        date, trading_value = row[0], row[4]*row[5]\n        if trading_value >= 100000000000:  # ê±°ë˜ëŒ€ê¸ˆ 1000ì–µ ì´ìƒ\n            lst_code_date.append([code, date.date().strftime(\"%Y%m%d\")])\n            \nprint(f'ì„ ë³„ëœ ë‚ ì§œëŠ” ì´ {len(lst_code_date)}ê°œ')\n\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1977/1977 [02:55<00:00, 11.26it/s]\n\n\nì„ ë³„ëœ ë‚ ì§œëŠ” ì´ 14182ê°œ\n\n\n\n\n\nê³¼ì œ III\n\nOF = open('assignment3.txt','w')\n\nfor code, date in tqdm(lst_code_date):\n    start_date = '20180101' \n    end_date = '20201231' \n    stock = fdr.DataReader(code, start = start_date, end = end_date)\n    stock.reset_index(inplace=True) # 'Date' index -> column \n    \n    \n    # ğŸŒŸ dataframe -> list \n    lst_stock = stock.values.tolist()\n    \n    \n    for idx, row in enumerate(lst_stock): \n        if (idx < 9) or (idx >= len(lst_stock)-1): # ì˜ˆì™¸ ì²˜ë¦¬ \n            continue \n        \n        if row[0].date().strftime(\"%Y%m%d\") == date: \n            \n            # D-9 ~ D0 ë°ì´í„°ë§Œ ë‹´ê¸°\n            sub_stock = lst_stock[idx-9:idx+1]\n            \n            # 10ì¼ ê°„ì˜ ë°ì´í„° \n            lst_info = []\n            for row2 in sub_stock:\n                lst_prices, trading_value = row2[1:5], row[4]*row[5]\n                lst_info += lst_prices + [trading_value]\n                \n            info = ','.join(map(str, lst_info))\n            \n            # D+1 ì¢…ê°€ 2% ìƒìŠ¹ ì—¬ë¶€ (up)\n            change = lst_stock[idx+1][6]\n            label = int(change >= 0.02)\n            \n            # ì €ì¥ \n            OF.write(f'{code}\\t{date}\\t{lst_info}\\t{label}\\n')\n            \nOF.close()\n\nprint(f'ìƒì„±ëœ ë°ì´í„°ì˜ ê°œìˆ˜ëŠ” {len(pd.read_csv(\"assignment3.txt\"))} ê°œ')\n\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 14182/14182 [17:43<00:00, 13.33it/s]\n\n\nìƒì„±ëœ ë°ì´í„°ì˜ ê°œìˆ˜ëŠ” 13939 ê°œ\n\n\n\n\n\n\nOF.write()ë¥¼ ì‚¬ìš©í•¨ìœ¼ë¡œì¨ ë¸”í•„ìš”í•œ DataFrame ì„ ìƒì„±í•˜ì§€ ì•ŠëŠ”ë‹¤.\n# 10ì¼ê°„ì˜ ë°ì´í„° ë¶€ë¶„ì€ join() í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ë¶ˆí•„ìš”í•œ forë¬¸ ì‚¬ìš©ì„ ì¤„ì˜€ê³ , # D+1 ì¢…ê°€ 2% ìƒìŠ¹ ì—¬ë¶€ (up) ë¶€ë¶„ì€ ì „ë‚  ëŒ€ë¹„ ì¢…ê°€ ë³€í™”ìœ¨ì„ ê³„ì‚°í•˜ì§€ ì•Šê³  change ì»¬ëŸ¼ì„ í™œìš©í•˜ëŠ” ê²ƒìœ¼ë¡œ ë°”ê¾¸ì—ˆë‹¤.\nì†ë„ì— ëŒ€í•œ ë¬¸ì œì  : ì „ ë³´ë‹¤ ê¹”ë”í•œ ì½”ë“œë¡œ ë³´ì™„ì´ ë˜ì—ˆì§€ë§Œ, ì•„ì§ ì†ë„ì— ëŒ€í•œ ë¬¸ì œì ì´ ë‚¨ì•„ìˆë‹¤. ì†ë„ë¥¼ ê°œì„ í•˜ê¸° ìœ„í•´ì„œëŠ” fdr ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ìµœì†Œí•œìœ¼ë¡œ ì‚¬ìš©í•´ì•¼í•œë‹¤. í•˜ì§€ë§Œ ì§€ê¸ˆ [ì½”ë“œ, ë‚ ì§œ]ì˜ í˜•íƒœë¡œ forë¬¸ì´ ëŒê³ ìˆê¸° ë•Œë¬¸ì—, ì¤‘ë³µë˜ëŠ” ê°™ì€ Codeë¥¼ ì—¬ëŸ¬ ë²ˆ ë¶ˆëŸ¬ì˜¤ê³  ìˆë‹¤. ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ lst_code_date ë¦¬ìŠ¤íŠ¸ ëŒ€ì‹ , dictionaryë¥¼ ì‚¬ìš©í•˜ëŠ” ë°©ë²•ìœ¼ë¡œ ë„˜ì–´ê°„ë‹¤.\n\n\n \n\n\n\ndictionaryë¥¼ ì‚¬ìš©í•˜ëŠ” ë°©ë²•ì€ ê³¼ì œIII ì˜ ì†ë„ë¥¼ ê°œì„ í•  ìˆ˜ ìˆë‹¤. fdr ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ìµœì†Œí•œìœ¼ë¡œ ì‚¬ìš©í•˜ê³ , forë¬¸ì„ ìµœëŒ€í•œ ì¤„ì¸ë‹¤. í˜„ì¬ ë¬¸ì œì ì€ ë‚ ì§œë¥¼ ê¸°ì¤€ìœ¼ë¡œ forë¬¸ì´ ëŒì•„ê°€ê¸° ë•Œë¬¸ì— ê°™ì€ ë°ì´í„°(ê°™ì€ ì¢…ëª©)ê°€ ì—¬ëŸ¬ë²ˆ ë¶ˆëŸ¬ì™€ì§€ëŠ” ê²½ìš°ê°€ ë‹¤ìˆ˜ ì¡´ì¬í•œë‹¤ëŠ” ê²ƒì´ë‹¤. ë”°ë¼ì„œ ê³¼ì œII ì—ì„œ codeë³„ D0 ë‚ ì§œ ë¦¬ìŠ¤íŠ¸ë¥¼ dictionary íƒ€ì…ìœ¼ë¡œ ìƒì„±í•˜ê³ , ê³¼ì œIII ì—ì„œ code ë‹¹ í•œë²ˆë§Œ fdr ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•˜ë„ë¡ ë°”ê¾¸ì–´ì¤€ë‹¤.\nê³¼ì œ I\nê³¼ì œI ì€ ì•ì˜ ë°©ë²•ê³¼ ê°™ìœ¼ë¯€ë¡œ ìƒëµí•œë‹¤.\n\nprint('ìƒì¥ì¼ 2018-01-01 ì´ì „ ì¢…ëª©ì½”ë“œ: ', lst_code[:4])\nprint()\nprint(f'ì´ {len(df_code)} ê°œì˜ ì¢…ëª© ì¤‘ {len(lst_code)} ê°œì˜ ì¢…ëª© ì„ ë³„')\n\nìƒì¥ì¼ 2018-01-01 ì´ì „ ì¢…ëª©ì½”ë“œ:  ['000210', '004840', '155660', '078930']\n\nì´ 2507 ê°œì˜ ì¢…ëª© ì¤‘ 1977 ê°œì˜ ì¢…ëª© ì„ ë³„\n\n\nê³¼ì œ II\n\ndict_code2date = {}\nfor code in tqdm(lst_code): \n    start_date = '20180102'\n    end_date = '20201231'\n    stock = fdr.DataReader(code, start = start_date, end = end_date)\n    stock.reset_index(inplace=True)    \n    \n    # ğŸŒŸ dataframe -> list     \n    lst_stock = stock.values.tolist()\n    \n    for row in lst_stock: \n        date, trading_value = row[0], row[4]*row[5]\n        if trading_value >= 100000000000:\n            if code not in dict_code2date.keys():\n                dict_code2date[code] = [date.date().strftime(\"%Y%m%d\")]\n            else:\n                dict_code2date[code].append(date.date().strftime(\"%Y%m%d\"))\n\nprint(f'ì´ {len(lst_code)} ê°œì˜ ì¢…ëª© ì¤‘ {len(dict_code2date)} ê°œì˜ ì¢…ëª© ì‚¬ìš©')\n\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1977/1977 [02:42<00:00, 12.14it/s]\n\n\nì´ 1977 ê°œì˜ ì¢…ëª© ì¤‘ 799 ê°œì˜ ì¢…ëª© ì‚¬ìš©\n\n\n\n\n\nê³¼ì œ III\n\nOF = open('assignment3.txt', 'w')\nfor code in tqdm(dict_code2date): \n    # codeì˜ stock \n    start_date = '20180101' \n    end_date = '20201231' \n    stock = fdr.DataReader(code, start = start_date, end = end_date)\n    stock.reset_index(inplace=True)\n    \n    # ğŸŒŸ dataframe -> list     \n    lst_stock = stock.values.tolist()  \n       \n    for idx, row in enumerate(lst_stock):   \n        if (idx < 9) or (idx >= len(lst_stock)-1): # ì˜ˆì™¸ ì²˜ë¦¬ \n            continue \n        \n        date = row[0].date().strftime(\"%Y%m%d\") \n        if date not in dict_code2date[code]: # ì¡°ê±´ì— ë¶€í•©í•˜ëŠ” ë‚ ì§œ (D0 ë‚ ì§œ)ë¥¼ ë°œê²¬í•  ë•Œê¹Œì§€ continue\n            continue \n\n        # D-9 ~ D0 ë°ì´í„°ë§Œ ë‹´ê¸°\n        sub_stock = lst_stock[idx-9:idx+1] \n        \n        # 10ì¼ê°„ì˜ ë°ì´í„° \n        lst_info = []\n        for row2 in sub_stock:\n            lst_prices, trading_value = row2[1:5], row2[4]*row2[5]\n            lst_info += lst_prices + [trading_value]\n        info = ','.join(map(str, lst_info))\n\n        # D+1 ì¢…ê°€ 2% ìƒìŠ¹ ì—¬ë¶€ \n        label = int(lst_stock[idx+1][6] >= 0.02)\n\n        # ì €ì¥ \n        OF.write(f'{code}\\t{date}\\t{info}\\t{label}\\n')\n                         \nOF.close()   \n\nprint(f'ìƒì„±ëœ ë°ì´í„°ì˜ ê°œìˆ˜ëŠ” {len(pd.read_csv(\"assignment3.txt\"))} ê°œ')\n\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 799/799 [01:07<00:00, 11.84it/s]\n\n\nìƒì„±ëœ ë°ì´í„°ì˜ ê°œìˆ˜ëŠ” 13939 ê°œ\n\n\n\n\n\në‚ ì§œ ì¤‘ì‹¬ì˜ forë¬¸ì„ ì½”ë“œ ì¤‘ì‹¬ìœ¼ë¡œ ë³€ê²½í•¨ìœ¼ë¡œì¨ fdr ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¶ˆëŸ¬ì˜¤ëŠ” ê²ƒì„ ìµœì†Œí™”í•˜ì—¬, ì•½ 17ë¶„ì´ ê±¸ë¦¬ë˜ ì‹œê°„ì´ 1ë¶„ìœ¼ë¡œ ëŒ€í­ ì¤„ì–´ë“  ê²ƒì„ í™•ì¸í•˜ì˜€ë‹¤.\n\n \n\n\n\n\nmulti processing(ë‹¤ì¤‘ ì²˜ë¦¬): ì»´í“¨í„° ì‹œìŠ¤í…œ í•œ ëŒ€ì— ë‘ê°œ ì´ìƒì˜ ì¤‘ì•™ ì²˜ë¦¬ ì¥ì¹˜ (CPU) ë¥¼ ì´ìš©í•˜ì—¬ ë³‘ë ¬ì²˜ë¦¬í•˜ëŠ” ê²ƒ\npython multi processing ë¬¸ì„œ: https://docs.python.org/ko/3/library/multiprocessing.html.\n\npythonì€ multiprocessing ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ í†µí•´ ë‹¤ì¤‘ ì²˜ë¦¬ë¥¼ ì§€ì›í•œë‹¤. ì—¬ëŸ¬ ê°œì˜ ì½”ì–´ë¥¼ ì—°ì‚°ì— ì‚¬ìš©í•¨ìœ¼ë¡œì¨ ë§ì€ ì‘ì—…ì„ ë¹ ë¥¸ ì‹œê°„ì— ì²˜ë¦¬í•´ì¤„ ìˆ˜ ìˆë‹¤ëŠ” ì¥ì ì´ ìˆë‹¤.\n [ core=10ìœ¼ë¡œ ì„¤ì •í•˜ì—¬ multi processingì„ ìˆ˜í–‰í•  ë•Œ ì½”ì–´ ì‚¬ìš© ]\n\n# MultiProcessingì„ ìœ„í•œ library import \nimport time, os\nfrom multiprocessing import Pool\n\nê³¼ì œ I\nê³¼ì œI ì€ ì•ì˜ ë°©ë²•ê³¼ ê°™ìœ¼ë¯€ë¡œ ìƒëµí•œë‹¤.\n\nprint('ìƒì¥ì¼ 2018-01-01 ì´ì „ ì¢…ëª©ì½”ë“œ: ', lst_code[:4])\nprint()\nprint(f'ì´ {len(df_code)} ê°œì˜ ì¢…ëª© ì¤‘ {len(lst_code)} ê°œì˜ ì¢…ëª© ì„ ë³„')\n\nìƒì¥ì¼ 2018-01-01 ì´ì „ ì¢…ëª©ì½”ë“œ:  ['000210', '004840', '155660', '078930']\n\nì´ 2507 ê°œì˜ ì¢…ëª© ì¤‘ 1977 ê°œì˜ ì¢…ëª© ì„ ë³„\n\n\nê³¼ì œ II\n\nmulti processingì„ ìœ„í•œ í•¨ìˆ˜ ì •ì˜\n\n\ndef make_lst_result(code): \n    start_date = '20180101'\n    end_date = '20201231'\n    \n    lst_date = []\n    \n    stock = fdr.DataReader(code, start = start_date, end = end_date)\n    stock.reset_index(inplace=True)\n  \n    # ğŸŒŸ dataframe -> list\n    lst_stock = stock.values.tolist()\n    \n    for row in lst_stock: \n        if row[4] * row[5] >= 100000000000: \n            lst_date.append(row[0].date().strftime(\"%Y%m%d\"))\n        \n    return [code, lst_date]\n\n\nmulti processing ìˆ˜í–‰\n\n\nstart_time = time.time()\nnum_cores = 10\npool = Pool(num_cores)\nlst_code_date = pool.map(make_lst_result, lst_code)\npool.close()\npool.join()\nprint(time.time() - start_time) \n\n17.97966194152832\n\n\nì•ì„œ ì•½ 2ë¶„ 40ì´ˆ ê°€ ê±¸ë ¸ë˜ ê³¼ì œII ë¥¼ multi processingì„ ì‚¬ìš©í•˜ì—¬ 17ì´ˆëŒ€ë¡œ ë‹¨ì¶•ì‹œì¼°ë‹¤.\n\ndictionary ìƒì„±\n\n\ndict_code2date = {}\n\nfor code, lst_date  in tqdm(lst_code_date):\n    if lst_date == []:\n        continue\n    dict_code2date[code] = lst_date\n            \nprint(f'ì´ {len(lst_code)} ê°œì˜ ì¢…ëª© ì¤‘ {len(dict_code2date)} ê°œì˜ ì¢…ëª© ì‚¬ìš©')\n\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1977/1977 [00:00<00:00, 1610124.08it/s]\n\n\nì´ 1977 ê°œì˜ ì¢…ëª© ì¤‘ 799 ê°œì˜ ì¢…ëª© ì‚¬ìš©\n\n\n\n\n\nê³¼ì œ III\n\nmulti processingì„ ìœ„í•œ í•¨ìˆ˜ ì •ì˜\n\n\ndef make_lst_result2(code): \n    # codeì˜ stock \n    start_date = '20180101' \n    end_date = '20201231' \n    stock = fdr.DataReader(code, start = start_date, end = end_date)\n    stock.reset_index(inplace=True)\n    \n    # ğŸŒŸ dataframe -> list     \n    lst_stock = stock.values.tolist()  \n       \n    lst_result = []\n        \n    for idx, row in enumerate(lst_stock): \n        \n        if (idx < 9) or (idx >= len(lst_stock)-1): # ì˜ˆì™¸ ì²˜ë¦¬ \n            continue \n            \n        date = row[0].date().strftime(\"%Y%m%d\") \n        if date not in dict_code2date[code]: # ì¡°ê±´ì— ë¶€í•©í•˜ëŠ” ë‚ ì§œ (D0 ë‚ ì§œ)ë¥¼ ë°œê²¬í•  ë•Œê¹Œì§€ continue\n            continue \n\n        # D-9 ~ D0 ë°ì´í„°ë§Œ ë‹´ê¸°\n        sub_stock = lst_stock[idx-9:idx+1] \n        \n        # 10ì¼ê°„ì˜ ë°ì´í„° \n        lst_info = []\n        for row2 in sub_stock:\n            lst_prices, trading_value = row2[1:5], row2[4]*row2[5]\n            lst_info += lst_prices + [trading_value]\n        info = ','.join(map(str, lst_info))\n\n        # D+1 ì¢…ê°€ 2% ìƒìŠ¹ ì—¬ë¶€ \n        label = int(lst_stock[idx+1][6] >= 0.02)\n        \n        lst_result.append([code, date, info, label])\n        \n    return lst_result\n\n\nmulti processing ìˆ˜í–‰\n\n\nstart_time = time.time()\nnum_cores = 10\npool = Pool(num_cores)\nlst_data = pool.map(make_lst_result2, dict_code2date.keys())\npool.close()\npool.join()\nprint(time.time() - start_time) \n\n8.186521768569946\n\n\nì•ì„œ ì§„í–‰í–ˆë˜ ë°©ë²•ì—ì„œ 1ë¶„ì´ ê±¸ë ¸ë˜ ì‘ì—…ì´ multi processingì„ ì‚¬ìš©í•˜ì—¬ 8ì´ˆë¡œ ì¤„ì–´ë“¤ì—ˆë‹¤.\n\ntxt íŒŒì¼ ìƒì„±\n\n\nOF = open(\"assignment3_multi_processing.txt\", 'w')\n\nfor row in lst_data: \n    for num in range(len(row)): \n        OF.write('\\t'.join(map(str, row[num])) + '\\n')\n        \nOF.close()\n\nprint(f'ìƒì„±ëœ ë°ì´í„°ì˜ ê°œìˆ˜ëŠ” {len(pd.read_csv(\"assignment3_multi_processing.txt\"))} ê°œ')\n\nìƒì„±ëœ ë°ì´í„°ì˜ ê°œìˆ˜ëŠ” 13939 ê°œ\n\n\n\n \n\n\n4ë²ˆì§¸ ë°©ë²•ì€ MySQLì„ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ë‹¤. ì™¸ë¶€ ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜¤ì§€ ì•Šì•„ë„ ë˜ê³ , ì„œë²„ DBì— ì €ì¥ëœ ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜¤ëŠ” ê²ƒì´ë¯€ë¡œ multi processingì„ ì‚¬ìš©í•˜ì§€ ì•Šê³ ë„ ë¹ ë¥¸ ì†ë„ë¡œ ë°ì´í„°ì…‹ì„ ìƒì„±í•  ìˆ˜ ìˆë‹¤.\nê³¼ì œ I\nìµœì¢…ì ìœ¼ë¡œ ë¨¸ì‹ ëŸ¬ë‹ ë¶„ì„ì— ì‚¬ìš©í•  ë°ì´í„°ì…‹ì€ ì½”ìŠ¤í”¼, ì½”ìŠ¤ë‹¥ ì‹œì¥ì— í•´ë‹¹í•˜ëŠ” ì¢…ëª©ë“¤ë§Œì„ ì‚¬ìš©í•œë‹¤. í˜„ì¬ dbì— ì €ì¥ë˜ì–´ ìˆëŠ” ë°ì´í„°ë„ ì½”ìŠ¤í”¼, ì½”ìŠ¤ë‹¥ ì‹œì¥ì— í•´ë‹¹í•˜ëŠ” ì¢…ëª©ë“¤ì´ ì…ë ¥ë˜ì–´ ìˆìœ¼ë©°, í•´ë‹¹ ì¢…ëª©ë“¤ì„ ì¶”ë¦° code_list.txtì—ì„œ ì¢…ëª©ë“¤ì„ ë¶ˆëŸ¬ì˜¨ lst_codeë¥¼ ì‚¬ìš©í•œë‹¤.\n\nIF = open('../data/code_list.txt')\nlst_code = IF.readlines()\n\nprint(f'ì´ {len(df_code)} ê°œì˜ ì¢…ëª© ì¤‘ {len(lst_code)} ê°œì˜ ì¢…ëª© ì„ ë³„')\n\nì´ 2536 ê°œì˜ ì¢…ëª© ì¤‘ 1561 ê°œì˜ ì¢…ëª© ì„ ë³„\n\n\n\n# pymysql ì„¤ì¹˜\n# ! pip install pymysql\n\nimport pymysql \nfrom sqlalchemy import create_engine\n\n\nMySQL ë°ì´í„° ì €ì¥ (dataframe -> sql) \n\ncode ë³„ë¡œ ë‹¤ë¥¸ í…Œì´ë¸”ì— ì €ì¥í•œë‹¤\n\ndb_connection_str = 'mysql+pymysql://[db username]:[db password]@[host address]/[db name]' \ndb_connection = create_engine(db_connection_str)\nconn = db_connection.connect()\n\nfor code in tqdm(lst_code): \n    start_date = '20170101'\n    end_date = '20211231'\n    stock = fdr.DataReader(code, start = start_date, end = end_date)\n    stock = stock.reset_index()\n    stock = stock[['Date', 'Open', 'High', 'Low', 'Close', 'Volume', 'Change']]\n    stock.to_sql(name=f'stock_{code}', con=db_connection, if_exists='fail', index=False)\n\n\nMySQL ì—ì„œ ë°ì´í„° ë¶ˆëŸ¬ì˜¨ í›„ ë°ì´í„°ì…‹ ìƒì„±\n\n\ndb_dsml = pymysql.connect(\n    host = 'localhost', \n    port = 3306, \n    user = '[db username]', \n    passwd = '[db password]', \n    db = '[db name]', \n    charset = 'utf8'\n)\ncursor = db_dsml.cursor()\n\nê³¼ì œ II\n\ndict_code2date = {}\nfor code in tqdm(lst_code): \n    code = code.strip()\n    sql_query = '''\n                SELECT *\n                FROM stock_{}\n                WHERE Date BETWEEN '2018-01-01' AND '2020-12-31'\n                '''.format(code)\n    stock = pd.read_sql(sql = sql_query, con = db_dsml)   \n    \n    # ğŸŒŸ dataframe -> list     \n    lst_stock = stock.values.tolist()\n    \n    for row in lst_stock: \n        date, trading_value = row[0], row[4]*row[5]\n        if trading_value >= 100000000000:\n            if code not in dict_code2date.keys():\n                dict_code2date[code] = [date.date().strftime(\"%Y%m%d\")]\n            else:\n                dict_code2date[code].append(date.date().strftime(\"%Y%m%d\"))\n\nprint(f'ì´ {len(lst_code)} ê°œì˜ ì¢…ëª© ì¤‘ {len(dict_code2date)} ê°œì˜ ì¢…ëª© ì‚¬ìš©')\n\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1561/1561 [00:18<00:00, 82.67it/s]\n\n\nì´ 1561 ê°œì˜ ì¢…ëª© ì¤‘ 679 ê°œì˜ ì¢…ëª© ì‚¬ìš©\n\n\n\n\n\nê³¼ì œ III\n\nOF = open('assignment3_sql.txt', 'w')\nfor code in tqdm(dict_code2date): \n    code = code.strip()\n    sql_query = '''\n                SELECT *\n                FROM stock_{}\n                WHERE Date BETWEEN '2018-01-01' AND '2020-12-31'\n                '''.format(code)\n    stock = pd.read_sql(sql = sql_query, con = db_dsml)  \n    \n    # ğŸŒŸ dataframe -> list     \n    lst_stock = stock.values.tolist()  \n       \n    for idx, row in enumerate(lst_stock):   \n        if (idx < 9) or (idx >= len(lst_stock)-1): # ì˜ˆì™¸ ì²˜ë¦¬ \n            continue \n        \n        date = row[0].date().strftime(\"%Y%m%d\") \n        if date not in dict_code2date[code]: # ì¡°ê±´ì— ë¶€í•©í•˜ëŠ” ë‚ ì§œ (D0 ë‚ ì§œ)ë¥¼ ë°œê²¬í•  ë•Œê¹Œì§€ continue\n            continue \n\n        # D-9 ~ D0 ë°ì´í„°ë§Œ ë‹´ê¸°\n        sub_stock = lst_stock[idx-9:idx+1] \n        \n        # 10ì¼ê°„ì˜ ë°ì´í„° \n        lst_info = []\n        for row2 in sub_stock:\n            lst_prices, trading_value = row2[1:5], row2[4]*row2[5]\n            lst_info += lst_prices + [trading_value]\n        info = ','.join(map(str, lst_info))\n\n        # D+1 ì¢…ê°€ 2% ìƒìŠ¹ ì—¬ë¶€ \n        label = int(lst_stock[idx+1][6] >= 0.02)\n\n        # ì €ì¥ \n        OF.write(f'{code}\\t{date}\\t{info}\\t{label}\\n')\n                         \nOF.close()   \n\nprint(f'ìƒì„±ëœ ë°ì´í„°ì˜ ê°œìˆ˜ëŠ” {len(pd.read_csv(\"assignment3_sql.txt\"))} ê°œ')\n\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 679/679 [00:09<00:00, 72.67it/s]\n\n\nìƒì„±ëœ ë°ì´í„°ì˜ ê°œìˆ˜ëŠ” 11934 ê°œ\n\n\nMySQLì—ì„œ ì½”ë“œë³„ë¡œ ì €ì¥ë˜ì–´ìˆëŠ” ë°ì´í„°ì…‹ì„ ë¶ˆëŸ¬ì™€ 9ì´ˆì˜ ì‹œê°„ì´ ê±¸ë ¸ë‹¤. multiprocessingìœ¼ë¡œ ì½”ì–´ë¥¼ ì—¬ëŸ¬ê°œ ì“´ ê²ƒê³¼ ë¹„ìŠ·í•œ ê²°ê³¼ê°€ ë‚˜ì˜¨ ê²ƒì„ í™•ì¸í•˜ì˜€ë‹¤.\n\n \n\n\n\nìµœì¢…ì ìœ¼ë¡œ ìƒì„±ëœ ë¨¸ì‹ ëŸ¬ë‹ ë°ì´í„°ì…‹ì˜ í˜•íƒœë¥¼ í™•ì¸í•œë‹¤.\n\nIF=open(\"assignment3_sql.txt\",'r')\nlst_code_date=[]\ntrainX=[]\ntrainY=[]\nfor line in IF:\n    code, date, x, y = line.strip().split(\"\\t\")\n    lst_code_date.append([code, date])\n    trainX.append(list(map(int, x.split(\",\"))))\n    trainY.append(int(y))\ntrainX=pd.DataFrame(trainX)\ntrainY=pd.DataFrame(trainY)\n\n\nprint(\"===== trainX =====\")\nprint(\"trainX shape:\", trainX.shape)\ndisplay(trainX.head())\nprint()\nprint(\"===== trainY =====\")\nprint(\"trainY shape:\", trainY.shape)\ndisplay(trainY.head())\n\n===== trainX =====\ntrainX shape: (11935, 50)\n\n\n\n\n\n\n  \n    \n      \n      0\n      1\n      2\n      3\n      4\n      5\n      6\n      7\n      8\n      9\n      ...\n      40\n      41\n      42\n      43\n      44\n      45\n      46\n      47\n      48\n      49\n    \n  \n  \n    \n      0\n      10250\n      12050\n      10150\n      11800\n      307823874200\n      11950\n      12450\n      10900\n      11750\n      240410569500\n      ...\n      15300\n      15400\n      12650\n      13700\n      789063638200\n      13700\n      16100\n      13400\n      15400\n      897154258000\n    \n    \n      1\n      11950\n      12450\n      10900\n      11750\n      240410569500\n      11850\n      14150\n      11600\n      12600\n      764364560400\n      ...\n      13700\n      16100\n      13400\n      15400\n      897154258000\n      14700\n      15500\n      14000\n      14350\n      277027065700\n    \n    \n      2\n      11850\n      14150\n      11600\n      12600\n      764364560400\n      12800\n      13200\n      12000\n      12200\n      170010147600\n      ...\n      14700\n      15500\n      14000\n      14350\n      277027065700\n      13050\n      13300\n      11650\n      11650\n      231873876050\n    \n    \n      3\n      12800\n      13200\n      12000\n      12200\n      170010147600\n      12450\n      13400\n      12350\n      12850\n      211661434950\n      ...\n      13050\n      13300\n      11650\n      11650\n      231873876050\n      12200\n      13150\n      11600\n      12200\n      222393934200\n    \n    \n      4\n      12450\n      13400\n      12350\n      12850\n      211661434950\n      12800\n      12950\n      11300\n      11700\n      91801277100\n      ...\n      12200\n      13150\n      11600\n      12200\n      222393934200\n      12200\n      13750\n      12100\n      12350\n      256196958550\n    \n  \n\n5 rows Ã— 50 columns\n\n\n\n\n===== trainY =====\ntrainY shape: (11935, 1)\n\n\n\n\n\n\n  \n    \n      \n      0\n    \n  \n  \n    \n      0\n      0\n    \n    \n      1\n      0\n    \n    \n      2\n      1\n    \n    \n      3\n      0\n    \n    \n      4\n      1\n    \n  \n\n\n\n\n\n \n4ë‹¨ê³„ì˜ ê³¼ì •ì„ ê±°ì³ ë¨¸ì‹ ëŸ¬ë‹ ë°ì´í„°ì…‹ ìƒì„±ì„ ë§ˆì³¤ë‹¤. ë‹¤ì–‘í•œ ë°©ë²•ì„ í†µí•´ ì „ì²˜ë¦¬ ì½”ë“œë¥¼ ê°œì„ ì‹œí‚¤ëŠ” ê³¼ì •ì„ ë°°ìš¸ ìˆ˜ ìˆì—ˆê³ , ì‹œê°„ íš¨ìœ¨ì„±ì„ ë†’ì´ëŠ” ê²ƒì€ ì¤‘ìš”í•œ ì‘ì—…ì„ì„ ê¹¨ë‹¬ì•˜ë‹¤. 17ë¶„ì´ ê±¸ë¦¬ë˜ ì‘ì—…ì„ 8~9ì´ˆë¡œ ì¤„ì˜€ìœ¼ë©°, ë°ì´í„°ê°€ ë§ì•„ì§ˆ ìˆ˜ë¡ ì‹œê°„ì´ ë” ëŠ˜ì–´ë‚˜ê¸° ë•Œë¬¸ì— ì‹œê°„ íš¨ìœ¨ì„±ì˜ ì¤‘ìš”ì„±ì´ ì»¤ì§ˆ ê²ƒì´ë‹¤. ë‹¤ìŒ ê¸€ì—ì„œëŠ” ìƒì„±ëœ ë¨¸ì‹ ëŸ¬ë‹ ë°ì´í„°ì…‹ì„ ì‚¬ìš©í•˜ì—¬ ì—¬ëŸ¬ ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ì„ í•™ìŠµ ë° í‰ê°€í•˜ì—¬ ì„±ëŠ¥ì´ ê°€ì¥ ì¢‹ì€ ëª¨ë¸ì„ ì„ ì •í•˜ëŠ” baseline model selectionì„ ì§„í–‰í•˜ê² ë‹¤."
  },
  {
    "objectID": "posts/02.model_selection.html",
    "href": "posts/02.model_selection.html",
    "title": "[stock prediction] 1.2 ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ ë¹„êµ",
    "section": "",
    "text": "ì´ì „ ê¸€ (1.1. ë¨¸ì‹ ëŸ¬ë‹ì„ ìœ„í•œ ì£¼ê°€ ë°ì´í„°ì…‹ ìƒì„±) ì—ì„œëŠ” ë¨¸ì‹ ëŸ¬ë‹ì„ ìœ„í•œ ì‹œê³„ì—´ êµ¬ì¡°ì˜ ë°ì´í„°ì…‹ì„ ìƒì„±í–ˆë‹¤. ì´ë²ˆ ê¸€ì—ì„œëŠ” ìƒì„±í•œ ë°ì´í„°ì…‹ì„ ì‚¬ìš©í•˜ì—¬ ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ì„ í•™ìŠµí•˜ê³ , í‰ê°€ì§€í‘œë¥¼ í†µí•´ ì—¬ëŸ¬ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ ë¹„êµí•˜ì—¬ ê°€ì¥ ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì´ëŠ” ëª¨ë¸ì„ ì„ íƒí•œë‹¤. ì´ë¥¼ í†µí•´ ìƒì„±í•œ ê¸°ë³¸ ì‹œê³„ì—´ ë°ì´í„°ì…‹ì„ ë¨¸ì‹ ëŸ¬ë‹ ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œ í•™ìŠµì‹œì¼°ì„ ë•Œ, ìµœì†Œí•œì˜ ì„±ëŠ¥ê³¼ ìˆ˜ìµë¥ ì´ ë°œìƒí•˜ëŠ”ì§€ í™•ì¸í•  ìˆ˜ ìˆê³ , ì´ë¥¼ í†µí•´ baseline modelì„ ì •ì˜í•œë‹¤.\n\n\n\n\n\në°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°\n\n\nì£¼ê°€ ì˜ˆì¸¡ ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ í•™ìŠµ\n\n\n\ní‰ê°€ì§€í‘œ ì‹œê°í™”ë¥¼ í†µí•œ ëª¨ë¸ í‰ê°€\n\n\nëª¨ë¸ ì„ íƒ\n\n\n\n\n\n\n\nimport pandas as pd\nimport numpy as np\nfrom tqdm import tqdm\nimport FinanceDataReader as fdr\nimport pymysql\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n \n\n\n\n\n\n\nì§€ë‚œ ê¸€ì—ì„œ ìƒì„±í•œ ë°ì´í„°ì…‹ì„ ë¶ˆëŸ¬ì˜¨ë‹¤. trainì€ 2018.01.02 ~ 2020.12.31, testëŠ” 2021.01.02 ~ 2021.06.31 ì„¤ì •í•œ ë°ì´í„°ì…‹ì´ë‹¤.\n\ntrain dataset\n\n\n#collapse-hide\nIF=open(\"assignment3_sql.txt\",'r')\nlst_code_date=[]\ntrainX=[]\ntrainY=[]\nfor line in IF:\n    code, date, x, y = line.strip().split(\"\\t\")\n    lst_code_date.append([code, date])\n    trainX.append(list(map(int, x.split(\",\"))))\n    trainY.append(int(y))\ntrainX=pd.DataFrame(trainX)\ntrainY=pd.DataFrame(trainY)\n\n\ntest dataset\n\n\n#collapse-hide\nIF=open(\"assignment3_sql_test.txt\",'r')\nlst_code_date_test=[]\ntestX=[]\ntestY=[]\nfor line in IF:\n    code, date, x, y = line.strip().split(\"\\t\")\n    lst_code_date_test.append([code, date])\n    testX.append(list(map(int, x.split(\",\"))))\n    testY.append(int(y))\ntestX=pd.DataFrame(testX)\ntestY=pd.DataFrame(testY)\n\n\nshape í™•ì¸\n\n\nprint(\"train dataset: \", trainX.shape, trainY.shape)\nprint(\"test dataset: \", testX.shape, testY.shape)\n\ntrain dataset:  (11935, 50) (11935, 1)\ntest dataset:  (4431, 50) (4431, 1)\n\n\n \n\n\n\nì•ì„œ ë¶ˆëŸ¬ì˜¨ ì£¼ê°€ ë°ì´í„°ì…‹ìœ¼ë¡œ ì—¬ëŸ¬ ê°œì˜ ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ì„ í•™ìŠµí•œë‹¤. ëª¨ë¸ì€ ì´ 9ê°œì˜ ë¶„ë¥˜ ì•Œê³ ë¦¬ì¦˜ì„ ì‚¬ìš©í•˜ì—¬ ë¹„êµí•œë‹¤.\n\nmodel list - Logistic Regression - Decision tree - Support vector machine - Gaussian naive bayes - K nearest neighbor - Random forest - Gradient boosing - Neural network - XGBoost\n\n\nëª¨ë¸í•™ìŠµ\n\n\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import roc_auc_score\nimport time\n\nresults=[]\n\n##### 1. Logistic regression    \nfrom sklearn.linear_model import LogisticRegression\nlr = LogisticRegression(max_iter=1000)\nlr.fit(trainX, trainY)\n\n##### 2. Decision tree\nfrom sklearn.tree import DecisionTreeClassifier\ndt = DecisionTreeClassifier()\ndt.fit(trainX, trainY)\n\n##### 3. Support vector machine\nfrom sklearn.svm import SVC\nsvc = SVC(probability=True)\nsvc.fit(trainX, trainY)\n\n##### 4. Gaussian naive bayes\nfrom sklearn.naive_bayes import GaussianNB\ngnb = GaussianNB()\ngnb.fit(trainX, trainY)\n\n##### 5. K nearest neighbor\nfrom sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier()\nknn.fit(trainX, trainY)\n\n##### 6. Random forest\nfrom sklearn.ensemble import RandomForestClassifier\nrf = RandomForestClassifier()\nrf.fit(trainX, trainY)\n\n##### 7. Gradient boosing\nfrom sklearn.ensemble import GradientBoostingClassifier\ngb = GradientBoostingClassifier()\ngb.fit(trainX, trainY)\n\n##### 8. Neural network\nfrom sklearn.neural_network import MLPClassifier\nmlp = MLPClassifier(max_iter=1000)\nmlp.fit(trainX, trainY)\n\n##### XGBoost \nfrom xgboost import XGBClassifier\nxgb = XGBClassifier(\n                   n_jobs=40,\n                   scale_pos_weight=4,\n                   learning_rate=0.01,\n                   max_depth=3,\n                   n_estimators=500,\n                   ) \n\nxgb.fit(trainX, trainY)\n\n[00:14:01] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n\n\nXGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n              gamma=0, gpu_id=-1, importance_type=None,\n              interaction_constraints='', learning_rate=0.01, max_delta_step=0,\n              max_depth=3, min_child_weight=1, missing=nan,\n              monotone_constraints='()', n_estimators=500, n_jobs=40,\n              num_parallel_tree=1, predictor='auto', random_state=0,\n              reg_alpha=0, reg_lambda=1, scale_pos_weight=4, subsample=1,\n              tree_method='exact', validate_parameters=1, verbosity=None)\n\n\n\nì˜ˆì¸¡\n\ní•™ìŠµì‹œí‚¨ ëª¨ë¸ ë³„ë¡œ Accuracyì™€ AUC scoreë¥¼ ì¸¡ì •í•˜ì—¬, lst_result_acc, lst_result_roc ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€í•œë‹¤.\n\nlst_model = [lr, dt, svc, gnb, knn, rf, gb, mlp, xgb]\n\ndic_model2name = {lr:'LogisticRegression', dt:'DecisionTree', svc:'SVM', \n                  gnb:'GaussianNaiveBayes', knn:'KNN', rf:'RandomForest', \n                  gb:'GradientBoosting', mlp:'NeuralNetwork', xgb:'XGBoost'}\n\nlst_result_acc = [] # [['ëª¨ë¸ëª…', train í‰ê°€ì§€í‘œ, test í‰ê°€ì§€í‘œ], ...]\nlst_result_roc = []\n\nfor model in lst_model: \n    # accuracy \n    predY_train = model.predict(trainX)\n    predY_test = model.predict(testX)\n    \n    accuracy_train = accuracy_score(trainY, predY_train)\n    accuracy_test = accuracy_score(testY, predY_test)\n\n    # auc score \n    probY_train = model.predict_proba(trainX)[:, 1]\n    probY_test = model.predict_proba(testX)[:, 1]\n    \n    roc_score_train = roc_auc_score(trainY, probY_train)\n    roc_score_test = roc_auc_score(testY, probY_test)\n    \n    # ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€ \n    lst_result_acc.append([dic_model2name[model], accuracy_train, accuracy_test])\n    lst_result_roc.append([dic_model2name[model], roc_score_train, roc_score_test])\n    \nprint('accuracy list: ', lst_result_acc[:2])\nprint()\nprint('auc score list: ', lst_result_roc[:2])\n\naccuracy list:  [['LogisticRegression', 0.7622957687473817, 0.7734145791017829], ['DecisionTree', 1.0, 0.598059128864816]]\n\nauc score list:  [['LogisticRegression', 0.5318105913341066, 0.5430800870053489], ['DecisionTree', 1.0, 0.4824088530616373]]\n\n\n \n\n\n\nìœ„ì—ì„œ ì–»ì€ 9ê°€ì§€ ëª¨ë¸ì˜ í‰ê°€ ì§€í‘œ(accuracy, auc score) ê²°ê³¼ ë¦¬ìŠ¤íŠ¸ë¥¼ ë°›ì•„ ëª¨ë¸ ë³„ë¡œ í‰ê°€ì§€í‘œë¥¼ ë¹„êµí•˜ëŠ” bar graph ì‹œê°í™” í•¨ìˆ˜ë¥¼ ì‘ì„±í•œë‹¤.\ntest ë°ì´í„°ì…‹ì˜ ì„±ëŠ¥ì´ ë†’ì€ ìˆœì„œëŒ€ë¡œ ì •ë ¬í•˜ì—¬ ì‹œê°í™”í•œë‹¤.\n\ndef result_plot(lst:list, index:str): \n    '''\n    lst: [['ëª¨ë¸ëª…', 'train í‰ê°€ì§€í‘œ', 'test í‰ê°€ì§€í‘œ'], ...]\n    index: 'acc' or 'auc'\n    '''\n    import seaborn as sns\n    import matplotlib.pyplot as plt\n    \n    if index == 'acc': \n        plt.figure(figsize=(12, 5))  \n        plt.title(\"ACCURACY\", fontsize=20)\n        \n        df = pd.DataFrame(data=lst_result_acc, \n                     columns=['name', 'train', 'test']).set_index('name')\n        \n        df_results = df.reset_index().melt(id_vars='name')\n      \n    elif index == 'auc': \n        plt.figure(figsize=(12, 5))  \n        plt.title(\"ROCAUC\", fontsize=20)\n        df = pd.DataFrame(data=lst_result_roc, \n                     columns=['name', 'train', 'test']).set_index('name')\n        df_results = df.reset_index().melt(id_vars='name')\n        \n    else:\n        print(\"í‰ê°€ì§€í‘œë¥¼ ë‹¤ì‹œ ì…ë ¥í•´ì£¼ì„¸ìš”.\")\n        \n      \n    ax = sns.barplot(x=\"name\", y=\"value\", hue='variable', data=df_results, capsize=.2, \n                     order=df['test'].sort_values(ascending=False).index)\n    ax.set(ylim=(0.00, 1.1))\n    ax.set_xticklabels(ax.get_xticklabels(),rotation=60) \n    \n    for p in ax.patches:\n        height = p.get_height()\n        ax.text(p.get_x() + p.get_width() / 2., height + 0.01, round(height, 2), ha = 'center', size = 10)\n\n    plt.show()\n\nAccuracy\n\nresult_plot(lst_result_acc, 'acc')\n\n\n\n\nAUROC\n\nresult_plot(lst_result_acc, 'auc')\n\n\n\n\nAccuracyëŠ” Logistic Regression, ROCAUCëŠ” XGBoostê°€ ê°€ì¥ ë†’ì€ ê°’ì„ ì–»ì€ ê²ƒì„ í™•ì¸í•˜ì˜€ë‹¤.\n \n\n\n\n\n1) ì„ íƒ ê¸°ì¤€ í‰ê°€ì§€í‘œ ì„ íƒ - ì¢…ì†ë³€ìˆ˜ ë¶ˆê· í˜• ë¬¸ì œ\n\nëª¨ë¸ ì„ íƒì„ ìœ„í•´ ì‚¬ìš©í•  í‰ê°€ì§€í‘œë¥¼ ì„ íƒí•œë‹¤.\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfig = plt.figure(figsize=(10, 3))\nax1, ax2 = fig.subplots(1, 2).flatten()\n\n# countplot ì‹œê°í™”\nsns.countplot(data=trainY, x=0, ax=ax1)\nsns.countplot(data=testY, x=0, ax=ax2)\n\n# title ì„¤ì •\nax1.set_title(\"train\", fontsize=17)\nax2.set_title(\"test\", fontsize=17)\n\n# ylim ì„¤ì •\nax1.set_ylim(0, 10500)\nax2.set_ylim(0, 3900)\n\n# text ì¶”ê°€\nfor p in ax1.patches:\n    height = p.get_height()\n    ax1.text(p.get_x() + p.get_width() / 2., height + 200, height, ha = 'center', size = 12)\n    \nfor p in ax2.patches:\n    height = p.get_height()\n    ax2.text(p.get_x() + p.get_width() / 2., height + 80, height, ha = 'center', size = 12)\n\n# class 0, 1 ë¹„ìœ¨ ê³„ì‚°\nprint(\"===============train===============\")\nprint(\"0ì˜ ë¹„ìœ¨: \", sum(trainY[0] == 0) / (sum(trainY[0]==0) + sum(trainY[0]==1)))\nprint(\"1ì˜ ë¹„ìœ¨: \", sum(trainY[0] == 1) / (sum(trainY[0]==0) + sum(trainY[0]==1)))\nprint()\nprint(\"===============test================\")\nprint(\"0ì˜ ë¹„ìœ¨: \", sum(testY[0] == 0) / (sum(testY[0]==0) + sum(testY[0]==1)))\nprint(\"1ì˜ ë¹„ìœ¨: \", sum(testY[0] == 1) / (sum(testY[0]==0) + sum(testY[0]==1)))\nprint()\n\n===============train===============\n0ì˜ ë¹„ìœ¨:  0.7622957687473817\n1ì˜ ë¹„ìœ¨:  0.23770423125261836\n\n===============test================\n0ì˜ ë¹„ìœ¨:  0.7734145791017829\n1ì˜ ë¹„ìœ¨:  0.2265854208982171\n\n\n\n\n\n\nì´ì§„ë¶„ë¥˜ ëœ ì¢…ì†ë³€ìˆ˜ë¥¼ countplotìœ¼ë¡œ ì‹œê°í™”í•˜ì—¬ ë¶„í¬ë¥¼ ì‚´í´ë³´ì•˜ì„ ë•Œ, ì¢…ì†ë³€ìˆ˜ê°€ 0ì¸ ë°ì´í„°ì˜ ë¹„ìœ¨ì´ ë†’ì€ ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ëŠ” ëª¨ë¸ì´ ëª¨ë‘ 0ì´ë¼ê³  ì˜ˆì¸¡í•´ë„ ì •í™•ë„ê°€ 76~77%ê°€ ë‚˜ì˜¨ë‹¤ëŠ” ëœ»ì´ë‹¤. ë”°ë¼ì„œ ì •í™•ë„ëŠ” í•´ë‹¹ ì£¼ê°€ ë°ì´í„°ë¥¼ í‰ê°€í•˜ê¸° ìœ„í•œ ì§€í‘œë¡œëŠ” ì‹ ë¢°ì„±ì´ ë–¨ì–´ì§„ë‹¤ê³  íŒë‹¨ë˜ì–´, thresholdë³„ scoreê°€ ê²°ì •ë˜ëŠ” ROCAUC scoreë¥¼ ê¸°ì¤€ìœ¼ë¡œ ëª¨ë¸ì„ ì„ íƒí•œë‹¤.\n\n\n2) ì˜ˆì¸¡ ì‹ ë¢°ì„±\n\ntest datasetì˜ AUCê°€ ë†’ì€ ìƒìœ„ 2ê°œ ëª¨ë¸(XGBoost, GradientBoosting)ì˜ ì˜ˆì¸¡ í™•ë¥ ì„ ì‚´í´ë³¸ë‹¤.\n\nxgb_prob = xgb.predict_proba(testX)[:, 1] \ngb_prob = gb.predict_proba(testX)[:, 1]\n\nprint('XGB:', sum(xgb_prob >= 0.6), 'ê°œ')\nprint('GB:', sum(gb_prob >= 0.6), 'ê°œ')\n\nXGB: 1268 ê°œ\nGB: 1 ê°œ\n\n\nclassë¥¼ 1ë¡œ ì˜ˆì¸¡í•  í™•ë¥ ì´ 60% ì´ìƒì¸ ë°ì´í„°ì˜ ê°œìˆ˜ë¥¼ í™•ì¸í•´ë³´ë©´, XGBoostëŠ” 1268ê°œ, GradientBoostingì€ 1ê°œê°€ ë‚˜ì˜¨ë‹¤.\n\n1), 2) ë¥¼ ê·¼ê±°ë¡œ í•˜ì—¬ ì—¬ëŸ¬ ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ ì¤‘ AUC ì ìˆ˜ 1ìœ„ì¸ ë™ì‹œì—, ì˜ˆì¸¡ ì‹ ë¢°ì„±ì´ ë” ë†’ë‹¤ê³  íŒë‹¨ë˜ëŠ” XGBoost ë¥¼ ìµœì¢…ì ìœ¼ë¡œ baseline ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ë¡œ ê²°ì •í•©ë‹ˆë‹¤.\n \n\n\n\nbaseline ëª¨ë¸ ì •ë¦¬\n\në°ì´í„°\n\nì¢…ëª©: kospi + kosdaq ì£¼ì‹ ì‹œì¥ 3ë…„ ì´ìƒ ì¡´ì†í•˜ì˜€ë˜ ì¢…ëª©ë“¤\nê¸°ê°„: 2018.01.01-2020.12.31 (ì´ 3ë…„)\ní•„í„°ë§: ê±°ë˜ëŒ€ê¸ˆ 1000ì–µ ì´ìƒ\nêµ¬ì¡°: 10ì¼ ê°„ì˜ ë°ì´í„°ë“¤ì„ featureë¡œ ë‘ì–´ ì‹œê³„ì—´ ë°ì´í„°ì…‹ ìƒì„±\n\nëª¨ë¸\nXGBClassifier(\nn_jobs=40,\nscale_pos_weight=4,\nlearning_rate=0.01,\nmax_depth=3,\nn_estimators=500,\n)\n\nìˆ˜ìµë¥  ì¸¡ì •\nbaselineëª¨ë¸ì„ ì„ ì •í•˜ì˜€ìœ¼ë¯€ë¡œ í”„ë¡œì íŠ¸ì—ì„œ ì¤‘ìš”í•œ ë¶€ë¶„ì¸ ìˆ˜ìµë¥  ì¸¡ì •ì„ ìˆ˜í–‰í•œë‹¤.\n\nprint('XGB:', sum(xgb_prob >= 0.65), 'ê°œ')\n\nXGB: 34 ê°œ\n\n\ní•´ë‹¹ ìˆ˜ìµë¥  ê³„ì‚° ì‹œë®¬ë ˆì´ì…˜ì—ì„œëŠ” prob ì„ê³„ê°’ì„ 0.65ë¡œ ì„¤ì •í•¨ìœ¼ë¡œì¨ 6ê°œì›” ë™ì•ˆ 1~2ì£¼ì— í•œë²ˆ ê¼´ë¡œ ë§¤ë§¤ê°€ ì´ë£¨ì–´ì§€ë„ë¡ í•˜ì˜€ë‹¤.\n\ndef compute_earnings_rate(lst_code_date, probY):\n    ##### DB \n    db_dsml = pymysql.connect(\n        host = 'localhost', \n        port = 3306, \n        user = '[db username]', \n        passwd = '[db password]', \n        db = '[db name]', \n        charset = 'utf8'\n    )\n    \n    cursor = db_dsml.cursor()\n    \n    ##### ì£¼ë¬¸ ì¼ì§€ ì‘ì„± #####\n    lst_output=[]\n    for (code, date), y in zip(lst_code_date, probY):\n        if y >= 0.65: # ì˜ˆì¸¡ í™•ë¥  (probY) ì„ê³„ê°’ 0.65 \n            lst_output.append([code, date, \"buy\", \"all\"])  \n            lst_output.append([code, date+\"n\", \"sell\", \"all\"])  \n            \n    lst_output.sort(key=lambda x:x[1]) # ë‚ ì§œë¡œ ì •ë ¬ \n    \n    \n    ##### ìˆ˜ìµë¥  ê³„ì‚° #####\n    start_money = 10000000 # ì´ˆê¸° í˜„ê¸ˆ 1ì²œë§Œì›\n    money = start_money\n    dic_code2num ={}  # ë³´ìœ  ì¢…ëª©\n    \n    for i, row in enumerate(tqdm(lst_output)): # ì£¼ë¬¸ ì¼ì§€ë¥¼ í•œ ì¤„ ì½ì–´ ì˜´\n        code, date, request, amount = row\n\n        sql_query = '''\n                    SELECT *\n                    FROM stock_{}\n                    WHERE Date BETWEEN '2021-01-01' AND '2021-06-31'\n                    '''.format(code)\n        stock = pd.read_sql(sql = sql_query, con = db_dsml)\n        lst_data = stock.values.tolist()    \n\n        \n        for idx, row in enumerate(lst_data):\n            data_date = row[0].date().strftime('%Y%m%d')\n\n            if 'n' in date: # ë§¤ë„ ë‚ ì§œ\n                date_n = date[:-1]\n                if data_date == date_n: \n                    close = lst_data[idx+1][-3] # ë§¤ë„ í•  ì¢…ê°€ (ë§¤ìˆ˜ ë‹¤ìŒë‚  ì¢…ê°€)\n                    break            \n\n            else: # ë§¤ìˆ˜ ë‚ ì§œ \n                if data_date == date:\n                    close = row[-3] # ë§¤ìˆ˜ í•  ì¢…ê°€ \n                    break\n        \n        if request == 'buy': # ë§¤ìˆ˜ \n            if amount.startswith('r'): # ë¶„í•  ë§¤ìˆ˜ ì‹œ ì‚¬ìš© \n                request_money = money * float(amount.lstrip(\"r\")) / 100\n            elif amount == 'all':\n                request_money = money\n            elif amount.isdigit():\n                request_money = int(amount)\n            else:\n                raise Exception('Not permitted option')\n                \n            request_money = min(request_money, money)\n            \n            buy_num = int(request_money / close) # ë§¤ìˆ˜ ê°œìˆ˜ \n            \n            money -= buy_num * close  # ë§¤ìˆ˜ í›„ ì”ê³  ì—…ë°ì´íŠ¸ \n            \n            if code not in dic_code2num: # í˜„ì¬ ë³´ìœ ì¢…ëª©ì´ ì•„ë‹ˆë¼ë©´ \n                dic_code2num[code] = 0 # dictì— key ìƒì„± \n                \n            dic_code2num[code] += buy_num # í•´ë‹¹ ì¢…ëª©ì˜ ë³´ìœ  ê°œìˆ˜ ì—…ë°ì´íŠ¸\n            \n        if request == 'sell': # ë§¤ë„ \n            if amount == 'all':   \n                sell_num = dic_code2num[code] # í•´ë‹¹ ì¢…ëª©ì˜ ë³´ìœ í•˜ê³  ìˆëŠ” ëª¨ë“  ê°œìˆ˜ ë§¤ë„\n            else:\n                raise Exception('Not permitted option')       \n                \n            money += sell_num * close # ë§¤ë„ í›„ ì”ê³  ì—…ë°ì´íŠ¸ \n            \n            dic_code2num[code] -= sell_num # í•´ë‹¹ ì¢…ëª© ë§¤ë„ í›„ ë³´ìœ  ê°œìˆ˜ ì—…ë°ì´íŠ¸ \n            if dic_code2num[code] == 0: # í•´ë‹¹ ì¢…ëª©ì´ ë³´ìœ í•˜ê³  ìˆëŠ” ê°œìˆ˜ê°€ ì—†ë‹¤ë©´ \n                del dic_code2num[code] # ë³´ìœ ì¢…ëª©ì—ì„œ ì‚­ì œ \n\n    if dic_code2num != {}: # ì£¼ë¬¸ì¼ì§€ë¥¼ ëª¨ë‘ ëŒê³ ë‚œ í›„ì—ëŠ” ë³´ìœ ì¢…ëª© dictê°€ ë¹„ì–´ìˆì–´ì•¼ í•¨. \n        raise Exception('Not empty stock') \n\n    print(\"Final earning rate : {} %\".format(str((money-start_money) / start_money * 100)))\n#     return str((money-start_money) / start_money * 100)\n\n\ncompute_earnings_rate(lst_code_date_test, xgb_prob)\n\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 68/68 [00:00<00:00, 265.33it/s]\n\n\nFinal earning rate : 54.73516000000001 %\n\n\n\n\n\nìµœì¢…ì ìœ¼ë¡œ baseline ëª¨ë¸ì˜ ìˆ˜ìµë¥ ì„ ê³„ì‚°í•´ë´¤ì„ ë•Œ, 6ê°œì›”ê°„ì˜ ì´ ìˆ˜ìµë¥ ì€ ì•½ 54%ê°€ ë‚˜ì™”ë‹¤. í‰ê°€ì§€í‘œì¸ ROCAUC scoreëŠ” ë‚®ì€ í¸ì— ì†í–ˆì§€ë§Œ, ë†’ì€ ìˆ˜ìµë¥ ì„ ë³´ì˜€ë‹¤.\në‹¤ìŒ ê¸€ ë¶€í„°ëŠ” data preprocessingì„ ì§„í–‰í•˜ë©°, ì£¼ê°€ ë¹…ë°ì´í„°ì˜ ì§ˆì„ ë†’ì—¬ baseline model ë³´ë‹¤ ë›°ì–´ë‚œ ì„±ëŠ¥ì„ ë³´ì´ë„ë¡ ê°œì„ í•œë‹¤."
  },
  {
    "objectID": "posts/03.add_index.html",
    "href": "posts/03.add_index.html",
    "title": "[stock prediction] 2.1 ì£¼ê°€ ë°ì´í„°ì…‹ ë³´ì¡°ì§€í‘œ ì¶”ê°€",
    "section": "",
    "text": "ì´ì „ ê¸€ [Stock Research] 1.2. ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ ë¹„êµê¹Œì§€ ì£¼ê°€ ë°ì´í„°ì…‹ ìƒì„±ê³¼ ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ ë¹„êµë¥¼ í†µí•´ baseline modelì„ ìƒì„±í–ˆë‹¤. ì´ë²ˆ ê¸€ ë¶€í„°ëŠ” ë°ì´í„° ì „ì²˜ë¦¬ë¥¼ ì§„í–‰í•œë‹¤. ëª¨ë¸ì„ ì´ì „ ê¸€ì—ì„œ ì„ íƒí–ˆë˜ baseline ëª¨ë¸ë¡œ fixí•˜ê³ , ë°ì´í„°ì˜ ì§ˆì„ ë†’ì„ìœ¼ë¡œì¨ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¨ë‹¤. ê·¸ ì²«ë²ˆì§¸ë¡œ ì£¼ê°€ë°ì´í„°ì…‹ì— ë³´ì¡°ì§€í‘œë¥¼ ì¶”ê°€í•˜ì—¬ ì„¤ëª…ë³€ìˆ˜ì˜ í¬ê¸°ë¥¼ ëŠ˜ë¦°ë‹¤.\n\n\n\n\nì£¼ì‹ ë°ì´í„°ì˜ ë³´ì¡°ì§€í‘œ\n\n\në³´ì¡°ì§€í‘œ ì¶”ê°€\n\n\n\nëª¨ë¸ í•™ìŠµ\n\n\n\n\nimport pandas as pd\nimport numpy as np\nfrom tqdm import tqdm\nimport FinanceDataReader as fdr\nimport pymysql\n\nimport warnings\nwarnings.filterwarnings('ignore')\n    \nimport ta\n\nfrom ipywidgets import interact, interact_manual\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.graph_objects as go\nimport plotly.offline as pyo\n%matplotlib inline\n%pylab inline\npylab.rcParams['figure.figsize'] = (12,5)\n\n%pylab is deprecated, use %matplotlib inline and import the required libraries.\nPopulating the interactive namespace from numpy and matplotlib\n\n\n \n\n\n\n\nì£¼ì‹ë°ì´í„°ì—ì„œ ë³´ì¡°ì§€í‘œë€ ê¸°ìˆ ì ì§€í‘œë¼ê³ ë„ ë¶ˆë¦¬ë©°, ë‹¤ì–‘í•œ ê°ë„ì™€ ê³„ì‚°ì‹, í†µê³„ ë“±ì„ ë°”íƒ•ìœ¼ë¡œ ê¸°ë³¸ì  ë¶„ì„ ë°©ë²•ë“¤ê³¼ ì¡°í•©í•˜ì—¬ ë³´ë‹¤ í­ ë„“ì€ ì‹œì¥ ì˜ˆì¸¡ì„ ê°€ëŠ¥í•˜ê²Œ ë„ì™€ì£¼ëŠ” ì°¨íŠ¸ ë¶„ì„ ë„êµ¬ì´ë‹¤.\n[ì°¸ê³ ]\n-01 ë³´ì¡°ì§€í‘œë€?\nê·¸ ì¤‘ì—ì„œë„, ì£¼ì‹ ì‹œì¥ì—ì„œ ë§ì´ ì•Œë ¤ì ¸ ìˆê³ , ì£¼ê°€ ì˜ˆì¸¡ì— ìì£¼ ì‚¬ìš©ë˜ëŠ” ë³´ì¡°ì§€í‘œë¡œëŠ” ì´ë™í‰ê· ì„ ì„ ì˜ˆë¡œ ë“¤ì–´ í™•ì¸í•´ë³´ê² ë‹¤. ì´ë™í‰ê· ì„ ì„ ìº”ë“¤ì°¨íŠ¸ì™€ í•¨ê»˜ ì‹œê°í™” í•˜ê³ , ê° ì¢…ëª©ê³¼ ë‚ ì§œì— ëŒ€í•´ ì–´ë–»ê²Œ ë‚˜íƒ€ë‚˜ëŠ”ì§€ ì§ì ‘ í™•ì¸í•œë‹¤.\n\në°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°\n\n2018-01-01 ~ 2020-12-31 ê¸°ê°„ ë™ì•ˆì˜ ëª¨ë“  ë°ì´í„°ë¥¼ ë¯¸ë¦¬ ì €ì¥í•´ ë†“ì€ ì£¼ê°€ ë°ì´í„°ì…‹ì„ ë¶ˆëŸ¬ì˜¨ë‹¤.\n\ndf_stock = pd.read_csv(\"stock_data_2018_2020.csv\")\ndf_stock['Code'] = df_stock['Code'].apply(lambda x : str(x).zfill(6))\ndf_stock\n\n\n\n\n\n  \n    \n      \n      Code\n      Date\n      Open\n      High\n      Low\n      Close\n      Volume\n      Change\n    \n  \n  \n    \n      0\n      050120\n      2018-01-02\n      10250\n      12050\n      10150\n      11800\n      26086769\n      0.145631\n    \n    \n      1\n      050120\n      2018-01-03\n      11950\n      12450\n      10900\n      11750\n      20460474\n      -0.004237\n    \n    \n      2\n      050120\n      2018-01-04\n      11850\n      14150\n      11600\n      12600\n      60663854\n      0.072340\n    \n    \n      3\n      050120\n      2018-01-05\n      12800\n      13200\n      12000\n      12200\n      13935258\n      -0.031746\n    \n    \n      4\n      050120\n      2018-01-08\n      12450\n      13400\n      12350\n      12850\n      16471707\n      0.053279\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      1152013\n      000540\n      2020-12-23\n      2880\n      2945\n      2835\n      2870\n      87318\n      -0.010345\n    \n    \n      1152014\n      000540\n      2020-12-24\n      2850\n      2875\n      2845\n      2860\n      28350\n      -0.003484\n    \n    \n      1152015\n      000540\n      2020-12-28\n      2860\n      3000\n      2805\n      2820\n      66036\n      -0.013986\n    \n    \n      1152016\n      000540\n      2020-12-29\n      2820\n      2920\n      2705\n      2775\n      83187\n      -0.015957\n    \n    \n      1152017\n      000540\n      2020-12-30\n      2835\n      2835\n      2755\n      2830\n      33270\n      0.019820\n    \n  \n\n1152018 rows Ã— 8 columns\n\n\n\n\nì´ë™í‰ê· ì„  ìº”ë“¤ì°¨íŠ¸ ì‹œê°í™”\n\n\nIF = open('../data/code_list.txt')\nlst_code = IF.readlines()\n\nfor idx, code in enumerate(lst_code):\n    lst_code[idx] = code.strip()\n\n\n@interact\ndef show_label_dist(code = lst_code):\n    df = df_stock[df_stock['Code']==code]    \n    ma_ls = [5, 20, 60, 120]\n    for i in range(len(ma_ls)):\n        sr1 = df['Close'].rolling(window=ma_ls[i]).mean()\n        df['MA'+str(ma_ls[i])] = sr1\n    df = df.dropna(axis=0)\n    \n    # ìº”ë“¤ ì°¨íŠ¸ \n    candle = go.Candlestick(x=df['Date'],\n                open=df['Open'],\n                high=df['High'],\n                low=df['Low'],\n                close=df['Close'], increasing_line_color= 'red', decreasing_line_color= 'blue')\n    \n    # ì´ë™í‰ê· ì„ \n    line_ma5 = go.Scatter(x=df['Date'], y=df['MA5'], mode='lines', name='MA5', line=dict(color='magenta', width=0.5))\n    line_ma20 = go.Scatter(x=df['Date'], y=df['MA20'], mode='lines', name='MA20', line=dict(color='blue', width=0.5))\n    line_ma60 = go.Scatter(x=df['Date'], y=df['MA60'], mode='lines', name='MA60', line=dict(color='green', width=0.5))\n    line_ma120 = go.Scatter(x=df['Date'], y=df['MA120'], mode='lines', name='MA120', line=dict(color='red', width=0.5))\n    \n    # ì œëª© ì¶”ê°€\n    layout = go.Layout(title='{} ìº”ë“¤ì°¨íŠ¸'.format(code), titlefont=dict(size=20, color='black'))\n    \n    fig = go.Figure(data=[candle, line_ma5, line_ma20, line_ma60, line_ma120], layout=layout)\n    \n    fig.show()\n\n\n\n\nìœ„ì˜ ìº”ë“¤ì°¨íŠ¸ì—ì„œ ì›í•˜ëŠ” ì½”ë“œë¥¼ ì„ íƒí•˜ì—¬ 4ì¼, 20ì¼, 60ì¼, 120ì¼ ì´ë™í‰ê· ì„ ì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤.\n \n\n\n\ní˜„ì¬ baseline ëª¨ë¸ì—ì„œëŠ” 10ì¼ì¹˜ì˜ ì‹œê°€, ê³ ê°€, ì €ê°€, ì¢…ê°€, ê±°ë˜ëŒ€ê¸ˆì„ ë…ë¦½ë³€ìˆ˜ë¡œ í•˜ì—¬ í•™ìŠµì„ ì§„í–‰í•˜ì˜€ë‹¤.\nì¶œì²˜-ë„¤ì´ë²„ ì¦ê¶Œ\ní•˜ì§€ë§Œ 10ì¼ê°„ì˜ ì£¼ê°€ ì°¨íŠ¸ ì˜ˆì‹œ ì‚¬ì§„ì„ ë³´ë©´, ë„ˆë¬´ ì ì€ ì •ë³´ë§Œì„ ë‹´ê³  ìˆì–´ ì£¼ê°€ ì˜ˆì¸¡ì„ ìœ„í•œ íŒ¨í„´ì„ ì°¾ì•„ë‚´ê¸°ë€ ë§¤ìš° ì–´ë µë‹¤. ë”°ë¼ì„œ ì£¼ì‹ì˜ ì—¬ëŸ¬ ë³´ì¡°ì§€í‘œë¥¼ ì‚¬ìš©í•´ì„œ í•´ë‹¹ ì£¼ì‹ì— ëŒ€í•œ ì••ì¶•ëœ ì •ë³´ë¥¼ ë…ë¦½ë³€ìˆ˜ì— ì¶”ê°€í•´ì¤€ë‹¤.\nì•ì„œ ì‹œê°í™” í•´ë³´ì•˜ë˜ ì´ë™í‰ê· ì„  ë§ê³ ë„ ë‹¤ì–‘í•œ ì£¼ì‹ ë³´ì¡°ì§€í‘œë“¤ì´ ì¡´ì¬í•œë‹¤. ë³´ì¡°ì§€í‘œë“¤ì„ ëª¨ë‘ ìˆ˜ë™ìœ¼ë¡œ ê³„ì‚°í•˜ê¸°ì—ëŠ” ì–´ë ¤ì›€ì´ ìˆìœ¼ë¯€ë¡œ TA ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•˜ì—¬ ì´ 49ê°œì˜ ë³´ì¡°ì§€í‘œë¥¼ ì¶”ê°€í•œë‹¤. (+ trading_value)\n-TA library github\n\në³´ì¡°ì§€í‘œ ì¶”ê°€ ì½”ë“œ\n\n\ndf_index = pd.DataFrame()\nfor code, stock_df in tqdm(df_stock.groupby('Code')):\n    \n    # ì´í‰ì„  ìƒì„±\n    ma = [5,20,60,120]\n    for days in ma:\n        stock_df['ma_'+str(days)] = stock_df['Close'].rolling(window = days).mean()\n    \n    # ì—¬ëŸ¬ ë³´ì¡° ì§€í‘œ ìƒì„±\n    H, L, C, V = stock_df['High'], stock_df['Low'], stock_df['Close'], stock_df['Volume']\n    \n    stock_df['trading_value'] = stock_df['Close']*stock_df['Volume']\n    \n    stock_df['MFI'] = ta.volume.money_flow_index(\n        high=H, low=L, close=C, volume=V, fillna=True)\n    \n    stock_df['ADI'] = ta.volume.acc_dist_index(\n        high=H, low=L, close=C, volume=V, fillna=True)\n    \n    stock_df['OBV'] = ta.volume.on_balance_volume(close=C, volume=V, fillna=True)\n    stock_df['CMF'] = ta.volume.chaikin_money_flow(\n        high=H, low=L, close=C, volume=V, fillna=True)\n    \n    stock_df['FI'] = ta.volume.force_index(close=C, volume=V, fillna=True)\n    stock_df['EOMEMV'] = ta.volume.ease_of_movement(\n        high=H, low=L, volume=V, fillna=True)\n    \n    stock_df['VPT'] = ta.volume.volume_price_trend(close=C, volume=V, fillna=True)\n    stock_df['NVI'] = ta.volume.negative_volume_index(close=C, volume=V, fillna=True)\n    stock_df['VMAP'] = ta.volume.volume_weighted_average_price(\n        high=H, low=L, close=C, volume=V, fillna=True)\n    \n    # Volatility\n    stock_df['ATR'] = ta.volatility.average_true_range(\n        high=H, low=L, close=C, fillna=True)\n    stock_df['BHB'] = ta.volatility.bollinger_hband(close=C, fillna=True)\n    stock_df['BLB'] = ta.volatility.bollinger_lband(close=C, fillna=True)\n    stock_df['KCH'] = ta.volatility.keltner_channel_hband(\n        high=H, low=L, close=C, fillna=True)\n    stock_df['KCL'] = ta.volatility.keltner_channel_lband(\n        high=H, low=L, close=C, fillna=True)\n    stock_df['KCM'] = ta.volatility.keltner_channel_mband(\n        high=H, low=L, close=C, fillna=True)\n    stock_df['DCH'] = ta.volatility.donchian_channel_hband(\n        high=H, low=L, close=C, fillna=True)\n    stock_df['DCL'] = ta.volatility.donchian_channel_lband(\n        high=H, low=L, close=C, fillna=True)\n    stock_df['DCM'] = ta.volatility.donchian_channel_mband(\n        high=H, low=L, close=C, fillna=True)\n    stock_df['UI'] = ta.volatility.ulcer_index(close=C, fillna=True)\n    # Trend\n    stock_df['SMA'] = ta.trend.sma_indicator(close=C, fillna=True)\n    stock_df['EMA'] = ta.trend.ema_indicator(close=C, fillna=True)\n    stock_df['WMA'] = ta.trend.wma_indicator(close=C, fillna=True)\n    stock_df['MACD'] = ta.trend.macd(close=C, fillna=True)\n    stock_df['ADX'] = ta.trend.adx(high=H, low=L, close=C, fillna=True)\n    stock_df['VIneg'] = ta.trend.vortex_indicator_neg(\n        high=H, low=L, close=C, fillna=True)\n    stock_df['VIpos'] = ta.trend.vortex_indicator_pos(\n        high=H, low=L, close=C, fillna=True)\n    stock_df['TRIX'] = ta.trend.trix(close=C, fillna=True)\n    stock_df['MI'] = ta.trend.mass_index(high=H, low=L, fillna=True)\n    stock_df['CCI'] = ta.trend.cci(high=H, low=L, close=C, fillna=True)\n    stock_df['DPO'] = ta.trend.dpo(close=C, fillna=True)\n    stock_df['KST'] = ta.trend.kst(close=C, fillna=True)\n    stock_df['Ichimoku'] = ta.trend.ichimoku_a(high=H, low=L, fillna=True)\n    stock_df['ParabolicSAR'] = ta.trend.psar_down(\n        high=H, low=L, close=C, fillna=True)\n    stock_df['STC'] = ta.trend.stc(close=C, fillna=True)\n    # Momentum\n    stock_df['RSI'] = ta.momentum.rsi(close=C, fillna=True)\n    stock_df['SRSI'] = ta.momentum.stochrsi(close=C, fillna=True)\n    stock_df['TSI'] = ta.momentum.tsi(close=C, fillna=True)\n    stock_df['UO'] = ta.momentum.ultimate_oscillator(\n        high=H, low=L, close=C, fillna=True)\n    stock_df['SR'] = ta.momentum.stoch(close=C, high=H, low=L, fillna=True)\n    stock_df['WR'] = ta.momentum.williams_r(high=H, low=L, close=C, fillna=True)\n    stock_df['AO'] = ta.momentum.awesome_oscillator(high=H, low=L, fillna=True)\n    stock_df['KAMA'] = ta.momentum.kama(close=C, fillna=True)\n    stock_df['ROC'] = ta.momentum.roc(close=C, fillna=True)\n    stock_df['PPO'] = ta.momentum.ppo(close=C, fillna=True)\n    stock_df['PVO'] = ta.momentum.pvo(volume=V, fillna=True)\n    \n    df_index = df_index.append(stock_df) \n\n# ì €ì¥\ndf_index.to_csv(\"stock_data_2018_2020_add_index.csv\", index=False)\ndf_index\n\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1561/1561 [07:35<00:00,  3.42it/s]\n\n\n\n\n\n\n  \n    \n      \n      Code\n      Date\n      Open\n      High\n      Low\n      Close\n      Volume\n      Change\n      ma_5\n      ma_20\n      ...\n      SRSI\n      TSI\n      UO\n      SR\n      WR\n      AO\n      KAMA\n      ROC\n      PPO\n      PVO\n    \n  \n  \n    \n      862985\n      000020\n      2018-01-02\n      9750\n      9900\n      9700\n      9870\n      120676\n      0.012308\n      NaN\n      NaN\n      ...\n      0.000000\n      0.000000\n      0.000000\n      85.000000\n      -15.000000\n      0.000000\n      9870.000000\n      0.000000\n      0.000000\n      0.000000\n    \n    \n      862986\n      000020\n      2018-01-03\n      9900\n      10250\n      9820\n      10000\n      268220\n      0.013171\n      NaN\n      NaN\n      ...\n      0.000000\n      100.000000\n      28.571429\n      54.545455\n      -45.454545\n      0.000000\n      9925.471999\n      0.000000\n      0.104967\n      8.943334\n    \n    \n      862987\n      000020\n      2018-01-04\n      10050\n      10050\n      9680\n      9750\n      161342\n      -0.025000\n      NaN\n      NaN\n      ...\n      0.000000\n      95.815900\n      25.000000\n      12.280702\n      -87.719298\n      0.000000\n      9854.744908\n      0.000000\n      -0.015865\n      9.215678\n    \n    \n      862988\n      000020\n      2018-01-05\n      9750\n      9980\n      9750\n      9910\n      116604\n      0.016410\n      NaN\n      NaN\n      ...\n      0.000000\n      92.627651\n      33.333333\n      40.350877\n      -59.649123\n      0.000000\n      9882.663078\n      0.000000\n      0.018877\n      6.837356\n    \n    \n      862989\n      000020\n      2018-01-08\n      10000\n      10150\n      9940\n      9950\n      158326\n      0.004036\n      9896.0\n      NaN\n      ...\n      0.000000\n      90.156386\n      30.612245\n      47.368421\n      -52.631579\n      0.000000\n      9935.876263\n      0.000000\n      0.078152\n      7.233628\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      43418\n      950130\n      2020-12-22\n      23950\n      23950\n      22800\n      22900\n      978011\n      -0.043841\n      23490.0\n      22212.5\n      ...\n      0.601825\n      -1.890990\n      29.641945\n      41.379310\n      -58.620690\n      577.058824\n      22725.390944\n      17.737789\n      -0.505568\n      -12.450728\n    \n    \n      43419\n      950130\n      2020-12-23\n      23000\n      23300\n      21000\n      21450\n      1239228\n      -0.063319\n      23080.0\n      22310.0\n      ...\n      0.353271\n      -3.660151\n      28.338277\n      23.963134\n      -76.036866\n      353.382353\n      22679.784965\n      -15.049505\n      -1.064724\n      -16.358507\n    \n    \n      43420\n      950130\n      2020-12-24\n      21500\n      21800\n      19600\n      21700\n      857865\n      0.011655\n      22700.0\n      22382.5\n      ...\n      0.083197\n      -4.812902\n      38.852203\n      20.792079\n      -79.207921\n      -58.235294\n      22583.248771\n      -12.323232\n      -1.408773\n      -20.829146\n    \n    \n      43421\n      950130\n      2020-12-28\n      21750\n      21800\n      19500\n      19650\n      1523257\n      -0.094470\n      21930.0\n      22322.5\n      ...\n      0.000000\n      -7.972071\n      33.462925\n      1.470588\n      -98.529412\n      -402.647059\n      22055.737842\n      -13.245033\n      -2.394064\n      -22.448964\n    \n    \n      43422\n      950130\n      2020-12-29\n      19950\n      22450\n      19900\n      21500\n      1568505\n      0.094148\n      21440.0\n      22392.5\n      ...\n      0.526558\n      -8.199398\n      40.132825\n      28.169014\n      -71.830986\n      -759.852941\n      22026.178902\n      -9.473684\n      -2.489633\n      -23.522675\n    \n  \n\n1149377 rows Ã— 58 columns\n\n\n\në³´ì¡°ì§€í‘œë¥¼ ì¶”ê°€í•˜ì—¬ 8ê°œì˜€ë˜ ì»¬ëŸ¼ì´ 58ê°œì˜ ì»¬ëŸ¼ì´ ëœ ê²ƒì„ í™•ì¸í•˜ì˜€ë‹¤.\n \n\n\n\n\në°ì´í„°ì…‹ ìƒì„± í•¨ìˆ˜\n\nìœ„ì—ì„œ ë³´ì¡°ì§€í‘œë¥¼ ì¶”ê°€ í–ˆë˜ ë°©ë²•ê³¼ ë™ì¼í•˜ê²Œ ë¯¸ë¦¬ ìƒì„±í•´ ë†“ì€ ë³´ì¡°ì§€í‘œ ì¶”ê°€ ë°ì´í„°ì…‹ì„ ì„œë²„ DBì—ì„œ ë¶ˆëŸ¬ì˜¨ë‹¤. baseline ëª¨ë¸ì—ì„œ ì§„í–‰í–ˆë˜ ë°ì´í„°ì…‹ì„ ë” ëŠ˜ë ¤ train 2017-2020, test 2021 ê¸°ê°„ ë™ì•ˆì˜ ë°ì´í„°ë¥¼ ì‚¬ìš©í•œë‹¤.\n\ndef make_dataset(train=True):\n    \n    IF = open('../data/code_list.txt')\n    lst_code = IF.readlines()\n\n    lst_X = []\n    lst_Y = []\n    lst_code_date = []\n    \n    db_dsml = pymysql.connect(\n        host = 'localhost', \n        port = 3306, \n        user = '[db username]', \n        passwd = '[db password]', \n        db = '[db name]', \n        charset = 'utf8'\n    )\n    cursor = db_dsml.cursor()\n    \n    \n    for code in tqdm(lst_code): \n        code = code.strip()\n        \n        if train: \n            sql_query = '''\n                        SELECT *\n                        FROM stock_{}\n                        WHERE Date BETWEEN '2017-01-01' AND '2020-12-31'\n                        '''.format(code)\n        else:\n            sql_query = '''\n                        SELECT *\n                        FROM stock_{}\n                        WHERE Date BETWEEN '2021-01-01' AND '2021-12-31'\n                        '''.format(code)\n        \n        stock = pd.read_sql(sql = sql_query, con = db_dsml)   \n\n        lst_stock = stock.values.tolist()\n\n\n        for idx, row in enumerate(lst_stock): \n            date, trading_value = row[0].date().strftime(\"%Y%m%d\"), row[4]*row[5]\n            if trading_value >= 100000000000:\n                if (idx < 9) or (idx >= len(lst_stock)-1): # ì˜ˆì™¸ ì²˜ë¦¬ \n                    continue \n                \n                # D-9 ~ D0 ë°ì´í„°ë§Œ ë‹´ê¸° \n                sub_stock = lst_stock[idx-9:idx+1] \n\n                # 10ì¼ê°„ì˜ ë°ì´í„° \n                lst_result = []\n                for row2 in sub_stock:\n                    lst_prices, lst_index = row2[1:6], row2[8:]\n                    lst_result += lst_prices + lst_index + [trading_value]\n\n                # D+1 ì¢…ê°€ 2% ìƒìŠ¹ ì—¬ë¶€ \n                label = int(row[7] >= 0.02)\n                \n                # ì¢…ì†ë³€ìˆ˜, ë…ë¦½ë³€ìˆ˜, ì¢…ëª©ì½”ë“œ, ë‚ ì§œ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€ \n                lst_X.append(lst_result)\n                lst_Y.append(label)\n                lst_code_date.append([code, date])\n            \n    return np.array(lst_X), np.array(lst_Y), np.array(lst_code_date)\n\n\ntrain dataset ìƒì„±\n\n\ntrainX, trainY, lst_code_date = make_dataset(train=True)\n\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1561/1561 [02:16<00:00, 11.44it/s]\n\n\n\ntest dataset ìƒì„±\n\n\ntestX, testY, lst_code_date_test = make_dataset(train=False)\n\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1561/1561 [00:40<00:00, 38.37it/s]\n\n\n\npickle ì €ì¥\n\n\n#collapse-hide\nimport pickle\n\ndic_result = {\n    'train': [trainX, trainY, lst_code_date],\n    'test': [testX, testY, lst_code_date_test]\n}\n\nwith open('dataset_2020_2021.pickle', 'wb') as handle:\n    pickle.dump(dic_result, handle, protocol=pickle.HIGHEST_PROTOCOL)\n\n\npickle ë¶ˆëŸ¬ì˜¤ê¸°\n\n\n#collapse-hide\nwith open('dataset_2020_2021.pickle', 'rb') as handle:\n    dataset = pickle.load(handle)\n\n\ntrainX, trainY, lst_code_date = dataset['train'][0], dataset['train'][1], dataset['train'][2]\ntestX, testY, lst_code_date_test = dataset['test'][0], dataset['test'][1], dataset['test'][2]\n\nprint('train dataset: ', trainX.shape, trainY.shape)\nprint('test dataset: ', testX.shape, testY.shape)\n\ntrain dataset:  (13870, 550) (13870,)\ntest dataset:  (8341, 550) (8341,)\n\n\n\nXGBoost ëª¨ë¸ í•™ìŠµ\n\n\nfrom xgboost import XGBClassifier\nxgb = XGBClassifier(\n                   n_jobs=40,\n                   scale_pos_weight=4,\n                   learning_rate=0.01,\n                   max_depth=3,\n                   n_estimators=500,\n                   ) \n\nxgb.fit(trainX, trainY)\n\n[17:26:28] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n\n\nXGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n              gamma=0, gpu_id=-1, importance_type=None,\n              interaction_constraints='', learning_rate=0.01, max_delta_step=0,\n              max_depth=3, min_child_weight=1, missing=nan,\n              monotone_constraints='()', n_estimators=500, n_jobs=40,\n              num_parallel_tree=1, predictor='auto', random_state=0,\n              reg_alpha=0, reg_lambda=1, scale_pos_weight=4, subsample=1,\n              tree_method='exact', validate_parameters=1, verbosity=None)\n\n\n\nëª¨ë¸ í‰ê°€ì§€í‘œ ì‹œê°í™” í•¨ìˆ˜\ntrain, test ë°ì´í„°ì…‹ì— ëŒ€í•˜ì—¬ Accuracy, AUC, f1_score, precision, recall í‰ê°€ì§€í‘œë¥¼ êµ¬í•˜ê³ , ì‹œê°í™” í•œë‹¤.\n\n\ndef plot_evauate(trainX, trainY, testX, testY, model):\n    from sklearn.metrics import roc_curve, roc_auc_score, f1_score, f1_score, accuracy_score, recall_score, precision_score\n    \n    train_pred = model.predict(trainX)\n    train_prob = model.predict_proba(trainX)[:, 1]\n    \n    test_pred = model.predict(testX) \n    test_prob = model.predict_proba(testX)[:, 1]\n    \n    \n    # ROC Curve ì‹œê°í™” \n    fpr, tpr, thresholds = roc_curve(testY, test_prob) \n    \n    plt.plot(fpr, tpr, color='red', label='ROC')\n    plt.plot([0, 1], [0, 1], color='green', linestyle='--')\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('test ROC : {}'.format(round(roc_auc_score(testY, test_prob),3)),fontsize=25)\n    plt.legend()\n    plt.show()\n    \n    \n    \n    # ì—¬ëŸ¬ í‰ê°€ì§€í‘œ ì‹œê°í™” \n    dic_name2func = { 'F1-score': f1_score, \n                     'Recall': recall_score, \n                     'Precision': precision_score, \n                     'Accuracy': accuracy_score, \n                     'ROCAUC': roc_auc_score }\n    \n    lst_result = []\n    for name, func in dic_name2func.items():\n        if name == 'ROCAUC':\n            train = func(trainY, train_prob)\n            test = func(testY, test_prob)\n            \n        else:\n            train = func(trainY, train_pred)\n            test = func(testY, test_pred)\n        \n        lst_result.append([name, train, test])\n    \n    df = pd.DataFrame(data=lst_result,\n                columns=['name', 'train', 'test'])\n    df = df.melt(id_vars='name')    \n        \n    ax = sns.barplot(data=df, x='name', y='value', hue='variable')\n    ax.set_ylim(0, 1)\n    \n    # í…ìŠ¤íŠ¸ ì¶”ê°€ \n    for p in ax.patches:\n        height = p.get_height()\n        ax.text(p.get_x() + p.get_width() / 2., height + 0.01, round(height, 3), ha = 'center', size = 10)\n\n    plt.show()\n\n\nì„±ëŠ¥ í‰ê°€\n\n\nplot_evauate(trainX, trainY, testX, testY, xgb)\n\n\n\n\n\n\n\nrocauc scoreëŠ” 0.595ì˜ ì„±ëŠ¥ì„ ë³´ì˜€ë‹¤. ë°ì´í„°ì˜ ê¸°ê°„ì„ ëŠ˜ë¦¬ê³ , ë³´ì¡°ì§€í‘œë¥¼ ì¶”ê°€í•¨ìœ¼ë¡œì¨ ì§€ë‚œ ê¸€ì˜ Baseline Modelì˜ 0.56 ë³´ë‹¤ ì„±ëŠ¥ì´ í–¥ìƒë˜ì—ˆë‹¤.\në‹¤ìŒ ê¸€ì—ì„œëŠ” í˜„ì¬ ì‚¬ìš©í•˜ê³  ìˆëŠ” í†µí•© ì¢…ëª© ì£¼ê°€ ë°ì´í„°ì…‹ì˜ ì¢…ëª©ë³„ ê°€ê²©ì´ ëª¨ë‘ ë‹¤ë¥¸ ì ì„ ë³´ì™„í•˜ê¸° ìœ„í•´ ë°ì´í„°ë¥¼ í‘œì¤€í™”í•˜ëŠ” ì‹œê°„ì„ ê°€ì§„ë‹¤."
  },
  {
    "objectID": "posts/04.scaling.html",
    "href": "posts/04.scaling.html",
    "title": "[stock prediction] 2.2 ì£¼ê°€ ë°ì´í„° ìŠ¤ì¼€ì¼ë§",
    "section": "",
    "text": "ì´ì „ ê¸€ 2.1 ì£¼ê°€ ë°ì´í„°ì…‹ ë³´ì¡°ì§€í‘œ ì¶”ê°€ê¹Œì§€ ì£¼ê°€ ë°ì´í„°ì— ë³´ì¡°ì§€í‘œë¥¼ ì¶”ê°€í•˜ì—¬ ì„¤ëª…ë³€ìˆ˜ë¥¼ 50ê°œ ê°€ëŸ‰ ëŠ˜ë ¸ë‹¤. ê·¸ëŸ°ë° í˜„ì¬ ì‚¬ìš©í•˜ê³  ìˆëŠ” í†µí•© ì¢…ëª© ì£¼ê°€ ë°ì´í„°ëŠ” ì¢…ëª©ë§ˆë‹¤ ê°€ê²©ì´ ë‹¤ë¥´ë‹¤ëŠ” ë¬¸ì œì ì´ ì¡´ì¬í•œë‹¤. ì´ëŠ” ì„±ëŠ¥ì„ ì €í•˜ì‹œê¸°ëŠ” ì›ì¸ì´ ë  ìˆ˜ ìˆë‹¤. ë”°ë¼ì„œ ì£¼ê°€ ë°ì´í„°ì…‹ì˜ ì „ì²˜ë¦¬ íŒŒíŠ¸ì—ì„œ ìŠ¤ì¼€ì¼ë§ì€ ë§¤ìš° ì¤‘ìš”í•œ ìš”ì†Œë¼ê³ í•  ìˆ˜ ìˆë‹¤. min-max scalingì„ ì—¬ëŸ¬ ë°©ë²•ìœ¼ë¡œ ì‹œë„í•˜ê³ , ì „ ë‚  ì¢…ê°€ë¡œ ë‚˜ëˆ„ëŠ” div-close ë°©ë²•ì„ ì‚¬ìš©í•˜ì—¬ ì´ 4ê°€ì§€ ìŠ¤ì¼€ì¼ë§ ì„±ëŠ¥ì„ ë¹„êµí•´ë³´ë„ë¡ í•˜ê² ë‹¤.\n\n\n\n\nìŠ¤ì¼€ì¼ëŸ¬ ë¹„êµ\n\n\n\nìŠ¤ì¼€ì¼ëŸ¬ ì„ íƒ\n\n\n\n\n\n\nimport pandas as pd\nimport numpy as np\nfrom tqdm import tqdm\nimport FinanceDataReader as fdr\nimport pymysql\n\nimport warnings\nwarnings.filterwarnings('ignore')\n    \nimport ta\n\nfrom ipywidgets import interact, interact_manual\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.graph_objects as go\nimport plotly.offline as pyo\n%matplotlib inline\n%pylab inline\npylab.rcParams['figure.figsize'] = (12,5)\n\n\n# ìì£¼ì“°ëŠ” í•¨ìˆ˜ë“¤ì€ ëª¨ë“ˆí™” í•¨ \nimport StockFunc as sf \n\nPopulating the interactive namespace from numpy and matplotlib\n\n\n \n\n\n\n\n\n\nëª¨ë“  ì»¬ëŸ¼ì„ ë…ë¦½ì ìœ¼ë¡œ ì¢…ëª© ë³„ ì „ì²´ë°ì´í„°ì— ëŒ€í•´ min-max scaling í•œë‹¤.\n\ndef make_dataset_minmax(trading):\n    from sklearn.preprocessing import MinMaxScaler\n    # ì¢…ëª©ì½”ë“œ ë¶ˆëŸ¬ì˜¤ê¸° \n    IF = open('../data/code_list.txt')\n    lst_code = IF.readlines()\n\n    lst_X = []\n    lst_Y = []\n    lst_code_date = []\n    \n    db_dsml = pymysql.connect(\n        host = 'localhost', \n        port = 3306, \n        user = '[db username]', \n        passwd = '[db password]', \n        db = '[db name]', \n        charset = 'utf8'\n    )\n    cursor = db_dsml.cursor()\n    \n    \n    for code in tqdm(lst_code): \n        code = code.strip()\n        \n        sql_query = '''\n                    SELECT *\n                    FROM stock_{0}\n                    WHERE Date BETWEEN '2017-01-01' AND '2021-12-31'\n                    '''.format(code)\n\n        \n        stock = pd.read_sql(sql = sql_query, con = db_dsml) \n        stock['trading_value'] = stock['Close'] * stock['Volume']\n        \n        lst_stock = stock.values.tolist()\n\n        \n        # ğŸŒŸ scaling \n        df_temp = stock.drop(columns=['Date', 'Change', 'Next Change'])\n        scaler = MinMaxScaler()\n        scaled = scaler.fit_transform(df_temp)\n        \n        lst_stock_scaled = scaled.tolist()\n         \n        \n        for idx, row in enumerate(lst_stock): \n            date, trading_value = row[0].date().strftime(\"%Y-%m-%d\"), row[-1]\n            if trading_value >= trading:\n                if (idx < 9) or (idx >= len(lst_stock)-1): # ì˜ˆì™¸ ì²˜ë¦¬ \n                    continue \n                \n                # D-9 ~ D0 ë°ì´í„°ë§Œ ë‹´ê¸° \n                sub_stock = lst_stock_scaled[idx-9:idx+1] \n\n                # 10ì¼ê°„ì˜ ë°ì´í„° \n                lst_result = []\n                for row2 in sub_stock:\n                    lst_result += row2\n\n                # D+1 ì¢…ê°€ 2% ìƒìŠ¹ ì—¬ë¶€ \n                label = int(row[7] >= 0.02)\n                \n                # ì¢…ì†ë³€ìˆ˜, ë…ë¦½ë³€ìˆ˜, ì¢…ëª©ì½”ë“œ, ë‚ ì§œ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€ \n                lst_X.append(lst_result)\n                lst_Y.append(label)\n                lst_code_date.append([code, date])\n            \n    return pd.concat([pd.DataFrame(lst_code_date), pd.DataFrame(lst_X), pd.DataFrame(lst_Y)], axis=1)\n\n\ndf_dataset = make_dataset_minmax(trading=1000000000)\n\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1561/1561 [03:37<00:00,  7.18it/s]\n\n\n\ntrainX_1, trainY_1, testX_1, testY_1, lst_code_date, lst_code_date_test = sf.split(df_dataset)\n\nprint('train dataset: ', trainX_1.shape, trainY_1.shape)\nprint('test dataset: ', testX_1.shape, testY_1.shape)\n\ntrain dataset:  (659095, 550) (659095,)\ntest dataset:  (239818, 550) (239818,)\n\n\n\nfrom xgboost import XGBClassifier\nxgb_1 = XGBClassifier(\n                   n_jobs=40,\n                   scale_pos_weight=4,\n                   learning_rate=0.01,\n                   max_depth=3,\n                   n_estimators=500,\n                   ) \n\nxgb_1.fit(trainX_1, trainY_1)\n\n[15:05:37] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n\n\nXGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n              gamma=0, gpu_id=-1, importance_type=None,\n              interaction_constraints='', learning_rate=0.01, max_delta_step=0,\n              max_depth=3, min_child_weight=1, missing=nan,\n              monotone_constraints='()', n_estimators=500, n_jobs=40,\n              num_parallel_tree=1, predictor='auto', random_state=0,\n              reg_alpha=0, reg_lambda=1, scale_pos_weight=4, subsample=1,\n              tree_method='exact', validate_parameters=1, verbosity=None)\n\n\n\nsf.plot_evauate(trainX_1, trainY_1, testX_1, testY_1, xgb_1)\n\n\n\n\n\n\n\nROCAUC ì ìˆ˜ê°€ ì§€ë‚œ 0.595ì˜ ì„±ëŠ¥ ë³´ë‹¤ëŠ” í–¥ìƒëœ 0.613ì˜ ì„±ëŠ¥ì„ ë³´ì—¬ì£¼ì—ˆë‹¤.\n \n\n\n\nëª¨ë“  ì»¬ëŸ¼ì„ ë…ë¦½ì ìœ¼ë¡œ ì¢…ëª© ë³„ 10ì¼ì¹˜ì— ëŒ€í•´ min-max scaling í•œë‹¤.\n\ndef make_dataset_minmax_window(trading):\n    from sklearn.preprocessing import MinMaxScaler\n    # ì¢…ëª©ì½”ë“œ ë¶ˆëŸ¬ì˜¤ê¸° \n    IF = open('../data/code_list.txt')\n    lst_code = IF.readlines()\n\n    lst_X = []\n    lst_Y = []\n    lst_code_date = []\n    \n    db_dsml = pymysql.connect(\n        host = 'localhost', \n        port = 3306, \n        user = '[db username]', \n        passwd = '[db password]', \n        db = '[db name]', \n        charset = 'utf8'\n    )\n    cursor = db_dsml.cursor()\n    \n    \n    for code in tqdm(lst_code): \n        code = code.strip()\n        \n        sql_query = '''\n                    SELECT *\n                    FROM stock_{0}\n                    WHERE Date BETWEEN '2017-01-01' AND '2021-12-31'\n                    '''.format(code)\n\n        \n        stock = pd.read_sql(sql = sql_query, con = db_dsml) \n        stock['trading_value'] = stock['Close'] * stock['Volume']\n        \n        lst_stock = stock.values.tolist()\n\n        \n        for idx, row in enumerate(lst_stock): \n            date, trading_value = row[0].date().strftime(\"%Y-%m-%d\"), row[-1]\n            if trading_value >= trading:\n                if (idx < 9) or (idx >= len(lst_stock)-1): # ì˜ˆì™¸ ì²˜ë¦¬ \n                    continue \n                \n                # D-9 ~ D0 ë°ì´í„°ë§Œ ë‹´ê¸° \n                arr_sub_stock = np.array(lst_stock[idx-9:idx+1])\n\n                # ğŸŒŸ scaling \n                arr_temp = np.concatenate((arr_sub_stock[:, 1:6], arr_sub_stock[:, 8:]), axis=1) \n                scaler = MinMaxScaler()\n                scaled = scaler.fit_transform(np.array(arr_temp))\n\n                lst_sub_stock_scaled = scaled.tolist()\n                \n                # 10ì¼ê°„ì˜ ë°ì´í„° \n                lst_result = []\n                for row2 in lst_sub_stock_scaled:\n                    lst_result += row2\n\n                # D+1 ì¢…ê°€ 2% ìƒìŠ¹ ì—¬ë¶€ \n                label = int(row[7] >= 0.02)\n                \n                # ì¢…ì†ë³€ìˆ˜, ë…ë¦½ë³€ìˆ˜, ì¢…ëª©ì½”ë“œ, ë‚ ì§œ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€ \n                lst_X.append(lst_result)\n                lst_Y.append(label)\n                lst_code_date.append([code, date])\n            \n    return pd.concat([pd.DataFrame(lst_code_date), pd.DataFrame(lst_X), pd.DataFrame(lst_Y)], axis=1)\n\n\ndf_dataset = make_dataset_minmax_window(trading=1000000000)\n\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1561/1561 [05:53<00:00,  4.41it/s]\n\n\n\ntrainX_2, trainY_2, testX_2, testY_2, lst_code_date, lst_code_date_test = sf.split(df_dataset)\n\nprint('train dataset: ', trainX_2.shape, trainY_2.shape)\nprint('test dataset: ', testX_2.shape, testY_2.shape)\n\ntrain dataset:  (659095, 550) (659095,)\ntest dataset:  (239818, 550) (239818,)\n\n\n\nfrom xgboost import XGBClassifier\nxgb_2 = XGBClassifier(\n                   n_jobs=40,\n                   scale_pos_weight=4,\n                   learning_rate=0.01,\n                   max_depth=3,\n                   n_estimators=500,\n                   ) \n\nxgb_2.fit(trainX_2, trainY_2)\n\n[16:05:07] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n\n\nXGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n              gamma=0, gpu_id=-1, importance_type=None,\n              interaction_constraints='', learning_rate=0.01, max_delta_step=0,\n              max_depth=3, min_child_weight=1, missing=nan,\n              monotone_constraints='()', n_estimators=500, n_jobs=40,\n              num_parallel_tree=1, predictor='auto', random_state=0,\n              reg_alpha=0, reg_lambda=1, scale_pos_weight=4, subsample=1,\n              tree_method='exact', validate_parameters=1, verbosity=None)\n\n\n\nsf.plot_evauate(trainX_2, trainY_2, testX_2, testY_2, xgb_2)\n\n\n\n\n\n\n\nì§€ë‚œ ê¸€ì—ì„œ 0.595 ì˜€ë˜ ROCAUC ì„±ëŠ¥ì´ 0.575ë¡œ í•˜ë½í•œ ê²ƒì„ ë³´ì˜€ë‹¤.\n \n\n\n\n\nê°€ê²© ê´€ë ¨ ì»¬ëŸ¼: ìº”ë“¤ì°¨íŠ¸ì˜ ìµœì†Ÿê°’, ìµœëŒ“ê°’ìœ¼ë¡œ min-max scaling\në‚˜ë¨¸ì§€ ì»¬ëŸ¼: scaling í•˜ì§€ ì•ŠëŠ”ë‹¤.\n\n\ndef make_dataset_minmax_price(trading):\n    from sklearn.preprocessing import MinMaxScaler\n    \n    col_price = ['Open', 'High', 'Low', 'Close', 'MA5', 'MA20', 'MA60', 'MA120', \n               'VMAP', 'BHB', 'BLB', 'KCH', 'KCL', 'KCM', 'DCH', 'DCL', 'DCM',\n               'SMA', 'EMA', 'WMA', 'Ichimoku', 'Parabolic SAR', 'KAMA','MACD']   \n\n    col_etc = ['Volume', 'MFI', 'ADI', 'OBV',\n           'CMF', 'FI', 'EOM, EMV', 'VPT', 'NVI', 'ATR', 'UI',\n           'ADX', '-VI', '+VI', 'TRIX', 'MI', 'CCI', 'DPO', 'KST',\n           'STC', 'RSI', 'SRSI', 'TSI', 'UO', 'SR',\n           'WR', 'AO', 'ROC', 'PPO', 'PVO', 'trading_value']\n    \n    # ì¢…ëª©ì½”ë“œ ë¶ˆëŸ¬ì˜¤ê¸° \n    IF = open('../data/code_list.txt')\n    lst_code = IF.readlines()\n\n    lst_X = []\n    lst_Y = []\n    lst_code_date = []\n    \n    db_dsml = pymysql.connect(\n        host = 'localhost', \n        port = 3306, \n        user = '[db username]', \n        passwd = '[db password]', \n        db = '[db name]', \n        charset = 'utf8'\n    )\n    cursor = db_dsml.cursor()\n    \n    \n    for code in tqdm(lst_code): \n        code = code.strip()\n        \n        sql_query = '''\n                    SELECT *\n                    FROM stock_{0}\n                    WHERE Date BETWEEN '2017-01-01' AND '2021-12-31'\n                    '''.format(code)\n\n        \n        stock = pd.read_sql(sql = sql_query, con = db_dsml) \n        stock['trading_value'] = stock['Close'] * stock['Volume']\n        \n        lst_stock = stock.values.tolist()\n\n        \n        # ğŸŒŸ scaling\n        # 1) ê°€ê²© ê´€ë ¨ ì»¬ëŸ¼ \n        df_price = stock[col_price]\n        minimum = df_price['Low'].min()\n        maximum = df_price['High'].max()\n        df_price_scaled = df_price.apply(lambda x: (x-minimum) / (maximum-minimum))\n        \n        # 2) ë‚˜ë¨¸ì§€ ì»¬ëŸ¼ \n        df_etc = stock[col_etc]\n        \n        df_scaled = pd.concat([df_price_scaled, df_etc_scaled], axis=1)\n        \n        lst_stock_scaled = df_scaled.values.tolist()\n         \n            \n        for idx, row in enumerate(lst_stock): \n            date, trading_value = row[0].date().strftime(\"%Y-%m-%d\"), row[-1]\n            if trading_value >= trading:\n                if (idx < 9) or (idx >= len(lst_stock)-1): # ì˜ˆì™¸ ì²˜ë¦¬ \n                    continue \n                \n                # D-9 ~ D0 ë°ì´í„°ë§Œ ë‹´ê¸° \n                sub_stock = lst_stock_scaled[idx-9:idx+1] \n\n                # 10ì¼ê°„ì˜ ë°ì´í„° \n                lst_result = []\n                for row2 in sub_stock:\n                    lst_result += row2\n\n                # D+1 ì¢…ê°€ 2% ìƒìŠ¹ ì—¬ë¶€ \n                label = int(row[7] >= 0.02)\n                \n                # ì¢…ì†ë³€ìˆ˜, ë…ë¦½ë³€ìˆ˜, ì¢…ëª©ì½”ë“œ, ë‚ ì§œ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€ \n                lst_X.append(lst_result)\n                lst_Y.append(label)\n                lst_code_date.append([code, date])\n            \n    return pd.concat([pd.DataFrame(lst_code_date), pd.DataFrame(lst_X), pd.DataFrame(lst_Y)], axis=1)\n\n\ndf_dataset = make_dataset_minmax_price(trading=1000000000)\n\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1561/1561 [03:53<00:00,  6.68it/s]\n\n\n\ntrainX_3, trainY_3, testX_3, testY_3, lst_code_date, lst_code_date_test = sf.split(df_dataset)\n\nprint('train dataset: ', trainX_3.shape, trainY_3.shape)\nprint('test dataset: ', testX_3.shape, testY_3.shape)\n\ntrain dataset:  (659095, 550) (659095,)\ntest dataset:  (239818, 550) (239818,)\n\n\n\nfrom xgboost import XGBClassifier\nxgb_3 = XGBClassifier(\n                   n_jobs=40,\n                   scale_pos_weight=4,\n                   learning_rate=0.01,\n                   max_depth=3,\n                   n_estimators=500,\n                   ) \n\nxgb_3.fit(trainX_3, trainY_3)\n\n[16:15:14] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n\n\nXGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n              gamma=0, gpu_id=-1, importance_type=None,\n              interaction_constraints='', learning_rate=0.01, max_delta_step=0,\n              max_depth=3, min_child_weight=1, missing=nan,\n              monotone_constraints='()', n_estimators=500, n_jobs=40,\n              num_parallel_tree=1, predictor='auto', random_state=0,\n              reg_alpha=0, reg_lambda=1, scale_pos_weight=4, subsample=1,\n              tree_method='exact', validate_parameters=1, verbosity=None)\n\n\n\nsf.plot_evauate(trainX_3, trainY_3, testX_3, testY_3, xgb_3)\n\n\n\n\n\n\n\nì§€ë‚œ ê¸€ì˜ 0.595 ë³´ë‹¤ í–¥ìƒëœ ì„±ëŠ¥ì„ ë³´ì—¬ì£¼ì—ˆë‹¤.\n \n\n\n\n\nê°€ê²© ê´€ë ¨ ì»¬ëŸ¼: ì „ë‚  ì¢…ê°€ë¡œ ë‚˜ëˆ„ì–´ ìŠ¤ì¼€ì¼ë§\në‚˜ë¨¸ì§€ ì»¬ëŸ¼: ìŠ¤ì¼€ì¼ë§ í•˜ì§€ ì•ŠëŠ”ë‹¤.\n\n\ndef make_dataset_div_previous_close(trading):\n    from sklearn.preprocessing import MinMaxScaler\n    \n    col_price = ['Open', 'High', 'Low', 'Close', 'MA5', 'MA20', 'MA60', 'MA120', \n               'VMAP', 'BHB', 'BLB', 'KCH', 'KCL', 'KCM', 'DCH', 'DCL', 'DCM',\n               'SMA', 'EMA', 'WMA', 'Ichimoku', 'Parabolic SAR', 'KAMA','MACD']   \n\n    col_etc = ['Volume', 'MFI', 'ADI', 'OBV',\n           'CMF', 'FI', 'EOM, EMV', 'VPT', 'NVI', 'ATR', 'UI',\n           'ADX', '-VI', '+VI', 'TRIX', 'MI', 'CCI', 'DPO', 'KST',\n           'STC', 'RSI', 'SRSI', 'TSI', 'UO', 'SR',\n           'WR', 'AO', 'ROC', 'PPO', 'PVO', 'trading_value']\n    \n    # ì¢…ëª©ì½”ë“œ ë¶ˆëŸ¬ì˜¤ê¸° \n    IF = open('../data/code_list.txt')\n    lst_code = IF.readlines()\n\n    lst_X = []\n    lst_Y = []\n    lst_code_date = []\n    \n    db_dsml = pymysql.connect(\n        host = 'localhost', \n        port = 3306, \n        user = '[db username]', \n        passwd = '[db password]', \n        db = '[db name]', \n        charset = 'utf8'\n    )\n    cursor = db_dsml.cursor()\n    \n    \n    for code in tqdm(lst_code): \n        code = code.strip()\n        \n        sql_query = '''\n                    SELECT *\n                    FROM stock_{0}\n                    WHERE Date BETWEEN '2017-01-01' AND '2021-12-31'\n                    '''.format(code)\n\n        \n        stock = pd.read_sql(sql = sql_query, con = db_dsml) \n        stock['PrevClose'] = stock['Close'].shift(1) # ì „ ë‚  ì¢…ê°€ ì»¬ëŸ¼ ì¶”ê°€\n        stock.dropna(inplace=True)\n        stock = stock.reset_index(drop=True)\n        stock['trading_value'] = stock['Close'] * stock['Volume']\n        lst_stock = stock.values.tolist()\n\n        \n        # ğŸŒŸ scaling\n        # 1) ê°€ê²© ê´€ë ¨ ì»¬ëŸ¼ \n        df_price = stock[col_price]\n        df_price_scaled = df_price.apply(lambda x: x / stock['PrevClose'])\n        \n        # 2) ë‚˜ë¨¸ì§€ ì»¬ëŸ¼ \n        df_etc = stock[col_etc]\n        \n        df_scaled = pd.concat([df_price_scaled, df_etc], axis=1)\n        lst_stock_scaled = df_scaled.values.tolist()\n\n        \n        for idx, row in enumerate(lst_stock): \n            date, trading_value = row[0].date().strftime(\"%Y-%m-%d\"), row[-1]\n            if trading_value >= trading:\n                if (idx < 9) or (idx >= len(lst_stock)-1): # ì˜ˆì™¸ ì²˜ë¦¬ \n                    continue \n                \n                # D-9 ~ D0 ë°ì´í„°ë§Œ ë‹´ê¸° \n                sub_stock = lst_stock_scaled[idx-9:idx+1] \n\n                # 10ì¼ê°„ì˜ ë°ì´í„° \n                lst_result = []\n                for row2 in sub_stock:               \n                    lst_result += row2\n\n                # D+1 ì¢…ê°€ 2% ìƒìŠ¹ ì—¬ë¶€ \n                label = int(row[7] >= 0.02)\n                \n                # ì¢…ì†ë³€ìˆ˜, ë…ë¦½ë³€ìˆ˜, ì¢…ëª©ì½”ë“œ, ë‚ ì§œ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€ \n                lst_X.append(lst_result)\n                lst_Y.append(label)\n                lst_code_date.append([code, date])\n            \n    return pd.concat([pd.DataFrame(lst_code_date), pd.DataFrame(lst_X), pd.DataFrame(lst_Y)], axis=1)\n\n\ndf_dataset = make_dataset_div_previous_close(trading=1000000000)\n\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1561/1561 [03:53<00:00,  6.70it/s]\n\n\n\ntrainX_4, trainY_4, testX_4, testY_4, lst_code_date, lst_code_date_test = sf.split(df_dataset)\n\nprint('train dataset: ', trainX_4.shape, trainY_4.shape)\nprint('test dataset: ', testX_4.shape, testY_4.shape)\n\ntrain dataset:  (658364, 550) (658364,)\ntest dataset:  (239817, 550) (239817,)\n\n\n\nfrom xgboost import XGBClassifier\nxgb_4 = XGBClassifier(\n                   n_jobs=40,\n                   scale_pos_weight=4,\n                   learning_rate=0.01,\n                   max_depth=3,\n                   n_estimators=500,\n                   ) \n\nxgb_4.fit(trainX_4, trainY_4)\n\n[16:26:12] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n\n\nXGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n              gamma=0, gpu_id=-1, importance_type=None,\n              interaction_constraints='', learning_rate=0.01, max_delta_step=0,\n              max_depth=3, min_child_weight=1, missing=nan,\n              monotone_constraints='()', n_estimators=500, n_jobs=40,\n              num_parallel_tree=1, predictor='auto', random_state=0,\n              reg_alpha=0, reg_lambda=1, scale_pos_weight=4, subsample=1,\n              tree_method='exact', validate_parameters=1, verbosity=None)\n\n\n\nsf.plot_evauate(trainX_4, trainY_4, testX_4, testY_4, xgb_4)\n\n\n\n\n\n\n\nì§€ë‚œ ê¸€ 0.595 ë³´ë‹¤ ì„±ëŠ¥ í–¥ìƒì´ ëœ ê²ƒì„ í™•ì¸í•˜ì˜€ë‹¤.\n \n\n\n\n\nìŠ¤ì¼€ì¼ë§ì„ í†µí•´ ì´ì „ë³´ë‹¤ í¬ê²Œ ì„±ëŠ¥ í–¥ìƒì´ ë˜ì—ˆë‹¤. ì„œë¡œ ë‹¤ë¥¸ ì¢…ëª© ë³„ ê°€ê²©ì˜ ë²”ìœ„ë¥¼ ì¡°ì •í•´ ì£¼ëŠ” ì‘ì—…ì€ ì¤‘ìš”í•˜ë‹¤ëŠ” ê²ƒì„ í™•ì¸í•˜ì˜€ë‹¤. ìˆ˜ìµë¥  ì‹œë®¬ë ˆì´ì…˜ê¹Œì§€ í•´ ë³¸ í›„, ë„¤ ê°€ì§€ ë°©ë²• ì¤‘ ì§€ì†ì ìœ¼ë¡œ ì‚¬ìš©í•  ìŠ¤ì¼€ì¼ëŸ¬ë¥¼ ì„ íƒí•˜ë„ë¡ í•œë‹¤.\n\n\n\nsf.compute_earnings_rate(lst_code_date_test, xgb_1, testX_1, threshold=0.75)\n\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 202/202 [00:04<00:00, 42.12it/s]\n\n\nFinal earning rate : 462.37913 %\n\n\n\n\n\n\nsf.compute_earnings_rate(lst_code_date_test, xgb_2, testX_2, threshold=0.75)\n\n0it [00:00, ?it/s]\n\n\nFinal earning rate : 0.0 %\n\n\n\n\n\n\nsf.compute_earnings_rate(lst_code_date_test, xgb_3, testX_3, threshold=0.75)\n\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 222/222 [00:05<00:00, 43.23it/s]\n\n\nFinal earning rate : 124.50311 %\n\n\n\n\n\n\nsf.compute_earnings_rate(lst_code_date_test, xgb_4, testX_4, threshold=0.75)\n\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 260/260 [00:06<00:00, 42.78it/s]\n\n\nFinal earning rate : 1487.42557 %\n\n\n\n\n\n\nì˜ˆì¸¡í™•ë¥  ì„ê³„ê°’ì„ 0.75ë¡œ ì„¤ì •í•˜ì˜€ëŠ”ë°, 4) ì „ ë‚  ì¢…ê°€ë¡œ ë‚˜ëˆ„ê¸° ë°©ë²•ì˜ ìŠ¤ì¼€ì¼ë§ì´ ê°€ì¥ ë§¤ë§¤ ê°œìˆ˜ê°€ ë§ì•˜ë‹¤. 1ë…„ ë™ì•ˆ 130ë²ˆì˜ ë§¤ìˆ˜ê°€ ì¼ì–´ë‚¬ìŒì„ ì•Œ ìˆ˜ ìˆë‹¤. ê·¸ì™€ ë™ì‹œì— ìˆ˜ìµë¥  ì‹œë®¬ë ˆì´ì…˜ì—ì„œ ìˆ˜ìµë¥ ì´ ê°€ì¥ ë†’ê²Œ ë‚˜ì˜¨ ì „ ë‚  ì¢…ê°€ë¡œ ë‚˜ëˆ„ëŠ” ìŠ¤ì¼€ì¼ë§ ë°©ë²•ì„ ìµœì¢…ì ìœ¼ë¡œ ì„ íƒí•œë‹¤."
  },
  {
    "objectID": "posts/05.filtering.html",
    "href": "posts/05.filtering.html",
    "title": "[stock prediction] 2.3 CCIë¥¼ ì´ìš©í•œ ì£¼ê°€ ë°ì´í„° í•„í„°ë§",
    "section": "",
    "text": "ì´ì „ê¸€ 2.2. ì£¼ê°€ ë°ì´í„° ìŠ¤ì¼€ì¼ë§ ì—ì„œëŠ” í†µí•© ì¢…ëª© ì£¼ê°€ ë°ì´í„°ì…‹ì˜ ë¬¸ì œì ì¸ ì¢…ëª©ë§ˆë‹¤ ë‹¤ë¥¸ ê°€ê²©ì„ í‘œì¤€í™”í•´ì£¼ëŠ” ì‘ì—…ì„ í–ˆë‹¤. ë§ˆì§€ë§‰ ë°ì´í„° ì „ì²˜ë¦¬ ê³¼ì •ì€ CCIë¥¼ ì´ìš©í•œ ì£¼ê°€ ë°ì´í„° í•„í„°ë§ì´ë‹¤. ë§¤ì¼ ìŸì•„ì ¸ ë‚˜ì˜¤ëŠ” ë°©ëŒ€í•œ ì£¼ê°€ ë°ì´í„°ì…‹ìœ¼ë¡œë¶€í„° íŠ¹ì • íŒ¨í„´ì„ ê²€ì¶œí•´ë‚´ëŠ” ê²ƒì€ ì–´ë ¤ìš´ ê³¼ì œë‹¤. ë³¸ ì—°êµ¬ì—ì„œëŠ” ê·¸ëŸ¬í•œ ë¬¸ì œì ì„ ìµœì†Œí™”í•˜ê³ ì, CCI êµ¬ê°„ë³„ë¡œ ë°ì´í„°ë¥¼ í•„í„°ë§ í•˜ëŠ” ë°©ë²•ì„ ì‚¬ìš©í•œë‹¤. ì´ë²ˆ ê¸€ì—ì„œëŠ” CCIê°€ ë¬´ì—‡ì¸ì§€ ì•Œì•„ë³´ê³ , CCIë¥¼ 3ê°€ì§€ êµ¬ê°„ìœ¼ë¡œ ë‚˜ëˆ„ì–´ í•„í„°ë§ í•œ ë°ì´í„°ë¡œ XGBoost ëª¨ë¸í•™ìŠµì„ ì§„í–‰í•œë‹¤.\n\n\n\n\n\nCCIë€?\n\n\nCCIë¥¼ í™œìš©í•œ ì£¼ê°€ë°ì´í„° í•„í„°ë§\n\n\n\n\nimport pandas as pd\nimport numpy as np\nfrom tqdm import tqdm\nimport pymysql\n\nimport warnings\nwarnings.filterwarnings('ignore')\n    \nimport ta\n\nfrom ipywidgets import interact, interact_manual\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.graph_objects as go\nimport plotly.offline as pyo\n%matplotlib inline\n%pylab inline\npylab.rcParams['figure.figsize'] = (12,5)\n\nimport StockFunc as sf\n\n%pylab is deprecated, use %matplotlib inline and import the required libraries.\nPopulating the interactive namespace from numpy and matplotlib\n\n\n \n\n\n\n\nCCI (ìƒí’ˆ ì±„ë„ ì§€ìˆ˜: Commodity Chnnel Index) ëŠ” ì¼ì • ê¸°ê°„ ë™ì•ˆ ì£¼ê°€ì˜ í‰ê· ê°’ì—ì„œ ì–¼ë§ˆë‚˜ ë–¨ì–´ì¡ŒëŠ”ì§€ë¥¼ ë³´ì—¬ì£¼ëŠ” ì¶”ì„¸ ì§€í‘œì´ë©°, ì¶”ì„¸ì˜ ë°©í–¥ê³¼ ê°•ë„ë¥¼ ì•Œ ìˆ˜ ìˆë‹¤. ì¶”ì„¸ì˜ ê°•ë„ë§Œì„ ì•Œë ¤ì£¼ëŠ” ADXì— ë¹„í•´ ì¶”ì„¸ì˜ ë°©í–¥ê¹Œì§€ ë™ì‹œì— ì•Œë ¤ì£¼ê¸° ë•Œë¬¸ì— ì¶”ì„¸ ì¶”ì¢…í˜• ê±°ë˜ìë“¤ì—ê²Œ ìœ ìš©í•œ ì§€í‘œë¡œ ì‚¬ìš©ë˜ê³  ìˆê³ , ì¶”ì„¸ì§€í‘œì§€ë§Œ ë³€ë™ì„± ì§€í‘œì²˜ëŸ¼ ì‚¬ìš©ë˜ê¸°ë„ í•œë‹¤.\n\nê³„ì‚°ì‹\n\n\ní•´ì„ ë°©ë²•\n\nCCI ê°’ì´ 0ì´ë©´ í˜„ì¬ ì£¼ê°€ê°€ ì´ë™ í‰ê· ì„ ê³¼ ì¼ì¹˜í•œë‹¤ëŠ” ëœ»\nCCI ê°’ì´ +ì´ë©´ ìƒìŠ¹ì¶”ì„¸, -ì´ë©´ í•˜ë½ì¶”ì„¸ì´ë‹¤.\nCCI ê°’ì´ í´ìˆ˜ë¡ ì¶”ì„¸ì˜ ê°•ë„ê°€ ê°•í•˜ê³ , ì‘ì„ìˆ˜ë¡ ì¶”ì„¸ì˜ ê°•ë„ê°€ ì•½í•˜ë‹¤.\nCCIì˜ ì ˆëŒ“ê°’ì´ 100 ì´ìƒìœ¼ë¡œ í¬ë‹¤ëŠ” ê²ƒì€ ê°€ê²©ì´ ê¸‰ë³€í•˜ì˜€ë‹¤ëŠ” ê²ƒì„ ì˜ë¯¸í•œë‹¤. (ê³¼ë§¤ìˆ˜/ê³¼ë§¤ë„ êµ¬ê°„)\n\n\n[ì°¸ê³ ]\n-[ì£¼ì‹íˆ¬ì] Chart ë³´ì¡° ì§€í‘œ CCI ë€?\n-ì£¼ì‹ìš©ì–´ ë…íŒŒ [25] CCIì˜ ì˜ë¯¸ì™€ ì‹¤ì „ í™œìš©ë²•\n \n\n\n\në³¸ ì—°êµ¬ì—ì„œëŠ” ì•ì„œ ì‚´í´ë³¸ ë³´ì¡°ì§€í‘œ CCIì˜ ë²”ìœ„ë¥¼ 3ê°€ì§€ êµ¬ê°„ìœ¼ë¡œ ë‚˜ëˆ„ì–´ ë°ì´í„°ë¥¼ í•„í„°ë§ í•©ë‹ˆë‹¤. ì´ ë•Œ, ì´ë™í‰ê· ì„ ìœ¼ë¡œë¶€í„° ê·¼ì ‘í•œ 1) ì¤‘ë¦½êµ¬ê°„, ê·¹ë‹¨ì ìœ¼ë¡œ ë–¨ì–´ì§„ 2) ê³¼ì—´êµ¬ê°„/ê³¼ë§¤ìˆ˜êµ¬ê°„ ê³¼ 3) ì¹¨ì²´ êµ¬ê°„/ê³¼ë§¤ë„êµ¬ê°„ìœ¼ë¡œ ë‚˜ëˆ•ë‹ˆë‹¤.\në‚˜ëˆ„ëŠ” CCIì˜ êµ¬ì²´ì ì¸ êµ¬ê°„ì€ ë‹¤ìŒê³¼ ê°™ë‹¤.\n1) ì¤‘ë¦½êµ¬ê°„ - CCI : (-20, 20)\n2) ê³¼ì—´êµ¬ê°„ - CCI : (100, \\(\\infty\\))\n3) ì¹¨ì²´ êµ¬ê°„ - CCI : (-\\(\\infty\\),-100)\n\nìŠ¤ì¼€ì¼ë§ (div-prev_close) + CCI êµ¬ê°„ í•„í„°ë§ ë°ì´í„°ì…‹ ìƒì„± í•¨ìˆ˜\n\n\ndef make_dataset_cci(trading, cci_d='_', cci_u='_', train=True):\n    from sklearn.preprocessing import MinMaxScaler\n    \n    col_price = ['Open', 'High', 'Low', 'Close', 'trading_value', 'MA5', 'MA20', 'MA60', 'MA120', \n               'VMAP', 'BHB', 'BLB', 'KCH', 'KCL', 'KCM', 'DCH', 'DCL', 'DCM',\n               'SMA', 'EMA', 'WMA', 'Ichimoku', 'Parabolic SAR', 'KAMA','MACD']   \n\n    col_etc = ['Change', 'Volume', 'MFI', 'ADI', 'OBV',\n           'CMF', 'FI', 'EOM, EMV', 'VPT', 'NVI', 'ATR', 'UI',\n           'ADX', '-VI', '+VI', 'TRIX', 'MI', 'CCI', 'DPO', 'KST',\n           'STC', 'RSI', 'SRSI', 'TSI', 'UO', 'SR',\n           'WR', 'AO', 'ROC', 'PPO', 'PVO']\n    \n    # ì¢…ëª©ì½”ë“œ ë¶ˆëŸ¬ì˜¤ê¸° \n    IF = open('../data/code_list.txt')\n    lst_code = IF.readlines()\n\n    lst_X = []\n    lst_Y = []\n    lst_code_date = []\n        \n    db_dsml = pymysql.connect(\n        host = 'localhost', \n        port = 3306, \n        user = '[db username]', \n        passwd = '[db password]', \n        db = '[db name]', \n        charset = 'utf8'\n    )\n    cursor = db_dsml.cursor()\n    \n    temp_cci_d, temp_cci_u = cci_d, cci_u # ğŸŒŸ ì´ˆê¸° cci ë²”ìœ„ ì €ì¥ \n    \n    for code in tqdm(lst_code): \n        code = code.strip()\n        \n        if train:\n            sql_query = '''\n                        SELECT *\n                        FROM stock_{}\n                        WHERE Date BETWEEN '2017-01-01' AND '2020-12-31'\n                        '''.format(code)\n        else: \n            sql_query = '''\n                        SELECT *\n                        FROM stock_{}\n                        WHERE Date BETWEEN '2021-01-01' AND '2021-12-31'\n                        '''.format(code)\n            \n        \n        stock = pd.read_sql(sql = sql_query, con = db_dsml) \n        stock['PrevClose'] = stock['Close'].shift(1) # ì „ ë‚  ì¢…ê°€ ì»¬ëŸ¼ ì¶”ê°€\n        stock.dropna(inplace=True)\n        stock = stock.reset_index(drop=True)\n        stock['trading_value'] = stock['Close'] * stock['Volume']\n        lst_stock = stock.values.tolist()\n\n        if temp_cci_d == '_': \n            cci_d = stock['CCI'].min()\n        if temp_cci_u == '_':\n            cci_u = stock['CCI'].max()    \n        \n        # scaling\n        # 1) ê°€ê²© ê´€ë ¨ ì»¬ëŸ¼ \n        df_price = stock[col_price]\n        df_price_scaled = df_price.apply(lambda x: x / stock['PrevClose'])\n        \n        # 2) ë‚˜ë¨¸ì§€ ì»¬ëŸ¼ \n        df_etc = stock[col_etc]\n        \n        df_scaled = pd.concat([df_price_scaled, df_etc], axis=1)\n        lst_stock_scaled = df_scaled.values.tolist()\n\n        \n        for idx, row in enumerate(lst_stock): \n            date, trading_value, cci_ = row[0].date().strftime(\"%Y-%m-%d\"), row[-1], row[40]\n            # ğŸŒŸ cci ì¡°ê±´ ì„¤ì • \n            if (trading_value >= trading) & ((cci_ >= cci_d) & (cci_ <= cci_u)):\n                if (idx < 9): # ì˜ˆì™¸ ì²˜ë¦¬ \n                    continue \n                \n                # D-9 ~ D0 ë°ì´í„°ë§Œ ë‹´ê¸° \n                sub_stock = lst_stock_scaled[idx-9:idx+1] \n\n                # 10ì¼ê°„ì˜ ë°ì´í„° \n                lst_result = []\n                for row2 in sub_stock:               \n                    lst_result += row2\n\n                # D+1 ì¢…ê°€ 2% ìƒìŠ¹ ì—¬ë¶€ \n                label = int(row[7] >= 0.02)\n                \n                # ì¢…ì†ë³€ìˆ˜, ë…ë¦½ë³€ìˆ˜, ì¢…ëª©ì½”ë“œ, ë‚ ì§œ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€ \n                lst_X.append(lst_result)\n                lst_Y.append(label)\n                lst_code_date.append([code, date])\n            \n    return np.array(lst_X), np.array(lst_Y), np.array(lst_code_date) \n\n\n\n\nì¤‘ë¦½êµ¬ê°„ì—ì„œ ë°ì´í„°ì…‹ì˜ ê°œìˆ˜ëŠ” train: 123321, test:36885 ê°œë¡œ, ì„¸ êµ¬ê°„ ì¤‘ ê°€ì¥ ì ì€ ë°ì´í„°ì…‹ì˜ ê°œìˆ˜ê°€ ë‚˜ì™”ë‹¤. ì •í™•ë„ëŠ” ê°€ì¥ ë†’ì§€ë§Œ, auc scoreëŠ” ë¯¸ë¯¸í•œ ê²°ê³¼ë¥¼ ë³´ì˜€ë‹¤.\n\ntrainX_1, trainY_1, lst_code_date_1 = make_dataset_cci(trading=100000000, cci_d=-20, cci_u=20, train=True)\ntestX_1, testY_1, lst_code_date_test_1 = make_dataset_cci(trading=100000000, cci_d=-20, cci_u=20, train=False)\n\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1561/1561 [02:12<00:00, 11.77it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1561/1561 [00:42<00:00, 36.60it/s]\n\n\n\nprint('train dataset: ', trainX_1.shape, trainY_1.shape)\nprint('test dataset: ', testX_1.shape, testY_1.shape)\n\ntrain dataset:  (123321, 560) (123321,)\ntest dataset:  (36885, 560) (36885,)\n\n\n\nfrom xgboost import XGBClassifier\nxgb_1 = XGBClassifier(\n                   n_jobs=40,\n                   scale_pos_weight=4,\n                   learning_rate=0.01,\n                   max_depth=3,\n                   n_estimators=800,\n                   ) \n\nxgb_1.fit(trainX_1, trainY_1)\n\nXGBClassifier(base_score=0.5, booster='gbtree', callbacks=None,\n              colsample_bylevel=1, colsample_bynode=1, colsample_bytree=1,\n              early_stopping_rounds=None, enable_categorical=False,\n              eval_metric=None, gamma=0, gpu_id=-1, grow_policy='depthwise',\n              importance_type=None, interaction_constraints='',\n              learning_rate=0.01, max_bin=256, max_cat_to_onehot=4,\n              max_delta_step=0, max_depth=3, max_leaves=0, min_child_weight=1,\n              missing=nan, monotone_constraints='()', n_estimators=800,\n              n_jobs=40, num_parallel_tree=1, predictor='auto', random_state=0,\n              reg_alpha=0, reg_lambda=1, ...)\n\n\n\nsf.plot_evauate(trainX_1, trainY_1, testX_1, testY_1, xgb_1)\n\n\n\n\n\n\n\n\n\n\n\nì¹¨ì²´êµ¬ê°„ì—ì„œ ë°ì´í„°ì…‹ì˜ ê°œìˆ˜ëŠ” train: 256399, test:64768 ê°œë¡œ, ê½¤ ë§ì€ ë°ì´í„°ì…‹ì˜ ê°œìˆ˜ê°€ ë‚˜ì™”ë‹¤. ì„¸ êµ¬ê°„ ì¤‘ ê°€ì¥ ë‚®ì€ ë¶„ë¥˜ ì„±ëŠ¥ì„ ë³´ì˜€ë‹¤.\n\ntrainX_2, trainY_2, lst_code_date_2 = make_dataset_cci(trading=100000000, cci_d=100, train=True)\ntestX_2, testY_2, lst_code_date_test_2 = make_dataset_cci(trading=100000000, cci_d=100, train=False)\n\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1561/1561 [02:16<00:00, 11.44it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1561/1561 [00:42<00:00, 36.67it/s]\n\n\n\nprint('train dataset: ', trainX_2.shape, trainY_2.shape)\nprint('test dataset: ', testX_2.shape, testY_2.shape)\n\ntrain dataset:  (256399, 560) (256399,)\ntest dataset:  (64768, 560) (64768,)\n\n\n\nfrom xgboost import XGBClassifier\nxgb_2 = XGBClassifier(\n                   n_jobs=40,\n                   scale_pos_weight=4,\n                   learning_rate=0.01,\n                   max_depth=3,\n                   n_estimators=800,\n                   ) \n\nxgb_2.fit(trainX_2, trainY_2)\n\nXGBClassifier(base_score=0.5, booster='gbtree', callbacks=None,\n              colsample_bylevel=1, colsample_bynode=1, colsample_bytree=1,\n              early_stopping_rounds=None, enable_categorical=False,\n              eval_metric=None, gamma=0, gpu_id=-1, grow_policy='depthwise',\n              importance_type=None, interaction_constraints='',\n              learning_rate=0.01, max_bin=256, max_cat_to_onehot=4,\n              max_delta_step=0, max_depth=3, max_leaves=0, min_child_weight=1,\n              missing=nan, monotone_constraints='()', n_estimators=800,\n              n_jobs=40, num_parallel_tree=1, predictor='auto', random_state=0,\n              reg_alpha=0, reg_lambda=1, ...)\n\n\n\nsf.plot_evauate(trainX_2, trainY_2, testX_2, testY_2, xgb_2)\n\n\n\n\n\n\n\n\n\n\n\nì¹¨ì²´êµ¬ê°„ì—ì„œ ë°ì´í„°ì…‹ì˜ ê°œìˆ˜ëŠ” train: 269977, test:73435 ê°œë¡œ ê°€ì¥ ë§ì€ ë°ì´í„°ì˜ ê°œìˆ˜ê°€ ë‚˜ì™”ìœ¼ë©°, auc scoreë¥¼ ë´¤ì„ ë•Œ, ê°€ì¥ ë†’ì€ ë¶„ë¥˜ ì„±ëŠ¥ì„ ë³´ì˜€ë‹¤.\n\ntrainX_3, trainY_3, lst_code_date_3 = make_dataset_cci(trading=100000000, cci_u=-100, train=True)\ntestX_3, testY_3, lst_code_date_test_3 = make_dataset_cci(trading=100000000, cci_u=-100, train=False)\n\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1561/1561 [02:16<00:00, 11.45it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1561/1561 [00:43<00:00, 36.16it/s]\n\n\n\nprint('train dataset: ', trainX_3.shape, trainY_3.shape)\nprint('test dataset: ', testX_3.shape, testY_3.shape)\n\ntrain dataset:  (269977, 560) (269977,)\ntest dataset:  (73435, 560) (73435,)\n\n\n\nfrom xgboost import XGBClassifier\nxgb_3 = XGBClassifier(\n                   n_jobs=40,\n                   scale_pos_weight=4,\n                   learning_rate=0.01,\n                   max_depth=3,\n                   n_estimators=800,\n                   ) \n\nxgb_3.fit(trainX_3, trainY_3)\n\nXGBClassifier(base_score=0.5, booster='gbtree', callbacks=None,\n              colsample_bylevel=1, colsample_bynode=1, colsample_bytree=1,\n              early_stopping_rounds=None, enable_categorical=False,\n              eval_metric=None, gamma=0, gpu_id=-1, grow_policy='depthwise',\n              importance_type=None, interaction_constraints='',\n              learning_rate=0.01, max_bin=256, max_cat_to_onehot=4,\n              max_delta_step=0, max_depth=3, max_leaves=0, min_child_weight=1,\n              missing=nan, monotone_constraints='()', n_estimators=800,\n              n_jobs=40, num_parallel_tree=1, predictor='auto', random_state=0,\n              reg_alpha=0, reg_lambda=1, ...)\n\n\n\nsf.plot_evauate(trainX_3, trainY_3, testX_3, testY_3, xgb_3)\n\n\n\n\n\n\n\n\në°ì´í„°ì…‹ ì €ì¥\n\nê° í•™ìŠµ, ì‹œí—˜ ë°ì´í„°ì…‹ê³¼ XGBoost ëª¨ë¸ì„ pickle íŒŒì¼ë¡œ ì €ì¥í•œë‹¤.\n\n#collapse-hide\nimport pickle \ndic_dataset_model = {'CCI -20~20': [trainX_1, trainY_1, testX_1, testY_1, lst_code_date_1, lst_code_date_test_1, xgb_1],\n                'CCI 100~': [trainX_2, trainY_2, testX_2, testY_2, lst_code_date_2, lst_code_date_test_2, xgb_2],\n                'CCI ~-100': [trainX_3, trainY_3, testX_3, testY_3, lst_code_date_3, lst_code_date_test_3, xgb_3]}\n\nwith open('dataset_cci_filtering.pickle', 'wb') as handle:\n    pickle.dump(dic_dataset_model, handle, protocol=pickle.HIGHEST_PROTOCOL)\n\n\në‹¤ìŒ ê¸€ì—ì„œëŠ” ìœ„ì—ì„œ CCI êµ¬ê°„ë³„ë¡œ í•™ìŠµí•œ XGBoost ëª¨ë¸ì— XAI ê¸°ë²• ì¤‘ í•˜ë‚˜ì¸ SHAP valueë¥¼ ì ìš©í•˜ì—¬ ë°ì´í„°ë¥¼ ë³€í™˜í•˜ê³ , ê° êµ¬ê°„ë§ˆë‹¤ summary plotì„ ê·¸ë ¤ë³´ë©° ì¤‘ìš”í•œ íŠ¹ì„±ê³¼ ê·¸ì— ëŒ€í•œ í•´ì„ì„ ì§„í–‰í•˜ë„ë¡ í•œë‹¤."
  },
  {
    "objectID": "posts/06.shap_value.html",
    "href": "posts/06.shap_value.html",
    "title": "[stock prediction] 3.1. ì„¤ëª…ê°€ëŠ¥ AI (XAI), SHAP value",
    "section": "",
    "text": "ì´ì „ ê¸€ ( [Stock Research] 2.3. CCIë¥¼ ì´ìš©í•œ ì£¼ê°€ ë°ì´í„° í•„í„°ë§) ê¹Œì§€ ë°ì´í„° ì „ì €ë¦¬ë¥¼ í†µí•´ baseline modelì— ë¹„í•´ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¬ ìˆ˜ ìˆì—ˆë‹¤. ì´ë²ˆ ê¸€ì—ì„œëŠ” ì„¤ëª…ê°€ëŠ¥ AI ê¸°ë²•ì¤‘ì—ì„œë„ SHAP valueê°€ ë¬´ì—‡ì¸ì§€ ì•Œì•„ë³´ê³ , ì•ì„œ í•™ìŠµí–ˆë˜ CCI êµ¬ê°„ ë³„ XGBoost ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ SHAP valueë¥¼ ê³„ì‚°í•¨ìœ¼ë¡œì¨ ì£¼ê°€ ì´ì§„ë¶„ë¥˜ ì˜ˆì¸¡ì— ì˜í–¥ì„ ë¯¸ì¹œ ì¤‘ìš” ë³€ìˆ˜ë¥¼ í•´ì„í•´ ë³¸ë‹¤. ì—¬ê¸°ì„œ summary plotì˜ ê²°ê³¼ë¡œ ë‚˜ì˜¨ ìƒìœ„ ì¤‘ìš” ë³€ìˆ˜ë¡œ ë°ì´í„°ë¥¼ í•„í„°ë§í•˜ì—¬ íŠ¹ì • ì§‘ë‹¨ì„ êµ¬ì„±í•˜ê³ , ê³µí†µëœ íŠ¹ì§•ì´ ìˆëŠ”ì§€ í™•ì¸í•œë‹¤.\n\n\n\n\nì„¤ëª…ê°€ëŠ¥ AI (XAI), SHAP valueë€?\n\n\nCCI êµ¬ê°„ ë³„ summary plot ë¹„êµ\n\n\n\n\në¼ì´ë¸ŒëŸ¬ë¦¬ import\n\n\nimport pandas as pd\nimport numpy as np\nfrom tqdm import tqdm\nimport pymysql\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom ipywidgets import interact, interact_manual\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.graph_objects as go\nimport plotly.offline as pyo\n%matplotlib inline\n%pylab inline\npylab.rcParams['figure.figsize'] = (12,5)\n\nimport StockFunc as sf\n\n%pylab is deprecated, use %matplotlib inline and import the required libraries.\nPopulating the interactive namespace from numpy and matplotlib\n\n\n \n\n\n\n\n\n\nì˜¤ëŠ˜ë‚  ì¸ê³µì§€ëŠ¥ì˜ ì„±ëŠ¥ì´ ì¢‹ì•„ì§ì— ë”°ë¼ ëª¨ë¸ì˜ ë³µì¡ë„ ë˜í•œ ë†’ì•„ì§€ê²Œ ë˜ì—ˆë‹¤. ì—¬ê¸°ì„œ ì˜ˆì¸¡ ê²°ê³¼ì— ëŒ€í•œ ëª¨ë¸ì˜ ì‹ ë¢°ì„± ë¬¸ì œê°€ ë”°ë¼ì˜¤ê²Œëœë‹¤. ì„¤ëª…ê°€ëŠ¥ AIëŠ” ì´ëŸ¬í•œ ë¬¸ì œì ì„ í•´ê²°í•˜ì—¬ ì¸ê³µì§€ëŠ¥ ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œ ì‘ì„±ëœ ê²°ê³¼ì™€ ì¶œë ¥ì„ ì¸ê°„ì¸ ì‚¬ìš©ìê°€ ì´í•´í•˜ê³  ì‹ ë¢°í•  ìˆ˜ ìˆë„ë¡ í•´ì¤€ë‹¤.\në³¸ í”„ë¡œì íŠ¸ì—ì„œëŠ” ì„¤ëª…ê°€ëŠ¥ AI ê¸°ë²• ì¤‘ì—ì„œ shap valueë¥¼ í™œìš©í•˜ì—¬ ë³€ìˆ˜ ì¤‘ìš”ë„ë¥¼ íŒŒì•…í•˜ê³ , ë³€í™˜ëœ SHAP í‘œì¤€í™” ë°ì´í„°ì…‹ì„ ì´ìš©í•œ ì—°êµ¬ë¥¼ ì§„í–‰í•œë‹¤. SHAP(SHAPley Additional Descriptions)ëŠ” ëª¨ë“  ê¸°ê³„ í•™ìŠµ ëª¨ë¸ì˜ ì¶œë ¥ì„ ì„¤ëª…í•˜ê¸° ìœ„í•œ ê²Œì„ ì´ë¡ ì  ì ‘ê·¼ë²•ì´ë‹¤. ê²Œì„ ì´ë¡ ì˜ ê³ ì „ì ì¸ shapley valueë¥¼ ê³„ì‚°í•˜ì—¬ ì…ë ¥ ë³€ìˆ˜ì™€ ëª¨ë¸ì˜ ê²°ê³¼ê°’ ì‚¬ì´ì˜ ê´€ê³„ë¥¼ ë¶„ì„í•˜ëŠ” XAI ê¸°ë²• ì¤‘ í•˜ë‚˜ì´ë‹¤.\n*shapley value\n-ê° ë³€ìˆ˜ê°€ ì˜ˆì¸¡ ê²°ê³¼ë¬¼ì— ì£¼ëŠ” ì˜í–¥ë ¥ì˜ í¬ê¸°\n-í•´ë‹¹ ë³€ìˆ˜ê°€ ì–´ë–¤ ì˜í–¥ì„ ì£¼ëŠ”ì§€ íŒŒì•…\n- (ex)\n- ê° ì„ ìˆ˜ê°€ íŒ€ ì„±ì ì— ì£¼ëŠ” ì˜í–¥ë ¥ í¬ê¸°\n- í•´ë‹¹ ì„ ìˆ˜ê°€ ì–´ë– í•œ ì˜í–¥ì„ ì£¼ëŠ”ê°€\n- (ì„ ìˆ˜ Aê°€ ìˆëŠ” íŒ€ Bì˜ ìŠ¹ë¥ ) - (ì„ ìˆ˜ Aê°€ ì—†ëŠ” íŒ€ Bì˜ ìŠ¹ë¥ ) = 7%\n-> â€œì„ ìˆ˜ AëŠ” íŒ€ ìŠ¹ë¥ ì— 7% ë§Œí¼ì˜ ì˜í–¥ë ¥ì´ ìˆë‹¤.â€\n\n[ì°¸ê³ ]\n-ì„¤ëª… ê°€ëŠ¥í•œ AI\n-shap github\n-A Unified Approach to Interpreting Model Predictions\n \n\n\n\n\n\në°ì´í„°ì…‹ ë¶ˆëŸ¬ì˜¤ê¸°\n\n\n#collapse-hide\nimport pickle\nwith open('dataset_cci_filtering.pickle', 'rb') as handle:\n    dic_dataset_model = pickle.load(handle)\n\n\nì»¬ë¦¼ ë¦¬ìŠ¤íŠ¸ ìƒì„±\n\n\n#collapse-hide\nlst_col_info = []\n\nlst_col = ['Open', 'High', 'Low', 'Close', 'trading_value', 'MA5', 'MA20', 'MA60', 'MA120', \n           'VMAP', 'BHB', 'BLB', 'KCH', 'KCL', 'KCM', 'DCH', 'DCL', 'DCM',\n           'SMA', 'EMA', 'WMA', 'Ichimoku', 'Parabolic SAR', 'KAMA','MACD']   +  ['Change', 'Volume', 'MFI', 'ADI', 'OBV', \n                                                                                   'CMF', 'FI', 'EOM, EMV', 'VPT', 'NVI', 'ATR', 'UI',\n                                                                                   'ADX', '-VI', '+VI', 'TRIX', 'MI', 'CCI', 'DPO', 'KST', \n                                                                                   'STC', 'RSI', 'SRSI', 'TSI', 'UO', 'SR',\n                                                                                   'WR', 'AO', 'ROC', 'PPO', 'PVO'] \n           \n\nfor day in range(9, -1, -1): \n    for col in lst_col: \n        lst_col_info.append(f'D-{day}_{col}')\n\n\n\n\n\nì¤‘ë¦½êµ¬ê°„ ë°ì´í„°ì…‹ ë¶ˆëŸ¬ì˜¤ê¸°\n\n\ntrainX_1, trainY_1, testX_1, testY_1, lst_code_date_1, lst_code_date_test_1, xgb_1 = dic_dataset_model['CCI -20~20']\nprint('train dataset: ', trainX_1.shape, trainY_1.shape)\nprint('test dataset: ', testX_1.shape, testY_1.shape)\n\ntrain dataset:  (123321, 560) (123321,)\ntest dataset:  (36885, 560) (36885,)\n\n\n\nnumpy -> DataFrame\n\n\n##### train\ndf_trainX_1 = pd.DataFrame(trainX_1)\ndf_trainX_1.columns = lst_col_info\ndf_trainY_1 = pd.DataFrame(trainY_1)\n\n##### test \ndf_testX_1 = pd.DataFrame(testX_1)\ndf_testX_1.columns = lst_col_info\ndf_testY_1 = pd.DataFrame(testY_1)\n\ndf_trainX_1.head()\n\n\n\n\n\n  \n    \n      \n      D-9_Open\n      D-9_High\n      D-9_Low\n      D-9_Close\n      D-9_trading_value\n      D-9_MA5\n      D-9_MA20\n      D-9_MA60\n      D-9_MA120\n      D-9_VMAP\n      ...\n      D-0_RSI\n      D-0_SRSI\n      D-0_TSI\n      D-0_UO\n      D-0_SR\n      D-0_WR\n      D-0_AO\n      D-0_ROC\n      D-0_PPO\n      D-0_PVO\n    \n  \n  \n    \n      0\n      1.005714\n      1.048000\n      1.000000\n      1.040000\n      462369.440000\n      0.994514\n      0.968971\n      0.981010\n      1.040524\n      0.993304\n      ...\n      52.767201\n      0.057077\n      20.137832\n      43.856662\n      46.236559\n      -53.763441\n      433.235294\n      2.564103\n      1.624561\n      -3.039166\n    \n    \n      1\n      1.000000\n      1.011236\n      0.974157\n      0.988764\n      94755.235955\n      1.004719\n      0.987640\n      0.954307\n      1.015178\n      1.002500\n      ...\n      58.029378\n      0.779487\n      6.199506\n      48.326356\n      75.903614\n      -24.096386\n      53.588235\n      -0.885936\n      0.530558\n      2.429462\n    \n    \n      2\n      1.007463\n      1.012793\n      0.996802\n      1.005330\n      105709.495736\n      0.974840\n      0.944350\n      0.910856\n      0.953198\n      0.950598\n      ...\n      46.479620\n      0.000000\n      16.267954\n      46.534386\n      14.444444\n      -85.555556\n      343.500000\n      0.898876\n      1.645809\n      10.986297\n    \n    \n      3\n      1.007423\n      1.033934\n      0.975610\n      1.033934\n      327239.156946\n      0.987063\n      0.943160\n      0.907971\n      0.948206\n      0.958880\n      ...\n      52.721907\n      0.225976\n      14.411703\n      44.991463\n      40.909091\n      -59.090909\n      289.705882\n      1.651982\n      1.484638\n      9.126668\n    \n    \n      4\n      0.994872\n      0.994872\n      0.950769\n      0.974359\n      168981.128205\n      0.966974\n      0.914769\n      0.879402\n      0.917145\n      0.932738\n      ...\n      49.489503\n      0.108960\n      11.827189\n      44.008782\n      26.136364\n      -73.863636\n      221.794118\n      -2.985075\n      1.229302\n      5.503306\n    \n  \n\n5 rows Ã— 560 columns\n\n\n\n\nSHAP í‘œì¤€í™”\n\n\nimport shap\nshap.initjs()\nexplainer = shap.TreeExplainer(xgb_1)\nshap_values_1 = explainer.shap_values(df_trainX_1) # train \nshap_values_test_1 = explainer.shap_values(df_testX_1) # test \n\n\n\n\n\n\n\n\nsummary plot í•´ì„ ë°©ë²•\n\nSummary plotì—ì„œ Xì¶•ì€ SHAP ê°’ìœ¼ë¡œ, ëª¨ë¸ ì˜ˆì¸¡ ê°’ì— ì˜í–¥ì„ ì¤€ ì •ë„ì˜ ìˆ˜ì¹˜ë¥¼ ì˜ë¯¸í•œë‹¤. (-1, 1) ì‚¬ì´ì˜ ê°’ì´ë©° ì˜í–¥ë ¥ì´ ì—†ì„ ìˆ˜ë¡ 0ì— ê°€ê¹Œìš´ ê°’ì´ë‹¤. ì´ëŸ¬í•œ SHAP valueê°€ ì–‘ìˆ˜ ê°’ì´ë©´ ê¸ì •ì ì¸ ì˜í–¥ (ì–‘ì˜ ì˜í–¥), ìŒìˆ˜ ê°’ì´ë©´ ë¶€ì •ì ì¸ ì˜í–¥ (ìŒì˜ ì˜í–¥)ì„ ë¼ì³¤ìŒì„ ëœ»í•œë‹¤. í•´ë‹¹ ëª¨ë¸ì—ì„œëŠ” ì–‘ìˆ˜ì˜ ê°’ì´ í´ ìˆ˜ë¡ label 1ë¡œ ì˜ˆì¸¡í•˜ëŠ” ë° ì˜í–¥ì„ ë§ì´ ë¯¸ì³¤ë‹¤ëŠ” ëœ»ìœ¼ë¡œ í•´ì„í•  ìˆ˜ ìˆë‹¤. Yì¶•ì€ ì„¤ëª… ë³€ìˆ˜ì´ê³ , ìƒ‰ê¹”ì€ ì„¤ëª… ë³€ìˆ˜ì˜ ê°œë³„ ë°ì´í„° ê°’ì˜ í¬ê¸°ë¥¼ ë§í•œë‹¤. ë¹¨ê°„ìƒ‰ì¼ ìˆ˜ë¡ ë°ì´í„°ì˜ ê°’ì´ ìƒëŒ€ì ìœ¼ë¡œ í¬ê³ , íŒŒë€ìƒ‰ì¼ ìˆ˜ë¡ ê°’ì´ ìƒëŒ€ì ìœ¼ë¡œ ì‘ì€ ë°ì´í„°ì„ì„ ëœ»í•œë‹¤.\n\nshap.summary_plot(shap_values_1, df_trainX_1)\n\n\n\n\n\n\n\n\nshap.summary_plot(shap_values_test_1, df_testX_1)\n\n\n\n\nì¤‘ë¦½êµ¬ê°„ì—ì„œ train, test ë°ì´í„°ì…‹ì˜ ì¤‘ìš” ë³€ìˆ˜ ìƒìœ„ 6ê°œê°€ ë™ì¼í•˜ê²Œ ë‚˜ì™”ìŒì„ ì•Œ ìˆ˜ ìˆë‹¤. ì¤‘ìš”ë„ ìƒìœ„ ì„¤ëª… ë³€ìˆ˜ 3ê°œë¥¼ í•´ì„í•´ë³´ë©´, D-0_KCHì˜ ê°’ì´ í´ ìˆ˜ë¡ ì–‘ì˜ ì˜í–¥ (ë‹¤ìŒ ë‚  ì¢…ê°€ 2% ìƒìŠ¹í•  ê²ƒì´ë¼ ì˜ˆì¸¡í•˜ëŠ” ë° ì˜í–¥)ì„ ë¯¸ì³¤ìœ¼ë©°, D-0_DCL ì˜ ê°’ì´ ì‘ì„ ìˆ˜ë¡ ì–‘ì˜ ì˜í–¥, D-0_KCLì˜ ê°’ì´ ì‘ì„ ìˆ˜ë¡ ì–‘ì˜ ì˜í–¥ì„ ë¼ì³¤ë‹¤.\n\n\n\n\n\n\nê³¼ë§¤ìˆ˜êµ¬ê°„ ë°ì´í„°ì…‹ ë¶ˆëŸ¬ì˜¤ê¸°\n\n\ntrainX_2, trainY_2, testX_2, testY_2, lst_code_date_2, lst_code_date_test_2, xgb_2 = dic_dataset_model['CCI 100~']\nprint('train dataset: ', trainX_2.shape, trainY_2.shape)\nprint('test dataset: ', testX_2.shape, testY_2.shape)\n\ntrain dataset:  (256399, 560) (256399,)\ntest dataset:  (64768, 560) (64768,)\n\n\n\nnumpy -> DataFrame\n\n\n##### train\ndf_trainX_2 = pd.DataFrame(trainX_2)\ndf_trainX_2.columns = lst_col_info\ndf_trainY_2 = pd.DataFrame(trainY_2)\n\n##### test \ndf_testX_2 = pd.DataFrame(testX_2)\ndf_testX_2.columns = lst_col_info\ndf_testY_2 = pd.DataFrame(testY_2)\n\ndf_trainX_2.head()\n\n\n\n\n\n  \n    \n      \n      D-9_Open\n      D-9_High\n      D-9_Low\n      D-9_Close\n      D-9_trading_value\n      D-9_MA5\n      D-9_MA20\n      D-9_MA60\n      D-9_MA120\n      D-9_VMAP\n      ...\n      D-0_RSI\n      D-0_SRSI\n      D-0_TSI\n      D-0_UO\n      D-0_SR\n      D-0_WR\n      D-0_AO\n      D-0_ROC\n      D-0_PPO\n      D-0_PVO\n    \n  \n  \n    \n      0\n      1.014235\n      1.014235\n      0.990510\n      1.002372\n      50865.391459\n      1.012100\n      1.039442\n      1.004251\n      1.067230\n      1.062162\n      ...\n      67.826155\n      1.000000\n      14.884190\n      55.338001\n      100.952381\n      0.952381\n      223.411765\n      10.613208\n      1.337481\n      6.227088\n    \n    \n      1\n      0.984615\n      1.023669\n      0.984615\n      0.997633\n      55152.152663\n      1.000947\n      1.036391\n      1.002367\n      1.063619\n      1.055599\n      ...\n      68.784853\n      1.000000\n      18.642720\n      51.178165\n      94.067797\n      -5.932203\n      310.029412\n      10.941176\n      1.688808\n      4.010564\n    \n    \n      2\n      0.990510\n      1.064057\n      0.990510\n      1.026097\n      78619.572954\n      1.007355\n      1.039739\n      1.005575\n      1.065055\n      1.054236\n      ...\n      74.103331\n      1.000000\n      23.963771\n      66.127796\n      100.000000\n      -0.000000\n      393.558824\n      15.658363\n      2.221982\n      14.120940\n    \n    \n      3\n      0.996532\n      1.040462\n      0.996532\n      1.038150\n      210838.980347\n      0.992832\n      1.016358\n      0.980732\n      1.037177\n      1.021560\n      ...\n      64.812483\n      0.721663\n      25.077575\n      61.734791\n      82.517483\n      -17.482517\n      451.588235\n      12.426036\n      2.389092\n      13.705978\n    \n    \n      4\n      1.013363\n      1.013363\n      0.982183\n      0.983296\n      303571.073497\n      0.965256\n      0.980401\n      0.944933\n      0.998181\n      0.984157\n      ...\n      66.615529\n      0.774168\n      26.576492\n      63.519219\n      89.510490\n      -10.489510\n      525.558824\n      13.879004\n      2.576580\n      18.168169\n    \n  \n\n5 rows Ã— 560 columns\n\n\n\n\nSHAP í‘œì¤€í™”\n\n\n##### train\nimport shap\nshap.initjs()\nexplainer = shap.TreeExplainer(xgb_2)\nshap_values_2 = explainer.shap_values(df_trainX_2)\nshap_values_test_2 = explainer.shap_values(df_testX_2)\n\n\n\n\n\n\n\n\nshap.summary_plot(shap_values_2, df_trainX_2)\n\n\n\n\n\n\n\n\nshap.summary_plot(shap_values_test_2, df_testX_2)\n\n\n\n\nê³¼ë§¤ìˆ˜êµ¬ê°„ë„ ë§ˆì°¬ê°€ì§€ë¡œ train, test ë°ì´í„°ì…‹ì€ ìœ ì‚¬í•œ ë³€ìˆ˜ ì¤‘ìš”ë„ë¥¼ ê°€ì§„ë‹¤. D-0_Highì˜ ê°’ì´ í´ ìˆ˜ë¡, D-0_KCLì˜ ê°’ì´ ì‘ì„ ìˆ˜ë¡, D-0_DCH ê°’ì´ í´ ìˆ˜ë¡ ì–‘ì˜ ì˜í–¥ì„ ë¯¸ì³¤ë‹¤ê³  í•´ì„ëœë‹¤.\n\n\n\n\n\n\nê³¼ë§¤ìˆ˜êµ¬ê°„ ë°ì´í„°ì…‹ ë¶ˆëŸ¬ì˜¤ê¸°\n\n\ntrainX_3, trainY_3, testX_3, testY_3, lst_code_date_3, lst_code_date_test_3, xgb_3 = dic_dataset_model['CCI ~-100']\nprint('train dataset: ', trainX_3.shape, trainY_3.shape)\nprint('test dataset: ', testX_3.shape, testY_3.shape)\n\ntrain dataset:  (269977, 560) (269977,)\ntest dataset:  (73435, 560) (73435,)\n\n\n\nnumpy -> DataFrame\n\n\n##### train\ndf_trainX_3 = pd.DataFrame(trainX_3)\ndf_trainX_3.columns = lst_col_info\ndf_trainY_3 = pd.DataFrame(trainY_3)\n\n##### test \ndf_testX_3 = pd.DataFrame(testX_3)\ndf_testX_3.columns = lst_col_info\ndf_testY_3 = pd.DataFrame(testY_3)\n\ndf_trainX_3.head()\n\n\n\n\n\n  \n    \n      \n      D-9_Open\n      D-9_High\n      D-9_Low\n      D-9_Close\n      D-9_trading_value\n      D-9_MA5\n      D-9_MA20\n      D-9_MA60\n      D-9_MA120\n      D-9_VMAP\n      ...\n      D-0_RSI\n      D-0_SRSI\n      D-0_TSI\n      D-0_UO\n      D-0_SR\n      D-0_WR\n      D-0_AO\n      D-0_ROC\n      D-0_PPO\n      D-0_PVO\n    \n  \n  \n    \n      0\n      0.991238\n      1.004381\n      0.971522\n      0.980285\n      140309.140197\n      0.984885\n      0.943045\n      0.936528\n      0.995007\n      0.969697\n      ...\n      40.723504\n      0.000000\n      5.486901\n      35.175106\n      8.421053\n      -91.578947\n      86.911765\n      -7.362637\n      0.401602\n      -13.360869\n    \n    \n      1\n      1.019084\n      1.019084\n      0.938931\n      0.946565\n      690931.786260\n      0.964885\n      0.889427\n      0.787226\n      0.717487\n      0.930415\n      ...\n      45.121178\n      0.022302\n      7.541308\n      40.870588\n      10.000000\n      -90.000000\n      43.235294\n      -13.127413\n      1.251577\n      -3.674717\n    \n    \n      2\n      0.991935\n      1.040323\n      0.987903\n      1.004032\n      317878.620968\n      1.026613\n      0.945484\n      0.836371\n      0.760813\n      0.987418\n      ...\n      42.403451\n      0.000000\n      3.945359\n      39.056401\n      1.960784\n      -98.039216\n      -168.382353\n      -13.725490\n      0.615608\n      -4.840261\n    \n    \n      3\n      1.000000\n      1.020080\n      0.979920\n      0.991968\n      350971.128514\n      1.012851\n      0.946787\n      0.837590\n      0.760281\n      0.986872\n      ...\n      36.691473\n      0.000000\n      -1.153744\n      34.272840\n      0.000000\n      -100.000000\n      -473.676471\n      -20.610687\n      -0.308624\n      -0.268302\n    \n    \n      4\n      1.004049\n      1.004049\n      0.955466\n      0.979757\n      327074.267206\n      1.010526\n      0.958907\n      0.848691\n      0.768900\n      1.001767\n      ...\n      38.186006\n      0.041239\n      -4.753380\n      32.124091\n      4.761905\n      -95.238095\n      -627.794118\n      -15.322581\n      -0.969999\n      0.192505\n    \n  \n\n5 rows Ã— 560 columns\n\n\n\n\nSHAP í‘œì¤€í™”\n\n\n##### train\nimport shap\nshap.initjs()\nexplainer = shap.TreeExplainer(xgb_3)\nshap_values_3 = explainer.shap_values(df_trainX_3)\nshap_values_test_3 = explainer.shap_values(df_testX_3)\n\n\n\n\n\n\n\n\nshap.summary_plot(shap_values_3, df_trainX_3)\n\n\n\n\n\n\n\n\nshap.summary_plot(shap_values_test_3, df_testX_3)\n\n\n\n\nê³¼ë§¤ë„êµ¬ê°„ ë˜í•œ train, test ë°ì´í„°ì…‹ì´ ìœ ì‚¬í•œ ë³€ìˆ˜ ì¤‘ìš”ë„ë¥¼ ê°€ì¡Œë‹¤. ê³¼ë§¤ë„êµ¬ê°„ì€ ëª¨ë¸ í•™ìŠµ ë‹¹ì‹œ ëª¨ë¸ì˜ ì„±ëŠ¥ì´ ê°€ì¥ ì¢‹ê²Œ ë‚˜ì™”ìœ¼ë©°, summary plot ì—ì„œë„ ìƒ‰ê¹”ì˜ ê²½ê³„ê°€ ê°€ì¥ ëšœë ·í•˜ê²Œ ë‚˜íƒ€ë‚¬ë‹¤. D-0_KCHì˜ ê°’ì´ í´ ìˆ˜ë¡, D-0_Closeì˜ ê°’ì´ ì‘ì„ ìˆ˜ë¡, D-0_SMA ê°’ì´ í´ ìˆ˜ë¡ ì–‘ì˜ ì˜í–¥ì„ ë¯¸ì³¤ë‹¤ê³  í•´ì„í•  ìˆ˜ ìˆë‹¤.\n \n\n\n\n\nìƒ‰ì˜ ê²½ê³„ê°€ ê°€ì¥ ëšœë ·í•˜ê²Œ ë‚˜íƒ€ë‚œ ê³¼ë§¤ë„ êµ¬ê°„ì—ì„œ ìƒìœ„ ë³€ìˆ˜ 3ê°œë¥¼ ì´ìš©í•˜ì—¬ ë°ì´í„°ë¥¼ í•„í„°ë§ í•˜ê³ , íŠ¹ì§•ì´ ë‚˜íƒ€ë‚˜ëŠ”ì§€ í™•ì¸í•´ë³¸ë‹¤.\n\ndf_testX_3['Code'] = lst_code_date_test_3[:, 0]\ndf_testX_3['Date'] = lst_code_date_test_3[:, 1]\n\ndf_shap_test_3 = pd.DataFrame(shap_values_test_3, columns=lst_col_info)\n\ncondition1 = df_shap_test_3['D-0_KCH'] > 0.3\ncondition2 = df_shap_test_3['D-0_Close'] > 0\ncondition3 = df_shap_test_3['D-0_SMA'] > 0.05\n\nprint('ì¡°ê±´ì„ ë§Œì¡±í•˜ëŠ” ë°ì´í„°ì˜ ê°œìˆ˜:', len(df_testX_3.loc[condition1 & condition2 & condition3]))\nprint()\nprint(\"<ë¹ˆë„ìˆ˜ ìƒìœ„ ë‚ ì§œ Top 5>\")\ndisplay(df_testX_3.loc[condition1 & condition2 & condition3, ['Date']].value_counts().head())\n\nì¡°ê±´ì„ ë§Œì¡±í•˜ëŠ” ë°ì´í„°ì˜ ê°œìˆ˜: 467\n\n<ë¹ˆë„ìˆ˜ ìƒìœ„ ë‚ ì§œ Top 5>\n\n\nDate      \n2021-10-06    58\n2021-08-20    45\n2021-03-10    16\n2021-10-01    13\n2021-10-05    13\ndtype: int64\n\n\nì¡°ê±´ì„ ë§Œì¡±í•˜ëŠ” ë°ì´í„° 467ê°œ ì¤‘ ë¹ˆë„ìˆ˜ê°€ ë†’ê²Œ ë‚˜ì˜¤ëŠ” ë‚ ì§œë“¤ì´ ìˆë‹¤. 2021ë…„ 10ì›” 6ì¼, 2021ë…„ 8ì›” 20ì¼ì˜ ë°ì´í„°ë¥¼ ì‚´í´ë³´ë„ë¡ í•œë‹¤. ì´ ë•Œ ì¢…ëª©ì½”ë“œëŠ” sample í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ëœë¤ìœ¼ë¡œ ì„ ì •í•œë‹¤.\n\nì¡°ê±´ì„ ë§Œì¡±í•˜ëŠ” ë°ì´í„° ì¤‘ ìƒìœ„ ë¹ˆë„ìˆ˜ ë‚ ì§œì˜ ëœë¤ ì¢…ëª© ì°¨íŠ¸ í™•ì¸\n\n\ndf_1006 = df_testX_3.loc[condition1 & condition2 & condition3 & (df_testX_3['Date'] == '2021-10-06'), ['Date', 'Code']].sample(5).reset_index(drop=True)\ndf_0820 = df_testX_3.loc[condition1 & condition2 & condition3 & (df_testX_3['Date'] == '2021-08-20'), ['Date', 'Code']].sample(5).reset_index(drop=True)\n\npd.concat([df_1006, df_0820], axis=1)\n\n\n\n\n\n  \n    \n      \n      Date\n      Code\n      Date\n      Code\n    \n  \n  \n    \n      0\n      2021-10-06\n      154040\n      2021-08-20\n      003830\n    \n    \n      1\n      2021-10-06\n      092070\n      2021-08-20\n      103590\n    \n    \n      2\n      2021-10-06\n      101390\n      2021-08-20\n      043340\n    \n    \n      3\n      2021-10-06\n      057030\n      2021-08-20\n      039610\n    \n    \n      4\n      2021-10-06\n      084870\n      2021-08-20\n      025620\n    \n  \n\n\n\n\n2021ë…„ 10ì›” 6ì¼\n\n\nâ€œì¢…ëª©ì½”ë“œ: 154040, 092070â€\n2021ë…„ 8ì›” 20ì¼\n \nâ€œì¢…ëª©ì½”ë“œ: 003830, 103590â€\nì´ë¯¸ì§€ ì¶œì²˜-Naver ì¦ê¶Œ\n\nëœë¤ìœ¼ë¡œ ì„ íƒí•œ 4ê°€ì§€ ì¢…ëª©ë“¤ì˜ ì°¨íŠ¸ë¥¼ í™•ì¸í•´ ë³´ì•˜ì„ ë•Œ, í•˜ë½ì„ ì¢…ë£Œí•˜ê³  ìƒìŠ¹ì˜ ì´ˆì… ë¶€ë¶„ì—ì„œ ë§¤ë§¤ ì‹œê·¸ë„ì„ ë³´ì˜€ë‹¤.\n\nSHAP í‘œì¤€í™” ë°ì´í„°ì…‹ ì €ì¥\n\n\n#collapse-hide\nimport pickle \ndic_shap_dataset = {'CCI -20~20': [shap_values_1, shap_values_test_1],\n                'CCI 100~': [shap_values_2, shap_values_test_2],\n                'CCI ~-100': [shap_values_3, shap_values_test_3]}\n\nwith open('shap_dataset_cci_filtering.pickle', 'wb') as handle:\n    pickle.dump(dic_shap_dataset, handle, protocol=pickle.HIGHEST_PROTOCOL)\n\n \në³¸ í”„ë¡œì íŠ¸ ë°©í–¥ì€ SHAPì„ í™œìš©í•˜ì—¬ ìœ ì‚¬í•œ íŠ¹ì§•ì„ ê°–ê³  ìˆëŠ” íŠ¹ì • ì§‘ë‹¨ì„ ë°œê²¬í•˜ê³  í•´ì„í•´ë‚´ëŠ” ê²ƒì´ë¼ê³  í•  ìˆ˜ ìˆë‹¤. ìœ„ì—ì„œ ë³€ìˆ˜ ì¤‘ìš”ë„ë¥¼ ì‹œê°í™” í•˜ê³ , ìƒìœ„ 3ê°œ ë³€ìˆ˜ì— ëŒ€í•˜ì—¬ ì–‘ì˜ ì˜í–¥ì„ ë¯¸ì¹˜ëŠ” ë¹„ìŠ·í•œ í¬ê¸°ì˜ ê°’ì¸ ë°ì´í„°ë“¤ë§Œ í•„í„°ë§í•˜ì—¬ ì°¨íŠ¸ë¥¼ í™•ì¸í•˜ì˜€ë‹¤. ì´ë²ˆ ê¸€ì—ì„œ SHAP value summary plotì„ í™œìš©í•˜ì—¬ ì§ì ‘ ì§‘ë‹¨ì„ êµ¬ì„±í•´ë³´ì•˜ë‹¤ë©´, ë‹¤ìŒì—ëŠ” ì´ëŸ¬í•œ ì§‘ë‹¨ì„ ì•Œê³ ë¦¬ì¦˜ì„ ì‚¬ìš©í•˜ì—¬, êµ¬ì²´ì ìœ¼ë¡œ í˜•ì„±í•˜ëŠ” ë°©ë²•ì„ ì—°êµ¬í•œë‹¤. ë‹¤ìŒ ê¸€ì—ì„œëŠ” ì €ì¥í•œ SHAP í‘œì¤€í™” ë°ì´í„°ì…‹ê³¼ ì£¼ê°€ ë°ì´í„°ì…‹ì„ 2ì°¨ì› í‰ë©´ì— ì‹œê°í™” í•˜ì—¬ êµ°ì§‘ë“¤ì´ í˜•ì„±ë˜ëŠ”ì§€ í™•ì¸í•˜ëŠ” ì‹œê°„ì„ ê°€ì§„ë‹¤."
  },
  {
    "objectID": "posts/07.tsne.html",
    "href": "posts/07.tsne.html",
    "title": "[stock prediction] 3.2 t-SNEë¥¼ ì‚¬ìš©í•œ ì£¼ê°€ë°ì´í„° 2ì°¨ì› ì‹œê°í™”",
    "section": "",
    "text": "ì´ì „ ê¸€ ( 3.1. ì„¤ëª…ê°€ëŠ¥ AI (XAI), SHAP value ) ì—ì„œëŠ” XAI ê¸°ë²• ì¤‘ í•˜ë‚˜ì¸ SHAPì„ ì‚¬ìš©í•˜ì—¬ ë³€ìˆ˜ ì¤‘ìš”ë„ë¥¼ ê³„ì‚°í•˜ê³ , summary plotì˜ ê²°ê³¼ë¡œ ë°ì´í„°ë¥¼ í•„í„°ë§í•˜ì—¬ íŠ¹ì • ì§‘ë‹¨ì„ êµ¬ì„±í•´ë³´ì•˜ë‹¤. ì´ë²ˆ ê¸€ì—ì„œëŠ” ì£¼ê°€ ë°ì´í„°ì…‹ê³¼, SHAPí‘œì¤€í™” ë°ì´í„°ì…‹ì„ 2ì°¨ì› í‰ë©´ì— ì‹œê°í™”í•¨ìœ¼ë¡œì¨ ì§‘ë‹¨ í˜•ì„±ì´ ì´ë£¨ì–´ì§€ëŠ” ê²ƒì„ í™•ì¸í•œë‹¤.\n\n\n\n\nSHAP í‘œì¤€í™” ë°ì´í„°ì…‹\n\n\nT-SNEë¥¼ ì‚¬ìš©í•œ 2ì°¨ì› ì‹œê°í™” ë¹„êµ (ì›ë³¸ ë°ì´í„°ì…‹ vs SHAP transform ë°ì´í„°ì…‹)\n\në¼ì´ë¸ŒëŸ¬ë¦¬ import\n\n\nimport pandas as pd\nimport numpy as np\nfrom tqdm import tqdm\nimport pymysql\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom ipywidgets import interact, interact_manual\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.graph_objects as go\nimport plotly.offline as pyo\n%matplotlib inline\n%pylab inline\npylab.rcParams['figure.figsize'] = (12,5)\n\nimport StockFunc as sf\n\n%pylab is deprecated, use %matplotlib inline and import the required libraries.\nPopulating the interactive namespace from numpy and matplotlib\n\n\n \n\n\n\n\nSHAP í‘œì¤€í™” ë°ì´í„°ì…‹ì´ë€ ì£¼ê°€ ì˜ˆì¸¡ ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸(XGBoost)ë¡œ ê° ë°ì´í„°ì— ëŒ€í•œ Shap valueë¥¼ ê³„ì‚°í•˜ì—¬ ê¸°ì¡´ì˜ ì£¼ê°€ ë°ì´í„°ì…‹ì„ ë³€í™˜í•œ ë°ì´í„°ë¥¼ ë§í•œë‹¤. ê¸°ì¡´ ì£¼ê°€ ë°ì´í„°ê°€ SHAP í‘œì¤€í™” ë°ì´í„°ë¡œ ë³€í™˜ë˜ì—ˆì„ ë•Œ, ì´ì§„ë¶„ë¥˜ ì£¼ê°€ ì˜ˆì¸¡ ëª¨ë¸ì˜ ì¤‘ìš” ë³€ìˆ˜ë¥¼ ìƒëŒ€ì ìœ¼ë¡œ ê°€ì¤‘í•˜ê²Œë¨ìœ¼ë¡œì¨ ë°ì´í„°ì—ì„œì˜ ë‚´ì¬ë˜ì–´ìˆëŠ” íŒ¨í„´ì´ ê°•ì¡°ë˜ëŠ” íš¨ê³¼ê°€ ìˆë‹¤. ë”°ë¼ì„œ ê¸°ì¡´ì˜ ì£¼ê°€ ë°ì´í„°ì…‹ê³¼ SHAP í‘œì¤€í™” ë°ì´í„°ì…‹ì„ 2ì°¨ì› í‰ë©´ì— í•¨ê»˜ ì‹œê°í™” í•˜ì—¬ SHAP í‘œì¤€í™” ë°ì´í„°ì…‹ì˜ êµ°ì§‘ì˜ ê²½ê³„ê°€ ë” ëª…í™•í•˜ê²Œ ë“œëŸ¬ë‚˜ëŠ” ê²ƒì„ í™•ì¸í•œë‹¤.\n\nCCI êµ¬ê°„ ë³„ ì£¼ê°€ ë°ì´í„°ì…‹ ë¶ˆëŸ¬ì˜¤ê¸°\n\n\nimport pickle\nwith open('dataset_cci_filtering.pickle', 'rb') as handle:\n    dic_dataset_model = pickle.load(handle)\n\nCCI filtering ê¸€ì—ì„œ ìƒì„±í•œ CCI êµ¬ê°„ ë³„ ë°ì´í„°ì…‹ê³¼ ëª¨ë¸ì„ ë¶ˆëŸ¬ì˜¨ë‹¤.\n\nì»¬ëŸ¼ ë¦¬ìŠ¤íŠ¸ ìƒì„±\n\n\n#collapse-hide\nlst_col_info = []\n\nlst_col = ['Open', 'High', 'Low', 'Close', 'trading_value', 'MA5', 'MA20', 'MA60', 'MA120', \n           'VMAP', 'BHB', 'BLB', 'KCH', 'KCL', 'KCM', 'DCH', 'DCL', 'DCM',\n           'SMA', 'EMA', 'WMA', 'Ichimoku', 'Parabolic SAR', 'KAMA','MACD']   +  ['Change', 'Volume', 'MFI', 'ADI', 'OBV', \n                                                                                   'CMF', 'FI', 'EOM, EMV', 'VPT', 'NVI', 'ATR', 'UI',\n                                                                                   'ADX', '-VI', '+VI', 'TRIX', 'MI', 'CCI', 'DPO', 'KST', \n                                                                                   'STC', 'RSI', 'SRSI', 'TSI', 'UO', 'SR',\n                                                                                   'WR', 'AO', 'ROC', 'PPO', 'PVO'] \n           \n\nfor day in range(9, -1, -1): \n    for col in lst_col: \n        lst_col_info.append(f'D-{day}_{col}')\n\në’¤ì—ì„œ 1ë…„ ë‹¨ìœ„ë¡œ 2ì°¨ì› ì‹œê°í™”ë¥¼ í•˜ê¸° ìœ„í•´ train, test ë°ì´í„°ì…‹ì„ í•©ì¹˜ê³ , ê° CCI êµ¬ê°„ ë³„ SHAP transform ë°ì´í„°ë¥¼ ìƒì„±í•œë‹¤.\n\n\n\n\ntrainX_1, trainY_1, testX_1, testY_1, lst_code_date_1, lst_code_date_test_1, xgb_1 = dic_dataset_model['CCI -20~20']\nprint('train dataset: ', trainX_1.shape, trainY_1.shape)\nprint('test dataset: ', testX_1.shape, testY_1.shape)\n\ntrain dataset:  (123321, 560) (123321,)\ntest dataset:  (36885, 560) (36885,)\n\n\n\ncode, date, labelì»¬ëŸ¼ ì¶”ê°€ ë° train, testì„ í•©ì¹œ ë°ì´í„°í”„ë ˆì„ ìƒì„±\n\n\n#collapse-hide\n# train + test (array)\narr_data_1 = np.concatenate((trainX_1, testX_1), axis=0)\narr_label_1 = np.concatenate((trainY_1, testY_1), axis=0)\narr_code_date_1 = np.concatenate((lst_code_date_1, lst_code_date_test_1), axis=0)\n\n# code + date + dataset + label (dataframe)\ndf_data_1 = pd.DataFrame(arr_data_1, columns=lst_col_info)\ndf_data_1['Code'], df_data_1['Date'], df_data_1['Label'] = arr_code_date_1[:, 0], arr_code_date_1[:, 1], arr_label_1\ndf_data_1 = df_data_1[['Code', 'Date'] + lst_col_info + ['Label']]\n\nprint('df_data_1 shape:', df_data_1.shape)\ndisplay(df_data_1.head())\n\ndf_data_1 shape: (160206, 563)\n\n\n\n\n\n\n  \n    \n      \n      Code\n      Date\n      D-9_Open\n      D-9_High\n      D-9_Low\n      D-9_Close\n      D-9_trading_value\n      D-9_MA5\n      D-9_MA20\n      D-9_MA60\n      ...\n      D-0_SRSI\n      D-0_TSI\n      D-0_UO\n      D-0_SR\n      D-0_WR\n      D-0_AO\n      D-0_ROC\n      D-0_PPO\n      D-0_PVO\n      Label\n    \n  \n  \n    \n      0\n      050120\n      2017-01-20\n      1.005714\n      1.048000\n      1.000000\n      1.040000\n      462369.440000\n      0.994514\n      0.968971\n      0.981010\n      ...\n      0.057077\n      20.137832\n      43.856662\n      46.236559\n      -53.763441\n      433.235294\n      2.564103\n      1.624561\n      -3.039166\n      0\n    \n    \n      1\n      050120\n      2017-02-06\n      1.000000\n      1.011236\n      0.974157\n      0.988764\n      94755.235955\n      1.004719\n      0.987640\n      0.954307\n      ...\n      0.779487\n      6.199506\n      48.326356\n      75.903614\n      -24.096386\n      53.588235\n      -0.885936\n      0.530558\n      2.429462\n      0\n    \n    \n      2\n      050120\n      2017-02-24\n      1.007463\n      1.012793\n      0.996802\n      1.005330\n      105709.495736\n      0.974840\n      0.944350\n      0.910856\n      ...\n      0.000000\n      16.267954\n      46.534386\n      14.444444\n      -85.555556\n      343.500000\n      0.898876\n      1.645809\n      10.986297\n      1\n    \n    \n      3\n      050120\n      2017-02-27\n      1.007423\n      1.033934\n      0.975610\n      1.033934\n      327239.156946\n      0.987063\n      0.943160\n      0.907971\n      ...\n      0.225976\n      14.411703\n      44.991463\n      40.909091\n      -59.090909\n      289.705882\n      1.651982\n      1.484638\n      9.126668\n      0\n    \n    \n      4\n      050120\n      2017-02-28\n      0.994872\n      0.994872\n      0.950769\n      0.974359\n      168981.128205\n      0.966974\n      0.914769\n      0.879402\n      ...\n      0.108960\n      11.827189\n      44.008782\n      26.136364\n      -73.863636\n      221.794118\n      -2.985075\n      1.229302\n      5.503306\n      0\n    \n  \n\n5 rows Ã— 563 columns\n\n\n\n\nSHAP transform ë°ì´í„°ì…‹ ìƒì„±\n\n\n#collapse-hide\nimport shap\nshap.initjs()\nexplainer = shap.TreeExplainer(xgb_1)\nshap_values = explainer.shap_values(df_data_1.drop(columns=['Code', 'Date', 'Label']))\n\ndf_shap_values_1 = pd.DataFrame(shap_values, columns=lst_col_info)\ndf_shap_values_1['Code'], df_shap_values_1['Date'], df_shap_values_1['Label'] = arr_code_date_1[:, 0], arr_code_date_1[:, 1], arr_label_1\ndf_shap_values_1 = df_shap_values_1[['Code', 'Date'] + lst_col_info + ['Label']]\n\nprint('df_shap_values_1 shape:', df_shap_values_1.shape)\ndisplay(df_shap_values_1.head())\n\n\n\n\n\ndf_shap_values_1 shape: (160206, 563)\n\n\n\n\n\n\n  \n    \n      \n      Code\n      Date\n      D-9_Open\n      D-9_High\n      D-9_Low\n      D-9_Close\n      D-9_trading_value\n      D-9_MA5\n      D-9_MA20\n      D-9_MA60\n      ...\n      D-0_SRSI\n      D-0_TSI\n      D-0_UO\n      D-0_SR\n      D-0_WR\n      D-0_AO\n      D-0_ROC\n      D-0_PPO\n      D-0_PVO\n      Label\n    \n  \n  \n    \n      0\n      050120\n      2017-01-20\n      -0.007915\n      -0.001929\n      -0.000805\n      0.001416\n      -0.000197\n      0.0\n      0.000018\n      -0.000571\n      ...\n      0.0\n      0.0\n      0.003014\n      0.000004\n      0.0\n      0.000062\n      -0.003845\n      0.000004\n      -0.001181\n      0\n    \n    \n      1\n      050120\n      2017-02-06\n      -0.007741\n      -0.001461\n      0.000470\n      0.000085\n      -0.000197\n      0.0\n      0.000012\n      -0.000229\n      ...\n      0.0\n      0.0\n      0.001020\n      0.000009\n      0.0\n      0.000052\n      0.004082\n      0.000004\n      0.007927\n      0\n    \n    \n      2\n      050120\n      2017-02-24\n      -0.009699\n      -0.001854\n      -0.000975\n      -0.000129\n      -0.000197\n      0.0\n      0.000018\n      -0.000571\n      ...\n      0.0\n      0.0\n      0.001876\n      -0.000005\n      0.0\n      0.000062\n      -0.004085\n      0.000004\n      0.007150\n      1\n    \n    \n      3\n      050120\n      2017-02-27\n      -0.008703\n      -0.001563\n      0.000816\n      0.000360\n      -0.000197\n      0.0\n      0.000018\n      -0.000571\n      ...\n      0.0\n      0.0\n      0.001332\n      0.000009\n      0.0\n      0.000062\n      -0.003878\n      0.000004\n      0.008917\n      0\n    \n    \n      4\n      050120\n      2017-02-28\n      -0.003862\n      0.007390\n      0.000849\n      0.000148\n      -0.000197\n      0.0\n      0.000018\n      -0.000571\n      ...\n      0.0\n      0.0\n      0.001248\n      -0.000005\n      0.0\n      0.000062\n      0.004082\n      0.000004\n      0.008872\n      0\n    \n  \n\n5 rows Ã— 563 columns\n\n\n\n\n\n\n\n\ntrainX_2, trainY_2, testX_2, testY_2, lst_code_date_2, lst_code_date_test_2, xgb_2 = dic_dataset_model['CCI 100~']\nprint('train dataset: ', trainX_2.shape, trainY_2.shape)\nprint('test dataset: ', testX_2.shape, testY_2.shape)\n\ntrain dataset:  (256399, 560) (256399,)\ntest dataset:  (64768, 560) (64768,)\n\n\n\ncode, date, labelì»¬ëŸ¼ ì¶”ê°€ ë° train, testì„ í•©ì¹œ ë°ì´í„°í”„ë ˆì„ ìƒì„±\n\n\n#collapse-hide\n# train + test (array)\narr_data_2 = np.concatenate((trainX_2, testX_2), axis=0)\narr_label_2 = np.concatenate((trainY_2, testY_2), axis=0)\narr_code_date_2 = np.concatenate((lst_code_date_2, lst_code_date_test_2), axis=0)\n\n# code + date + dataset + label (dataframe)\ndf_data_2 = pd.DataFrame(arr_data_2, columns=lst_col_info)\ndf_data_2['Code'], df_data_2['Date'], df_data_2['Label'] = arr_code_date_2[:, 0], arr_code_date_2[:, 1], arr_label_2\ndf_data_2 = df_data_2[['Code', 'Date'] + lst_col_info + ['Label']]\n\nprint('df_data_2 shape:', df_data_2.shape)\ndisplay(df_data_2.head())\n\ndf_data_2 shape: (321167, 563)\n\n\n\n\n\n\n  \n    \n      \n      Code\n      Date\n      D-9_Open\n      D-9_High\n      D-9_Low\n      D-9_Close\n      D-9_trading_value\n      D-9_MA5\n      D-9_MA20\n      D-9_MA60\n      ...\n      D-0_SRSI\n      D-0_TSI\n      D-0_UO\n      D-0_SR\n      D-0_WR\n      D-0_AO\n      D-0_ROC\n      D-0_PPO\n      D-0_PVO\n      Label\n    \n  \n  \n    \n      0\n      050120\n      2017-02-10\n      1.014235\n      1.014235\n      0.990510\n      1.002372\n      50865.391459\n      1.012100\n      1.039442\n      1.004251\n      ...\n      1.000000\n      14.884190\n      55.338001\n      100.952381\n      0.952381\n      223.411765\n      10.613208\n      1.337481\n      6.227088\n      0\n    \n    \n      1\n      050120\n      2017-02-13\n      0.984615\n      1.023669\n      0.984615\n      0.997633\n      55152.152663\n      1.000947\n      1.036391\n      1.002367\n      ...\n      1.000000\n      18.642720\n      51.178165\n      94.067797\n      -5.932203\n      310.029412\n      10.941176\n      1.688808\n      4.010564\n      1\n    \n    \n      2\n      050120\n      2017-02-14\n      0.990510\n      1.064057\n      0.990510\n      1.026097\n      78619.572954\n      1.007355\n      1.039739\n      1.005575\n      ...\n      1.000000\n      23.963771\n      66.127796\n      100.000000\n      -0.000000\n      393.558824\n      15.658363\n      2.221982\n      14.120940\n      0\n    \n    \n      3\n      050120\n      2017-02-15\n      0.996532\n      1.040462\n      0.996532\n      1.038150\n      210838.980347\n      0.992832\n      1.016358\n      0.980732\n      ...\n      0.721663\n      25.077575\n      61.734791\n      82.517483\n      -17.482517\n      451.588235\n      12.426036\n      2.389092\n      13.705978\n      0\n    \n    \n      4\n      050120\n      2017-02-16\n      1.013363\n      1.013363\n      0.982183\n      0.983296\n      303571.073497\n      0.965256\n      0.980401\n      0.944933\n      ...\n      0.774168\n      26.576492\n      63.519219\n      89.510490\n      -10.489510\n      525.558824\n      13.879004\n      2.576580\n      18.168169\n      0\n    \n  \n\n5 rows Ã— 563 columns\n\n\n\n\nSHAP transform ë°ì´í„°ì…‹ ìƒì„±\n\n\n#collapse-hide\nimport shap\nshap.initjs()\nexplainer = shap.TreeExplainer(xgb_2)\nshap_values = explainer.shap_values(df_data_2.drop(columns=['Code', 'Date', 'Label']))\n\ndf_shap_values_2 = pd.DataFrame(shap_values, columns=lst_col_info)\ndf_shap_values_2['Code'], df_shap_values_2['Date'], df_shap_values_2['Label'] = arr_code_date_2[:, 0], arr_code_date_2[:, 1], arr_label_2\ndf_shap_values_2 = df_shap_values_2[['Code', 'Date'] + lst_col_info + ['Label']]\n\nprint('df_shap_values_2 shape:', df_shap_values_2.shape)\ndisplay(df_shap_values_2.head())\n\n\n\n\n\ndf_shap_values_2 shape: (321167, 563)\n\n\n\n\n\n\n  \n    \n      \n      Code\n      Date\n      D-9_Open\n      D-9_High\n      D-9_Low\n      D-9_Close\n      D-9_trading_value\n      D-9_MA5\n      D-9_MA20\n      D-9_MA60\n      ...\n      D-0_SRSI\n      D-0_TSI\n      D-0_UO\n      D-0_SR\n      D-0_WR\n      D-0_AO\n      D-0_ROC\n      D-0_PPO\n      D-0_PVO\n      Label\n    \n  \n  \n    \n      0\n      050120\n      2017-02-10\n      -0.000128\n      1.306584e-06\n      -0.002761\n      0.0\n      0.000007\n      0.000003\n      0.000001\n      0.0\n      ...\n      -0.001276\n      0.000247\n      0.0\n      -0.010375\n      0.0\n      0.000695\n      -0.005244\n      0.001141\n      -0.002147\n      0\n    \n    \n      1\n      050120\n      2017-02-13\n      0.002046\n      -8.618027e-07\n      -0.002228\n      0.0\n      0.000004\n      -0.000001\n      0.000001\n      0.0\n      ...\n      -0.000481\n      0.000249\n      0.0\n      -0.011002\n      0.0\n      0.000695\n      -0.005244\n      0.001374\n      -0.002653\n      1\n    \n    \n      2\n      050120\n      2017-02-14\n      -0.000148\n      -8.618027e-07\n      -0.002317\n      0.0\n      0.000007\n      0.000041\n      0.000001\n      0.0\n      ...\n      -0.000370\n      0.000249\n      0.0\n      -0.011764\n      0.0\n      0.000695\n      -0.002502\n      0.001067\n      -0.002653\n      0\n    \n    \n      3\n      050120\n      2017-02-15\n      -0.000173\n      -8.618027e-07\n      -0.002759\n      0.0\n      0.000046\n      -0.000001\n      0.000001\n      0.0\n      ...\n      0.000536\n      0.000249\n      0.0\n      -0.012226\n      0.0\n      0.000695\n      -0.005757\n      0.001003\n      -0.003974\n      0\n    \n    \n      4\n      050120\n      2017-02-16\n      -0.000172\n      1.306584e-06\n      -0.001784\n      0.0\n      0.000024\n      -0.000020\n      0.000001\n      0.0\n      ...\n      0.000400\n      -0.000031\n      0.0\n      -0.011209\n      0.0\n      0.000255\n      -0.004513\n      0.000797\n      0.010798\n      0\n    \n  \n\n5 rows Ã— 563 columns\n\n\n\n\n\n\n\n\ntrainX_3, trainY_3, testX_3, testY_3, lst_code_date_3, lst_code_date_test_3, xgb_3 = dic_dataset_model['CCI ~-100']\nprint('train dataset: ', trainX_3.shape, trainY_3.shape)\nprint('test dataset: ', testX_3.shape, testY_3.shape)\n\ntrain dataset:  (269977, 560) (269977,)\ntest dataset:  (73435, 560) (73435,)\n\n\n\ncode, date, labelì»¬ëŸ¼ ì¶”ê°€ ë° train, testì„ í•©ì¹œ ë°ì´í„°í”„ë ˆì„ ìƒì„±\n\n\n#collapse-hide\n# train + test (array)\narr_data_3 = np.concatenate((trainX_3, testX_3), axis=0)\narr_label_3 = np.concatenate((trainY_3, testY_3), axis=0)\narr_code_date_3 = np.concatenate((lst_code_date_3, lst_code_date_test_3), axis=0)\n\n# code + date + dataset + label (dataframe)\ndf_data_3 = pd.DataFrame(arr_data_3, columns=lst_col_info)\ndf_data_3['Code'], df_data_3['Date'], df_data_3['Label'] = arr_code_date_3[:, 0], arr_code_date_3[:, 1], arr_label_3\ndf_data_3 = df_data_3[['Code', 'Date'] + lst_col_info + ['Label']]\n\nprint('df_data_3 shape:', df_data_3.shape)\ndisplay(df_data_3.head())\n\ndf_data_3 shape: (343412, 563)\n\n\n\n\n\n\n  \n    \n      \n      Code\n      Date\n      D-9_Open\n      D-9_High\n      D-9_Low\n      D-9_Close\n      D-9_trading_value\n      D-9_MA5\n      D-9_MA20\n      D-9_MA60\n      ...\n      D-0_SRSI\n      D-0_TSI\n      D-0_UO\n      D-0_SR\n      D-0_WR\n      D-0_AO\n      D-0_ROC\n      D-0_PPO\n      D-0_PVO\n      Label\n    \n  \n  \n    \n      0\n      050120\n      2017-01-25\n      0.991238\n      1.004381\n      0.971522\n      0.980285\n      140309.140197\n      0.984885\n      0.943045\n      0.936528\n      ...\n      0.000000\n      5.486901\n      35.175106\n      8.421053\n      -91.578947\n      86.911765\n      -7.362637\n      0.401602\n      -13.360869\n      0\n    \n    \n      1\n      050120\n      2017-05-18\n      1.019084\n      1.019084\n      0.938931\n      0.946565\n      690931.786260\n      0.964885\n      0.889427\n      0.787226\n      ...\n      0.022302\n      7.541308\n      40.870588\n      10.000000\n      -90.000000\n      43.235294\n      -13.127413\n      1.251577\n      -3.674717\n      0\n    \n    \n      2\n      050120\n      2017-05-19\n      0.991935\n      1.040323\n      0.987903\n      1.004032\n      317878.620968\n      1.026613\n      0.945484\n      0.836371\n      ...\n      0.000000\n      3.945359\n      39.056401\n      1.960784\n      -98.039216\n      -168.382353\n      -13.725490\n      0.615608\n      -4.840261\n      0\n    \n    \n      3\n      050120\n      2017-05-22\n      1.000000\n      1.020080\n      0.979920\n      0.991968\n      350971.128514\n      1.012851\n      0.946787\n      0.837590\n      ...\n      0.000000\n      -1.153744\n      34.272840\n      0.000000\n      -100.000000\n      -473.676471\n      -20.610687\n      -0.308624\n      -0.268302\n      0\n    \n    \n      4\n      050120\n      2017-05-23\n      1.004049\n      1.004049\n      0.955466\n      0.979757\n      327074.267206\n      1.010526\n      0.958907\n      0.848691\n      ...\n      0.041239\n      -4.753380\n      32.124091\n      4.761905\n      -95.238095\n      -627.794118\n      -15.322581\n      -0.969999\n      0.192505\n      0\n    \n  \n\n5 rows Ã— 563 columns\n\n\n\n\nSHAP transform ë°ì´í„°ì…‹ ìƒì„±\n\n\n#collapse-hide\nimport shap\nshap.initjs()\nexplainer = shap.TreeExplainer(xgb_3)\nshap_values = explainer.shap_values(df_data_3.drop(columns=['Code', 'Date', 'Label']))\n\ndf_shap_values_3 = pd.DataFrame(shap_values, columns=lst_col_info)\ndf_shap_values_3['Code'], df_shap_values_3['Date'], df_shap_values_3['Label'] = arr_code_date_3[:, 0], arr_code_date_3[:, 1], arr_label_3\ndf_shap_values_3 = df_shap_values_3[['Code', 'Date'] + lst_col_info + ['Label']]\n\nprint('df_shap_values_3 shape:', df_shap_values_3.shape)\ndisplay(df_shap_values_3.head())\n\n\n\n\n\ndf_shap_values_3 shape: (343412, 563)\n\n\n\n\n\n\n  \n    \n      \n      Code\n      Date\n      D-9_Open\n      D-9_High\n      D-9_Low\n      D-9_Close\n      D-9_trading_value\n      D-9_MA5\n      D-9_MA20\n      D-9_MA60\n      ...\n      D-0_SRSI\n      D-0_TSI\n      D-0_UO\n      D-0_SR\n      D-0_WR\n      D-0_AO\n      D-0_ROC\n      D-0_PPO\n      D-0_PVO\n      Label\n    \n  \n  \n    \n      0\n      050120\n      2017-01-25\n      0.0\n      0.000021\n      0.0\n      0.001773\n      0.0\n      -0.000586\n      0.0\n      -0.000619\n      ...\n      0.000374\n      0.0\n      0.004017\n      -0.024822\n      0.0\n      0.0\n      -0.011074\n      0.0\n      0.000174\n      0\n    \n    \n      1\n      050120\n      2017-05-18\n      0.0\n      0.000238\n      0.0\n      -0.032219\n      0.0\n      -0.000341\n      0.0\n      -0.000356\n      ...\n      0.000017\n      0.0\n      0.003150\n      -0.035779\n      0.0\n      0.0\n      0.019929\n      0.0\n      0.000174\n      0\n    \n    \n      2\n      050120\n      2017-05-19\n      0.0\n      0.000282\n      0.0\n      0.001756\n      0.0\n      0.000070\n      0.0\n      -0.000222\n      ...\n      0.000374\n      0.0\n      0.004979\n      0.058416\n      0.0\n      0.0\n      0.019747\n      0.0\n      0.000174\n      0\n    \n    \n      3\n      050120\n      2017-05-22\n      0.0\n      0.000177\n      0.0\n      0.001917\n      0.0\n      0.000153\n      0.0\n      -0.000356\n      ...\n      0.000374\n      0.0\n      0.003013\n      0.243910\n      0.0\n      0.0\n      0.019765\n      0.0\n      0.000029\n      0\n    \n    \n      4\n      050120\n      2017-05-23\n      0.0\n      -0.000847\n      0.0\n      0.002323\n      0.0\n      0.001116\n      0.0\n      -0.001705\n      ...\n      0.000856\n      0.0\n      0.002451\n      -0.002878\n      0.0\n      0.0\n      0.024483\n      0.0\n      0.000029\n      0\n    \n  \n\n5 rows Ã— 563 columns\n\n\n\n\nëª¨ë“  dataframe ì €ì¥\n\n\n#collapse-hide\nimport pickle \ndict_all_dataset = {1: [df_data_1, df_shap_values_1],\n                  2: [df_data_2, df_shap_values_2],\n                  3: [df_data_3, df_shap_values_3]}\n\nwith open(f'all_dataset', 'wb') as handle:\n    pickle.dump(dict_all_dataset, handle, protocol=pickle.HIGHEST_PROTOCOL)\n\n \n\n\n\n\n2ì°¨ì› ì‹œê°í™”ëŠ” CCI êµ¬ê°„ë³„, ì—°ë„ë³„ë¡œ ë°ì´í„°ë¥¼ ë¶„í• í•˜ì—¬ ì§„í–‰í•œë‹¤. ì´ ë•Œ ì›ë³¸ ì£¼ê°€ ë°ì´í„°ì…‹ê³¼ SHAP í‘œì¤€í™” ë°ì´í„°ì…‹ì„ ê°™ì´ ì‹œê°í™”í•˜ì—¬ êµ°ì§‘ì´ í˜•ì„±ë˜ëŠ” í˜•íƒœë¥¼ ë¹„êµí•œë‹¤.\në°ì´í„° ë¶„í• ì— ì‚¬ìš©ë˜ëŠ” CCI êµ¬ê°„, ì—°ë„ëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤.\n*CCI êµ¬ê°„\n1) ì¤‘ë¦½êµ¬ê°„ - CCI : (-20, 20)\n2) ê³¼ì—´êµ¬ê°„ - CCI : (100, \\(\\infty\\))\n3) ì¹¨ì²´ êµ¬ê°„ - CCI : (-\\(\\infty\\),-100)\n*ì—°ë„\n1) 2019ë…„\n2) 2020ë…„\n3) 2021ë…„\n\nt-SNE 2ì°¨ì› ì‹œê°í™” í•¨ìˆ˜ ìƒì„±\n\nì›ë³¸ ì£¼ê°€ ë°ì´í„°ì…‹ê³¼ SHAP í‘œì¤€í™” ë°ì´í„°ì…‹, ì—°ë„, cci êµ¬ê°„ ì •ë³´ë¥¼ ë„£ì–´ì£¼ë©´, ë‘ ë°ì´í„°ì…‹ì— ëŒ€í•œ 2ì°¨ì› ì‹œê°í™” ê²°ê³¼ë¥¼ ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜ë¥¼ ìƒì„±í•œë‹¤. ì‹œê°„ ë‹¨ì¶•ì„ ìœ„í•´ ìµœì´ˆ ì‹¤í–‰í•  ë•Œ tsne_load=Falseë¡œ ì„¤ì •í•˜ì—¬ tsne ë°ì´í„°ë¥¼ ì €ì¥í•˜ê³ , tsne_loae=Trueë¡œ ë°”ê¾¸ì–´ ë°”ë¡œ ì‹œê°í™” í•  ìˆ˜ ìˆë„ë¡ í•œë‹¤. 2ì°¨ì› ì‹œê°í™” í•  ë•ŒëŠ” labelë³„ë¡œ ìƒ‰ê¹”ì„ ë‹¤ë¥´ê²Œ ì§€ì •í•˜ì—¬ íŠ¹ì • labelì˜ ë¶„í¬ê°€ ì§‘ì¤‘ë˜ì–´ ìˆëŠ” êµ°ì§‘ì´ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸í•œë‹¤.\n\ndef tsne_visualization(dataset, shap_dataset, year, cci_type, tsne_load=False, alpha=0.4, size=3):\n    \"\"\"\n    dataset: pd.DataFrame() / ì›ë³¸ ì£¼ê°€ ë°ì´í„°ì…‹ \n    shap_dataset: pd.DataFrame() / SHAP í‘œì¤€í™” ë°ì´í„°ì…‹\n    year: Int / ì‹œê°í™” í•  ì—°ë„ \n    cci_type: Int / 1(ì¤‘ë¦½êµ¬ê°„), 2(ê³¼ì—´êµ¬ê°„), 3(ì¹¨ì²´êµ¬ê°„) - íŒŒì¼ ì €ì¥ ë° ë¡œë“œë¥¼ ìœ„í•¨ \n    tsne_load(default:True): Boolean / False: ìµœì´ˆ ì‹¤í–‰ ì‹œ tsne data ìƒì„± í›„ ì €ì¥, True: ì €ì¥ëœ tsne dataë¥¼ ë¶ˆëŸ¬ì™€ì„œ ì‹œê°í™” - ì‹œê°„ ì ˆì•½ì„ ìœ„í•¨ \n    alpha(default:0.4): Float / íˆ¬ëª…ë„ \n    size=3(default:3): Int / ì  í¬ê¸° \n    \"\"\"\n    \n    from sklearn.manifold import TSNE\n    import pickle \n    plt.rcParams['axes.unicode_minus'] = False\n    plt.rc('font', family='NanumGothic')\n    # plt.style.use('default')\n    \n    fig = plt.figure(figsize=(15, 5))\n    ax1, ax2 = fig.subplots(1, 2)\n    \n    ##### ì›ë³¸ ë°ì´í„° #####\n    dataset_year = dataset[(dataset['Date'] >= f'{year}-01-01') & (dataset['Date'] <= f'{year}-12-31')].reset_index(drop=True)\n    \n    if not tsne_load:\n        np_tsne = TSNE(n_components=2, random_state=42).fit_transform(dataset_year.drop(columns=['Code', 'Date', 'Label'])) # 2ì°¨ì› t-sne ì„ë² ë”©  \n        # save np_tsne\n        with open(f'np_tsne_{year}_{cci_type}', 'wb') as handle:\n            pickle.dump(np_tsne, handle, protocol=pickle.HIGHEST_PROTOCOL)\n    else: \n        # load np_tsne\n        with open(f'np_tsne_{year}_{cci_type}', 'rb') as handle:\n            np_tsne = pickle.load(handle)\n    \n    df_tsne = pd.DataFrame(np_tsne, columns=['component0', 'component1']) # numpy array -> Dataframe     \n    df_tsne['Label'] = dataset_year['Label'] # Label ì •ë³´ ë¶ˆëŸ¬ì˜¤ê¸°\n\n    # Label ë³„ ë¶„ë¦¬ \n    df_tsne_0 = df_tsne[df_tsne['Label']==0]\n    df_tsne_1 = df_tsne[df_tsne['Label']==1]\n\n    # Label ë³„ ì‹œê°í™” \n    ax1.scatter(df_tsne_0['component0'], df_tsne_0['component1'], color = 'green', label = 'Label 0', alpha=alpha, s=size)\n    ax1.scatter(df_tsne_1['component0'], df_tsne_1['component1'], color = 'pink', label = 'Label 1', alpha=alpha, s=size)\n\n    ax1.set_title('ì›ë³¸ ë°ì´í„°ì…‹')\n    ax1.set_xlabel('component 0')\n    ax1.set_ylabel('component 1')\n    ax1.legend()\n    \n    ##### SHAP transform data #####\n    shap_dataset_year = shap_dataset[(shap_dataset['Date'] >= f'{year}-01-01') & (shap_dataset['Date'] <= f'{year}-12-31')].reset_index(drop=True)\n    \n    if not tsne_load:\n        np_tsne_shap = TSNE(n_components=2, random_state=42).fit_transform(shap_dataset_year.drop(columns=['Code', 'Date', 'Label'])) # 2ì°¨ì› t-sne ì„ë² ë”©  \n        # save np_tsne\n        with open(f'np_tsne_shap_{year}_{cci_type}', 'wb') as handle:\n            pickle.dump(np_tsne_shap, handle, protocol=pickle.HIGHEST_PROTOCOL)\n    else: \n        # load np_tsne\n        with open(f'np_tsne_shap_{year}_{cci_type}', 'rb') as handle:\n            np_tsne_shap = pickle.load(handle)\n   \n    df_tsne_shap = pd.DataFrame(np_tsne_shap, columns=['component0', 'component1']) # numpy array -> Dataframe     \n    df_tsne_shap['Label'] = shap_dataset_year['Label'] # Label ì •ë³´ ë¶ˆëŸ¬ì˜¤ê¸°\n\n    # Label ë³„ ë¶„ë¦¬ \n    df_tsne_shap_0 = df_tsne_shap[df_tsne_shap['Label']==0]\n    df_tsne_shap_1 = df_tsne_shap[df_tsne_shap['Label']==1]\n\n    # Label ë³„ ì‹œê°í™” \n    ax2.scatter(df_tsne_shap_0['component0'], df_tsne_shap_0['component1'], color = 'green', label = 'Label 0', alpha=alpha, s=size)\n    ax2.scatter(df_tsne_shap_1['component0'], df_tsne_shap_1['component1'], color = 'pink', label = 'Label 1', alpha=alpha, s=size)\n\n    ax2.set_title('SHAP transform ë°ì´í„°ì…‹')\n    ax2.set_xlabel('component 0')\n    ax2.set_ylabel('component 1')\n    ax2.legend()   \n    \n    \n    plt.show()\n\n\nCCI êµ¬ê°„ ë³„, ì—°ë„ ë³„ t-SNE ì‹œê°í™”ë¥¼ ì§„í–‰í•œë‹¤. ì›ë³¸ ë°ì´í„°ì…‹ê³¼ SHAP transform ë°ì´í„°ì…‹ì˜ ê·¸ë¦¼ì„ ë¹„êµí•˜ì—¬ ì§‘ë‹¨ í˜•ì„±ì´ ì´ë£¨ì–´ì§€ëŠ”ì§€ í™•ì¸í•œë‹¤.\n\n\n\n\n\n\ntsne_visualization(dataset=df_data_1, shap_dataset=df_shap_values_1, year=2019, cci_type=1, tsne_load=True, alpha=0.5, size=3)\n\n\n\n\n\n\n\n\ntsne_visualization(dataset=df_data_1, shap_dataset=df_shap_values_1, year=2020, cci_type=1, tsne_load=True, alpha=0.5, size=3)\n\n\n\n\n\n\n\n\ntsne_visualization(dataset=df_data_1, shap_dataset=df_shap_values_1, year=2021, cci_type=1, tsne_load=True, alpha=0.5, size=3)\n\n\n\n\n\n\n\n\n\n\n\n\ntsne_visualization(dataset=df_data_2, shap_dataset=df_shap_values_2, year=2019, cci_type=2, tsne_load=True, alpha=0.4, size=3)\n\n\n\n\n\n\n\n\ntsne_visualization(dataset=df_data_2, shap_dataset=df_shap_values_2, year=2020, cci_type=2, tsne_load=True, alpha=0.4, size=3)\n\n\n\n\n\n\n\n\ntsne_visualization(dataset=df_data_2, shap_dataset=df_shap_values_2, year=2021, cci_type=2, tsne_load=True, alpha=0.4, size=3)\n\n\n\n\n\n\n\n\n\n\n\n\ntsne_visualization(dataset=df_data_3, shap_dataset=df_shap_values_3, year=2019, cci_type=3, tsne_load=True, alpha=0.4, size=3)\n\n\n\n\n\n\n\n\ntsne_visualization(dataset=df_data_3, shap_dataset=df_shap_values_3, year=2020, cci_type=3, tsne_load=True, alpha=0.4, size=3)\n\n\n\n\n\n\n\n\ntsne_visualization(dataset=df_data_3, shap_dataset=df_shap_values_3, year=2021, cci_type=3, tsne_load=True, alpha=0.4, size=3)\n\n\n\n\nëª¨ë“  CCI êµ¬ê°„, ì—°ë„ì—ì„œ ì›ë³¸ ì£¼ê°€ ë°ì´í„°ì…‹ ë³´ë‹¤ SHAP í‘œì¤€í™” ë°ì´í„°ì…‹ì˜ êµ°ì§‘ì˜ ê²½ê³„ê°€ ë” ëª…í™•í•˜ê²Œ ë“œëŸ¬ë‚¬ìœ¼ë©°, íŠ¹ì • labelì˜ ë¶„í¬ ë˜í•œ ì§‘ì¤‘ë˜ì–´ìˆëŠ” êµ°ì§‘ì´ ì¡´ì¬í•¨ì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤. ì´ëŠ” SHAP í‘œì¤€í™” ë°ì´í„°ì…‹ì„ ì‚¬ìš©í•¨ìœ¼ë¡œì¨, ì˜ˆì¸¡ ëª¨ë¸ì˜ ì¤‘ìš” ë³€ìˆ˜ë¥¼ ìƒëŒ€ì ìœ¼ë¡œ ê°€ì¤‘í•˜ê²Œë˜ì–´ ë°ì´í„°ì—ì„œì˜ ë‚´ì¬ë˜ì–´ìˆëŠ” íŒ¨í„´ì´ ê°•ì¡°ë˜ëŠ” íš¨ê³¼ê°€ ë‚˜íƒ€ë‚œ ê²ƒìœ¼ë¡œ í•´ì„ëœë‹¤. ë³¸ ê¸€ì—ì„œ êµ°ì§‘ì´ ë‚˜ëˆ„ì–´ì§€ëŠ” ê²ƒì„ í™•ì¸í•˜ì˜€ë‹¤ë©´, ë‹¤ìŒ ê¸€ì—ì„œëŠ” SHAP í‘œì¤€í™” ë°ì´í„°ì…‹ì— ëŒ€í•˜ì—¬ ì‹¤ì œë¡œ í´ëŸ¬ìŠ¤í„°ë§ ì•Œê³ ë¦¬ì¦˜ì„ ì ìš©í•˜ì—¬ êµ°ì§‘ì„ ëª…ì‹œì ìœ¼ë¡œ ë¶„ë¥˜í•˜ëŠ” ì‹œê°„ì„ ê°€ì§„ë‹¤."
  },
  {
    "objectID": "posts/08.cluster_filtering.html",
    "href": "posts/08.cluster_filtering.html",
    "title": "[stock prediction] 3.3 í´ëŸ¬ìŠ¤í„° íƒìƒ‰ì„ í†µí•œ ì£¼ê°€ìƒìŠ¹ íŒ¨í„´ ê²€ì¶œ",
    "section": "",
    "text": "ì§€ë‚œ ê¸€ ( 3.2 t-SNEë¥¼ ì‚¬ìš©í•œ ì£¼ê°€ë°ì´í„° 2ì°¨ì› ì‹œê°í™” ) ì—ì„œëŠ” ì›ë³¸ ì£¼ê°€ë°ì´í„°ì…‹ê³¼ SHAP í‘œì¤€í™” ë°ì´í„°ì…‹ì— ëŒ€í•œ t-SNE 2ì°¨ì› ì‚°ì ë„ ì‹œê°í™”ë¥¼ ìˆ˜í–‰í•˜ì—¬ SHAP í‘œì¤€í™” ë°ì´í„°ì…‹ì˜ êµ°ì§‘ì˜ ê²½ê³„ê°€ ë” ëª…í™•í•˜ê²Œ ë“œëŸ¬ë‚¨ì„ í™•ì¸í•˜ì˜€ë‹¤. ì´ë²ˆ ê¸€ì—ì„œëŠ” SHAP í‘œì¤€í™” ë°ì´í„°ì…‹ì— ê³„ì¸µì  í´ëŸ¬ìŠ¤í„°ë§ ì•Œê³ ë¦¬ì¦˜ì„ ì ìš©í•˜ì—¬ ëª…ì‹œì ìœ¼ë¡œ êµ°ì§‘ì„ ë¶„ë¥˜í•´ë³´ê³ , label1ì˜ ë¹„ìœ¨ë¡œ êµ°ì§‘ì„ í•„í„°ë§í•˜ì—¬ ìƒìŠ¹ ì¶”ì„¸ êµ°ì§‘ì„ ì„ íƒí•œë‹¤. ì„ íƒëœ ìƒìŠ¹ ì¶”ì„¸ êµ°ì§‘ì—ì„œ ë¹ˆë„ìˆ˜ ìƒìœ„ ë‚ ì§œë“¤ì˜ ê°œë³„ ì¢…ëª© ì°¨íŠ¸ë¥¼ í™•ì¸í•˜ì—¬ ê³µí†µëœ íŒ¨í„´ì„ ê²€ì¶œí•œë‹¤.\n\n\n\n\nê³„ì¸µì  í´ëŸ¬ìŠ¤í„°ë§ & ìƒìŠ¹ ì¶”ì„¸ êµ°ì§‘ ì„ íƒ\n\n\n\nì •ë¦¬\n\në¼ì´ë¸ŒëŸ¬ë¦¬ import\n\n\nimport pandas as pd\nimport numpy as np\nfrom tqdm import tqdm\nimport pymysql\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom ipywidgets import interact, interact_manual\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.graph_objects as go\nimport plotly.offline as pyo\n%matplotlib inline\n%pylab inline\npylab.rcParams['figure.figsize'] = (12,5)\nplt.rcParams['axes.unicode_minus'] = False\nplt.rc('font', family='NanumGothic')\n\nimport StockFunc as sf\n\n%pylab is deprecated, use %matplotlib inline and import the required libraries.\nPopulating the interactive namespace from numpy and matplotlib\n\n\n \n\n\n\n\nìƒìŠ¹ ì¶”ì„¸ êµ°ì§‘ì´ë€ ê¸°ì¤€ì¼ (D0) ëŒ€ë¹„ ë‹¤ìŒ ë‚  (D+1) ì¢…ê°€ 2% ì´ìƒ ìƒìŠ¹í•œ ë°ì´í„°ë“¤ì´ ì¼ì • ë¹„ìœ¨ ì´ìƒ ì†í•˜ëŠ” êµ°ì§‘ë“¤ì„ ì˜ë¯¸í•œë‹¤. ê³„ì¸µì  í´ëŸ¬ìŠ¤í„°ë§ì„ í†µí•´ ìƒì„±ëœ êµ°ì§‘ë“¤ì„ label1ì˜ ë¹„ìœ¨ë¡œ í•„í„°ë§í•˜ì—¬ ìƒìŠ¹ ì¶”ì„¸ êµ°ì§‘ì„ ì„ íƒí•œë‹¤.\n\nlinkage, ë´ë“œë¡œê·¸ë¨ ì‹œê°í™” í•¨ìˆ˜\n\nê³„ì¸µì  í´ëŸ¬ìŠ¤í„°ë§ì—ì„œì˜ ë´ë“œë¡œê·¸ë¨ ì‹œê°í™”ë¥¼ ìˆ˜í–‰í•˜ì—¬ íŠ¸ë¦¬ì˜ ë†’ì´ë¥¼ ê²°ì •í•  ìˆ˜ ìˆë„ë¡ í•œë‹¤.\n\ndef hierarchical_clustering_plot(method, year, cci_type, dendrogram=False, n_clusters=5, min_samples=5, alpha=0.3, size=4):\n    '''\n    method: str / complete, average, ward\n    year: int / 2019, 2020, 2021\n    cci_type:int / 1, 2, 3\n    dendrogram:Boolean / True, False(default) - ì‹œê°„ì´ ì˜¤ë˜ê±¸ë¦¬ë¯€ë¡œ ì„ íƒ \n    n_clusters: int / default:5\n    min_samples: int / default:5\n    alpha: float / default: 0.3\n    size: int / default: 4\n    '''\n\n    import pickle # tsne íŒŒì¼ ë¶ˆëŸ¬ì˜¤ê¸° \n    with open(f'np_tsne_shap_{year}_{cci_type}', 'rb') as handle: \n        np_tsne = pickle.load(handle)\n    \n    if method in ('complete', 'average', 'ward'): # linkage method ì„ íƒ \n        from scipy.cluster.hierarchy import linkage, dendrogram\n        import matplotlib.pyplot as plt\n        clusters = linkage(y=np_tsne, method=method, metric='euclidean')\n        print(\"linkage complete\")\n        \n        if dendrogram: # True: ë´ë“œë¡œê·¸ë¨ ì‹œê°í™” \n            plt.title(f\"{year} dendrogram\", fontsize=15)\n            dendrogram(clusters, leaf_rotation=90, leaf_font_size=12,)\n            plt.show() \n            \n        return clusters \n        \n    else:\n        print(\"methodë¥¼ ì˜ëª» ì…ë ¥í•˜ì˜€ìŠµë‹ˆë‹¤.\")\n\n\nê³„ì¸µì  í´ëŸ¬ìŠ¤í„°ë§, ìƒìŠ¹ ì¶”ì„¸ êµ°ì§‘ ì„ íƒ ì‹œê°í™”\n\në´ë“œë¡œê·¸ë¨ì„ ì°¸ê³ í•˜ì—¬ 40 ~ 60ê°œì˜ êµ°ì§‘ì´ í˜•ì„±ë˜ë„ë¡ të¥¼ ì„¤ì •í•œë‹¤. ìƒìŠ¹ ì¶”ì„¸ êµ°ì§‘ì„ ì„ íƒí•˜ê¸° ìœ„í•´ ì ì ˆí•œ ratio(íŠ¹ì • ì§‘ë‹¨ì—ì„œì˜ label1ì˜ ë¹„ìœ¨)ê°’ì„ ì§€ì •í•´ì£¼ëŠ”ë°, êµ°ì§‘ì˜ ê°œìˆ˜ëŠ” 4 ~ 10ê°œ, ë°ì´í„°ì˜ ê°œìˆ˜ëŠ” 3000 ~ 6000ê°œ ì‚¬ì´ê°€ ë˜ë„ë¡ ì„¤ì •í•œë‹¤.\n\ndef fcluster_plot_and_filtering(np_clusters, year, cci_type, t=30, ratio=0.5, alpha=0.3, size=4, xlim=70, ylim=70):\n    '''\n    np_clusters: np.array\n    year: int / 2019, 2020, 2021\n    cci_type: int / 1, 2, 3 \n    t: int / default: 30 (ë´ë“œë¡œê·¸ë¨ íŠ¸ë¦¬ì˜ ë†’ì´) \n    ratio: float / default:0.5 (1ì˜ ë¹„ìœ¨ì´ ì§€ì •)  \n    '''\n    \n    from scipy.cluster.hierarchy import fcluster # ì§€ì •í•œ í´ëŸ¬ìŠ¤í„° ìë¥´ê¸°\n    import pickle     \n    \n    \n    with open('all_dataset', 'rb') as handle: # Code, Date, Label ì •ë³´ê°€ ëª¨ë‘ ë“¤ì–´ìˆëŠ” ë°ì´í„°ì…‹ ë¶ˆëŸ¬ì˜¤ê¸°\n        dict_all_dataset = pickle.load(handle)\n        \n    with open(f'np_tsne_shap_{year}_{cci_type}', 'rb') as handle: # ì—°ë„, CCI êµ¬ê°„ì— ë§ëŠ” tsne ë°ì´í„°ì…‹ ë¶ˆëŸ¬ì˜¤ê¸°\n        np_tsne = pickle.load(handle)\n    \n    df_shap_cci = dict_all_dataset[cci_type][1]\n    df_shap_year = df_shap_cci[(df_shap_cci['Date'] >= f'{year}-01-01') & (df_shap_cci['Date'] <= f'{year}-12-31')].reset_index(drop=True) # ì—°ë„ì— ë§ëŠ” ë°ì´í„° í•„í„°ë§\n    \n    cut_tree = fcluster(np_clusters, t=t, criterion='distance') # êµ°ì§‘í™” ê²°ê³¼ ë°ì´í„° \n    print(\"êµ°ì§‘ì˜ ê°œìˆ˜:\", len(pd.DataFrame(cut_tree)[0].unique())) # êµ°ì§‘ì˜ ê°œìˆ˜ ì¶œë ¥ \n    \n    ##### í´ëŸ¬ìŠ¤í„°ë§ ì‹œê°í™” \n    fig = plt.figure(figsize=(15, 5))\n    ax1, ax2 = fig.subplots(1, 2)\n        \n    scatter = ax1.scatter(x=np_tsne[:, 0], y=np_tsne[:, 1], c=cut_tree, cmap='gist_rainbow', alpha=alpha, s=size) # êµ°ì§‘(cut_tree)ë³„ë¡œ ì‹œê°í™” \n    ax1.legend(*scatter.legend_elements())\n    ax1.set_title(f\"{year} Hierarchical Clustering\", fontsize=15)\n    \n    ##### ë¼ë²¨ 1ì˜ ë¹„ìœ¨ì„ ì‚¬ìš©í•œ í´ëŸ¬ìŠ¤í„° í•„í„°ë§: ìƒìŠ¹ ì¶”ì„¸ êµ°ì§‘ ì„ íƒ\n    df_tsne = pd.DataFrame(np_tsne, columns=['component1', 'component2'])\n    df_tsne['Code'], df_tsne['Date'], df_tsne['Label'], df_tsne['Cluster'] = df_shap_year['Code'], df_shap_year['Date'], df_shap_year['Label'], cut_tree\n    \n    gb = df_tsne.groupby('Cluster')['Label'].value_counts(sort=False).unstack() # êµ°ì§‘ ë³„ ë¼ë²¨ ê°œìˆ˜\n    idx_label_1 = gb[gb[1]/(gb[0]+gb[1]) > ratio].index # label 1ì˜ ë¹„ìœ¨ì´ ratio ì´ìƒì¸ êµ°ì§‘ ë²ˆí˜¸\n    print(f'label 1 > {ratio} êµ°ì§‘ ë²ˆí˜¸: ', idx_label_1)\n    df_tsne_1 = df_tsne[df_tsne['Cluster'].isin(idx_label_1)] # ë¼ë²¨ 1ì˜ ë¹„ìœ¨ì´ ratio ì´ìƒì¸ êµ°ì§‘ ì¶”ì¶œ (ìƒìŠ¹ ì¶”ì„¸ êµ°ì§‘)\n    print(\"ë°ì´í„°ì˜ ê°œìˆ˜:\", len(df_tsne_1))\n    print(\"ì¢…ëª©ì˜ ì¢…ë¥˜:\", df_tsne_1['Code'].nunique(), \" | \", \"ë‚ ì§œì˜ ì¢…ë¥˜: \", df_tsne_1['Date'].nunique())\n    \n    ##### ìƒìŠ¹ ì¶”ì„¸ êµ°ì§‘ ì‹œê°í™” \n    ax2.set_title(f\"label 1 > {ratio}\", fontsize=15)\n    scatter = ax2.scatter(df_tsne_1['component1'],df_tsne_1['component2'],c=df_tsne_1['Cluster'], cmap='gist_rainbow', s=3, alpha=0.4)\n    ax2.legend(*scatter.legend_elements())\n    ax2.set_ylim(-ylim, ylim) # tsne ë²”ìœ„ì™€ ë§ì¶”ê¸°\n    ax2.set_xlim(-xlim, xlim)\n        \n    return df_shap_year, df_tsne_1\n\n\nì£¼ ì»¬ëŸ¼ ìƒì„± ë° ë¹ˆë„ìˆ˜ ìƒìœ„ ì£¼ ì‹œê°í™” í•¨ìˆ˜\n\n[ì£¼ ì»¬ëŸ¼ ìƒì„± ì°¸ê³ ]\n-(Python) ê·¸ ë‚ ì§œê°€ ëª‡ ì£¼ì§¸ì¸ì§€ ê³„ì‚°í•˜ê¸°\n\ndef visualization_week(data_list, week_num=3, day_num=5): # ë¹ˆë„ìˆ˜ ìƒìœ„ ì£¼ ì‹œê°í™” \n    ##### ì£¼ ì»¬ëŸ¼ ìƒì„±\n    def get_week_no(target):  \n        from datetime import timedelta\n\n        firstday = target.replace(day=1)\n\n        if firstday.weekday() == 6:\n            origin = firstday\n        elif firstday.weekday() < 3:\n            origin = firstday - timedelta(days=firstday.weekday() + 1)\n        else:\n            origin = firstday + timedelta(days=6-firstday.weekday())\n\n        return f'{target.month}ì›” {(target - origin).days // 7 + 1}ì£¼ì°¨'\n\n    mpl.rcParams['font.family'] = 'NanumSquare'    \n    fig = plt.figure(figsize=(11, 5))\n    ax = fig.subplots(2, 3)\n    lst_year = [2019, 2020, 2021]\n\n    ##### ì—°ë„ ë³„ ìƒìœ„ ë¹ˆë„ìˆ˜ ì£¼, ë‚ ì§œ ì‹œê°í™” \n    for i, data in enumerate(data_list): # ì—°ë„ ë³„ ë°ì´í„°ì…‹ \n        data['Date'] = pd.to_datetime(data['Date']).dt.date # datetime type ë³€ê²½ \n        data['date_month_week'] = data['Date'].apply(get_week_no) # xxì›” xxì£¼ì°¨ ì»¬ëŸ¼ ìƒì„± \n        df_week = pd.DataFrame(data['date_month_week'].value_counts().head(week_num)).reset_index().rename(columns={'index':'month-week', 'date_month_week':'count'}) # ë¹ˆë„ìˆ˜ ìƒìœ„ ì£¼ì°¨ 5ê°œ \n        df_day = pd.DataFrame(data['Date'].value_counts().head(day_num)).reset_index().rename(columns={'index':'Date', 'Date':'count'}) # ë¹ˆë„ìˆ˜ ìƒìœ„ ì£¼ì°¨ 5ê°œ \n        \n        ax[0, i].set_title(f\"<{lst_year[i]}ë…„ ë¹ˆë„ìˆ˜ ìƒìœ„ ì£¼>\")\n        sns.barplot(data=df_week, x='count', y='month-week', palette=\"Pastel1\", ax=ax[0, i])\n\n        for height, p in enumerate(ax[0, i].patches):\n            width = p.get_width()\n            ax[0, i].text(width, height+0.2, f'{round(p.get_width())}', ha = 'center', size = 13)\n\n        ax[1, i].set_title(f\"<{lst_year[i]}ë…„ ë¹ˆë„ìˆ˜ ìƒìœ„ ë‚ ì§œ>\")\n        sns.barplot(data=df_day, x='count', y='Date', palette=\"Pastel1\", ax=ax[1, i])\n\n        for height, p in enumerate(ax[1, i].patches):\n            width = p.get_width()\n            ax[1, i].text(width , height+0.2, f'{round(p.get_width())}', ha = 'center', size = 13)\n    \n    plt.xlabel(\"\")\n    plt.ylabel(\"\")\n    \n    plt.tight_layout()\n    plt.show()\n\n\n\n\n\n\n\në´ë“œë¡œê·¸ë¨\n\n\nclusters_2019_1 = hierarchical_clustering_plot(method='average', year=2019, cci_type=1, dendrogram=True)\n\nlinkage complete\n\n\n\n\n\n\nê³„ì¸µì  í´ëŸ¬ìŠ¤í„°ë§ & ìƒìŠ¹ ì¶”ì„¸ êµ°ì§‘ ì„ íƒ\n\n\ndf_shap_2019_1, df_tsne_2019_1_1 = fcluster_plot_and_filtering(clusters_2019_1, 2019, 1, t=13, ratio=0.22, xlim=80, ylim=80)\n\nêµ°ì§‘ì˜ ê°œìˆ˜: 53\nlabel 1 > 0.22 êµ°ì§‘ ë²ˆí˜¸:  Int64Index([21, 23, 24, 26, 27, 36, 47, 49], dtype='int64', name='Cluster')\në°ì´í„°ì˜ ê°œìˆ˜: 4313\nì¢…ëª©ì˜ ì¢…ë¥˜: 1024  |  ë‚ ì§œì˜ ì¢…ë¥˜:  246\n\n\n\n\n\n\n\n\n\n\në´ë“œë¡œê·¸ë¨\n\n\nclusters_2020_1 = hierarchical_clustering_plot(method='average', year=2020, cci_type=1, dendrogram=True)\n\nlinkage complete\n\n\n\n\n\n\nê³„ì¸µì  í´ëŸ¬ìŠ¤í„°ë§ & ìƒìŠ¹ ì¶”ì„¸ êµ°ì§‘ ì„ íƒ\n\n\ndf_shap_2020_1, df_tsne_2020_1_1 = fcluster_plot_and_filtering(clusters_2020_1, 2020, 1, t=13, ratio=0.29, xlim=80, ylim=80)\n\nêµ°ì§‘ì˜ ê°œìˆ˜: 53\nlabel 1 > 0.29 êµ°ì§‘ ë²ˆí˜¸:  Int64Index([24, 26, 27, 30, 38, 40, 41, 42, 43], dtype='int64', name='Cluster')\në°ì´í„°ì˜ ê°œìˆ˜: 4827\nì¢…ëª©ì˜ ì¢…ë¥˜: 1306  |  ë‚ ì§œì˜ ì¢…ë¥˜:  247\n\n\n\n\n\n\n\n\n\n\në´ë“œë¡œê·¸ë¨\n\n\nclusters_2021_1 = hierarchical_clustering_plot(method='average', year=2021, cci_type=1, dendrogram=True)\n\nlinkage complete\n\n\n\n\n\n\nê³„ì¸µì  í´ëŸ¬ìŠ¤í„°ë§ & ìƒìŠ¹ ì¶”ì„¸ êµ°ì§‘ ì„ íƒ\n\n\ndf_shap_2021_1, df_tsne_2021_1_1 = fcluster_plot_and_filtering(clusters_2021_1, 2021, 1, t=13, ratio=0.207)\n\nêµ°ì§‘ì˜ ê°œìˆ˜: 51\nlabel 1 > 0.207 êµ°ì§‘ ë²ˆí˜¸:  Int64Index([3, 4, 13, 18, 21, 22, 24, 25], dtype='int64', name='Cluster')\në°ì´í„°ì˜ ê°œìˆ˜: 4237\nì¢…ëª©ì˜ ì¢…ë¥˜: 1092  |  ë‚ ì§œì˜ ì¢…ë¥˜:  237\n\n\n\n\n\n\n\n\n\n\nì—°ë„ë³„ ë¹ˆë„ìˆ˜ ìƒìœ„ ì£¼ & ë‚ ì§œ\n\n\nlst_data = [df_tsne_2019_1_1, df_tsne_2020_1_1, df_tsne_2021_1_1]\nvisualization_week(data_list=lst_data, week_num=5, day_num=5)\n\n\n\n\nì¤‘ë¦½êµ¬ê°„ì—ì„œ ìƒìŠ¹ ì¶”ì„¸ êµ°ì§‘ ë°ì´í„°ì˜ ë‚ ì§œ ë¹ˆë„ìˆ˜ëŠ” 2019ë…„ë„ëŠ” 8ì›” 3ì£¼ì°¨, 2020ë…„ë„ëŠ” 4ì›” 1ì£¼ì°¨, 2021ë…„ë„ëŠ” 2ì›” 1ì£¼ì°¨ê°€ ê°€ì¥ ë†’ê²Œ ë‚˜ì™”ë‹¤. ê·¸ ì¤‘ì—ì„œë„ ê°€ì¥ í° ë‚ ì§œ ë¹ˆë„ ì°¨ì´ë¥¼ ë³´ì¸ ì—°ë„ëŠ” 2020ë…„ì´ì—ˆë‹¤.\n\n\nì—°ë„ë³„ ìƒìœ„ ë‚ ì§œì˜ ê°œë³„ ì¢…ëª© ì°¨íŠ¸ í™•ì¸\n\n\nì—°ë„ë³„ ìƒìœ„ ë‚ ì§œì˜ ëœë¤ ì¢…ëª©ì½”ë“œ ë°ì´í„°í”„ë ˆì„ ìƒì„±\n\nìƒìœ„ ë‚ ì§œë“¤ì„ ë°”ê¿”ê°€ë©° ì‹¤í–‰í•˜ê³ , ëœë¤ìœ¼ë¡œ ë‚˜ì˜¤ëŠ” ì¢…ëª©ì½”ë“œë“¤ì˜ ê°œë³„ ì¢…ëª© ì°¨íŠ¸ë¥¼ í™•ì¸í•œë‹¤.\n\n#collapse-hide\nimport datetime\ndf_tsne_2019_1_1['Date'] = df_tsne_2019_1_1['Date'].astype(str)\ndf_tsne_2020_1_1['Date'] = df_tsne_2020_1_1['Date'].astype(str)\ndf_tsne_2021_1_1['Date'] = df_tsne_2021_1_1['Date'].astype(str)\n\ndf1 = df_tsne_2019_1_1.loc[df_tsne_2019_1_1['Date'] == '2019-08-20', ['Date', 'Code']].sample(5).reset_index(drop=True)\ndf2 = df_tsne_2020_1_1.loc[df_tsne_2020_1_1['Date'] == '2020-04-01', ['Date', 'Code']].sample(5).reset_index(drop=True)\ndf3 = df_tsne_2021_1_1.loc[df_tsne_2021_1_1['Date'] == '2021-02-01', ['Date', 'Code']].sample(5).reset_index(drop=True)\n\npd.concat([df1, df2, df3], axis=1)\n\n\n\n\n\n  \n    \n      \n      Date\n      Code\n      Date\n      Code\n      Date\n      Code\n    \n  \n  \n    \n      0\n      2019-08-20\n      007540\n      2020-04-01\n      160980\n      2021-02-01\n      023800\n    \n    \n      1\n      2019-08-20\n      047400\n      2020-04-01\n      119850\n      2021-02-01\n      033240\n    \n    \n      2\n      2019-08-20\n      010820\n      2020-04-01\n      041190\n      2021-02-01\n      038110\n    \n    \n      3\n      2019-08-20\n      020120\n      2020-04-01\n      024110\n      2021-02-01\n      039020\n    \n    \n      4\n      2019-08-20\n      081150\n      2020-04-01\n      002900\n      2021-02-01\n      001810\n    \n  \n\n\n\n\n\nì°¨íŠ¸ í™•ì¸ ì˜ˆì‹œ\n\n2019ë…„ 8ì›” 20ì¼\n\n2020ë…„ 4ì›” 1ì¼\n\n2021ë…„ 2ì›” 1ì¼\n\nì—¬ëŸ¬ê°œì˜ ì°¨íŠ¸ í™•ì¸ì„ í•´ë³´ì•˜ì„ ë•Œ, ê°€ì¥ í° ë‚ ì§œ í¸ì¤‘ì„ ë³´ì˜€ë˜ 2020ë…„ë„ëŠ” ì°¨íŠ¸ì˜ íŒ¨í„´ì´ ê°€ì¥ ì¼ì •í•˜ê²Œ ë‚˜íƒ€ë‚¬ìœ¼ë©°, 2019,2021ë…„ë„ëŠ” 2020ë…„ë„ ë§Œí¼ ì¼ì •í•œ íŒ¨í„´ì„ ë³´ì´ì§€ëŠ” ì•Šì•˜ë‹¤. í•˜ì§€ë§Œ ì„¸ ì—°ë„ì—ì„œ ê³µí†µì ìœ¼ë¡œ ë‚˜ì˜¤ëŠ” íŒ¨í„´ì´ ì¡´ì¬í–ˆë‹¤. í•´ë‹¹ íŒ¨í„´ì„ ë¶„ì„í•´ë³´ì•˜ì„ ë•Œ, ìœ„ì˜ ì„¸ ì‚¬ì§„ê³¼ ê°™ì´, í•˜ë½ ì¶”ì„¸ì—ì„œ ìƒìŠ¹ ì¶”ì„¸ë¡œ ì „í™˜ë˜ëŠ” Vì í˜•íƒœì˜€ìœ¼ë©°, ê·¸ ì¤‘ì—ì„œë„ ê¸°ì¤€ì¼(D0)[íšŒìƒ‰ ì„ ]ì´ ì˜¤ë¥¸ìª½ì— ìœ„ì¹˜í•œë‹¤ëŠ” ê³µí†µì ì´ ìˆì—ˆë‹¤.\n \n\n\n\n\n\n\n\në´ë“œë¡œê·¸ë¨\n\n\nclusters_2019_2 = hierarchical_clustering_plot(method='average', year=2019, cci_type=2, dendrogram=True)\n\nlinkage complete\n\n\n\n\n\n\nê³„ì¸µì  í´ëŸ¬ìŠ¤í„°ë§ & ìƒìŠ¹ ì¶”ì„¸ êµ°ì§‘ ì„ íƒ\n\n\ndf_shap_2019_2, df_tsne_2019_1_2 = fcluster_plot_and_filtering(clusters_2019_2, 2019, 2, t=12, ratio=0.25)\n\nêµ°ì§‘ì˜ ê°œìˆ˜: 48\nlabel 1 > 0.25 êµ°ì§‘ ë²ˆí˜¸:  Int64Index([8, 16, 17, 21, 30], dtype='int64', name='Cluster')\në°ì´í„°ì˜ ê°œìˆ˜: 5016\nì¢…ëª©ì˜ ì¢…ë¥˜: 1023  |  ë‚ ì§œì˜ ì¢…ë¥˜:  246\n\n\n\n\n\n\n\n\n\n\në´ë“œë¡œê·¸ë¨\n\n\nclusters_2020_2 = hierarchical_clustering_plot(method='average', year=2020, cci_type=2, dendrogram=True)\n\nlinkage complete\n\n\n\n\n\n\nê³„ì¸µì  í´ëŸ¬ìŠ¤í„°ë§ & ìƒìŠ¹ ì¶”ì„¸ êµ°ì§‘ ì„ íƒ\n\n\ndf_shap_2020_2, df_tsne_2020_1_2 = fcluster_plot_and_filtering(clusters_2020_2, 2020, 2, t=11, ratio=0.31, xlim=65, ylim=65)\n\nêµ°ì§‘ì˜ ê°œìˆ˜: 55\nlabel 1 > 0.31 êµ°ì§‘ ë²ˆí˜¸:  Int64Index([1, 14, 16, 22, 36, 39], dtype='int64', name='Cluster')\në°ì´í„°ì˜ ê°œìˆ˜: 5457\nì¢…ëª©ì˜ ì¢…ë¥˜: 1190  |  ë‚ ì§œì˜ ì¢…ë¥˜:  247\n\n\n\n\n\n\n\n\n\n\në´ë“œë¡œê·¸ë¨\n\n\nclusters_2021_2 = hierarchical_clustering_plot(method='average', year=2021, cci_type=2, dendrogram=True)\n\nlinkage complete\n\n\n\n\n\n\nê³„ì¸µì  í´ëŸ¬ìŠ¤í„°ë§ & ìƒìŠ¹ ì¶”ì„¸ êµ°ì§‘ ì„ íƒ\n\n\ndf_shap_2021_2, df_tsne_2021_1_2 = fcluster_plot_and_filtering(clusters_2021_2, 2021, 2, t=12, ratio=0.268)\n\nêµ°ì§‘ì˜ ê°œìˆ˜: 52\nlabel 1 > 0.268 êµ°ì§‘ ë²ˆí˜¸:  Int64Index([5, 9, 14, 16, 31, 32, 43, 51], dtype='int64', name='Cluster')\në°ì´í„°ì˜ ê°œìˆ˜: 5632\nì¢…ëª©ì˜ ì¢…ë¥˜: 1110  |  ë‚ ì§œì˜ ì¢…ë¥˜:  237\n\n\n\n\n\n\n\n\n\n\nì—°ë„ë³„ ë¹ˆë„ìˆ˜ ìƒìœ„ ì£¼ & ë‚ ì§œ\n\n\nlst_data = [df_tsne_2019_1_2, df_tsne_2020_1_2, df_tsne_2021_1_2]\nvisualization_week(data_list=lst_data, week_num=5, day_num=5)\n\n\n\n\nê³¼ë§¤ìˆ˜êµ¬ê°„ì—ì„œ ë¹ˆë„ìˆ˜ê°€ ë†’ì€ ë‚ ì§œëŠ” 2019ë…„ 1ì›” 4ì£¼ì°¨, 2020ë…„ 4ì›” 3ì£¼ì°¨, 2021ë…„ 1ì›” 3ì£¼ì°¨ì„ì„ ì•Œ ìˆ˜ ìˆ ìŠµë‹ˆë‹¤. ë‹¤ë¥¸ CCI êµ¬ê°„ì— ë¹„í•´ ìƒìœ„ ë¹ˆë„ìˆ˜ì˜ í¬ê¸° ì°¨ì´ê°€ ë§ì§€ ì•Šì•˜ë‹¤. ê°€ì¥ ë†’ì€ ë‚ ì§œ ë¹ˆë„ ì°¨ì´ë¥¼ ë³´ì´ëŠ” ì—°ë„ëŠ” ì¤‘ë¦½êµ¬ê°„ê³¼ ë§ˆì°¬ê°€ì§€ë¡œ 2020ë…„ë„ì˜€ë‹¤.\n\n\nì—°ë„ë³„ ìƒìœ„ ë‚ ì§œì˜ ê°œë³„ ì¢…ëª© ì°¨íŠ¸ í™•ì¸\n\n\nì—°ë„ë³„ ìƒìœ„ ë‚ ì§œì˜ ëœë¤ ì¢…ëª©ì½”ë“œ ë°ì´í„°í”„ë ˆì„ ìƒì„±\n\nìƒìœ„ ë‚ ì§œë“¤ì„ ë°”ê¿”ê°€ë©° ì‹¤í–‰í•˜ê³ , ëœë¤ìœ¼ë¡œ ë‚˜ì˜¤ëŠ” ì¢…ëª©ì½”ë“œë“¤ì˜ ê°œë³„ ì¢…ëª© ì°¨íŠ¸ë¥¼ í™•ì¸í•œë‹¤.\n\n#collapse-hide\nimport datetime\ndf_tsne_2019_1_2['Date'] = df_tsne_2019_1_2['Date'].astype(str)\ndf_tsne_2020_1_2['Date'] = df_tsne_2020_1_2['Date'].astype(str)\ndf_tsne_2021_1_2['Date'] = df_tsne_2021_1_2['Date'].astype(str)\n\ndf1 = df_tsne_2019_1_2.loc[df_tsne_2019_1_2['Date'] == '2019-01-17', ['Date', 'Code']].sample(5).reset_index(drop=True)\ndf2 = df_tsne_2020_1_2.loc[df_tsne_2020_1_2['Date'] == '2020-04-17', ['Date', 'Code']].sample(5).reset_index(drop=True)\ndf3 = df_tsne_2021_1_2.loc[df_tsne_2021_1_2['Date'] == '2021-01-20', ['Date', 'Code']].sample(5).reset_index(drop=True)\n\npd.concat([df1, df2, df3], axis=1)\n\n\n\n\n\n  \n    \n      \n      Date\n      Code\n      Date\n      Code\n      Date\n      Code\n    \n  \n  \n    \n      0\n      2019-01-17\n      042370\n      2020-04-17\n      008250\n      2021-01-20\n      095340\n    \n    \n      1\n      2019-01-17\n      187220\n      2020-04-17\n      017550\n      2021-01-20\n      025880\n    \n    \n      2\n      2019-01-17\n      024900\n      2020-04-17\n      035620\n      2021-01-20\n      011500\n    \n    \n      3\n      2019-01-17\n      106240\n      2020-04-17\n      011370\n      2021-01-20\n      010690\n    \n    \n      4\n      2019-01-17\n      007390\n      2020-04-17\n      048430\n      2021-01-20\n      000270\n    \n  \n\n\n\n\n\nì°¨íŠ¸ í™•ì¸ ì˜ˆì‹œ\n\n2019ë…„ 1ì›” 17ì¼\n\n2020ë…„ 4ì›” 17ì¼\n\n2021ë…„ 1ì›” 20ì¼\n\nê³¼ë§¤ìˆ˜êµ¬ê°„ì€ 20ì¼ ì´ë™í‰ê· ì„ ì˜ ìœ„ì— ê·¹ë‹¨ì ìœ¼ë¡œ ë–¨ì–´ì ¸ìˆëŠ” ë°ì´í„°ë“¤ì´ë¯€ë¡œ, ê¸°ì¤€ì¼(D0)[íšŒìƒ‰ ì„ ]ì´ ìƒìŠ¹ì¶”ì„¸ì—ì„œì˜ ì¤‘ê°„ ~ ë ë¬´ë µì— ìœ„ì¹˜í•˜ì˜€ë‹¤. ë‹¤ë¥¸ CCI êµ¬ê°„ë³´ë‹¤ íŒ¨í„´ì´ ê°€ì¥ ë¶ˆê·œì¹™ì ì´ì—ˆì§€ë§Œ, í•˜ë½ì¶”ì„¸ì—ì„œ ìƒìŠ¹ì¶”ì„¸ë¡œ ì „í™˜ë˜ëŠ” Vì í˜•íƒœì—ì„œ ìƒìŠ¹ì¶”ì„¸ ëë¬´ë µì— ìœ„ì¹˜í•˜ëŠ” ê³µí†µëœ íŒ¨í„´ì„ ì¼ë¶€ ë°ì´í„°ì—ì„œ ê²€ì¶œí•  ìˆ˜ ìˆì—ˆë‹¤.\n \n\n\n\n\n\n\n\në´ë“œë¡œê·¸ë¨\n\n\nclusters_2019_3 = hierarchical_clustering_plot(method='average', year=2019, cci_type=3, dendrogram=True)\n\nlinkage complete\n\n\n\n\n\n\nê³„ì¸µì  í´ëŸ¬ìŠ¤í„°ë§ & ìƒìŠ¹ ì¶”ì„¸ êµ°ì§‘ ì„ íƒ\n\n\ndf_shap_2019_3, df_tsne_2019_1_3 = fcluster_plot_and_filtering(clusters_2019_3, 2019, 3, t=10, ratio=0.365)\n\nêµ°ì§‘ì˜ ê°œìˆ˜: 58\nlabel 1 > 0.365 êµ°ì§‘ ë²ˆí˜¸:  Int64Index([1, 2, 25, 26, 50], dtype='int64', name='Cluster')\në°ì´í„°ì˜ ê°œìˆ˜: 3959\nì¢…ëª©ì˜ ì¢…ë¥˜: 994  |  ë‚ ì§œì˜ ì¢…ë¥˜:  222\n\n\n\n\n\n\n\n\n\n\në´ë“œë¡œê·¸ë¨\n\n\nclusters_2020_3 = hierarchical_clustering_plot(method='average', year=2020, cci_type=3, dendrogram=True)\n\nlinkage complete\n\n\n\n\n\n\nê³„ì¸µì  í´ëŸ¬ìŠ¤í„°ë§ & ìƒìŠ¹ ì¶”ì„¸ êµ°ì§‘ ì„ íƒ\n\n\ndf_shap_2020_3, df_tsne_2020_1_3 = fcluster_plot_and_filtering(clusters_2020_3, 2020, 3, t=12, ratio=0.5, xlim=80, ylim=80)\n\nêµ°ì§‘ì˜ ê°œìˆ˜: 47\nlabel 1 > 0.5 êµ°ì§‘ ë²ˆí˜¸:  Int64Index([2, 3, 15, 28, 37], dtype='int64', name='Cluster')\në°ì´í„°ì˜ ê°œìˆ˜: 3874\nì¢…ëª©ì˜ ì¢…ë¥˜: 1356  |  ë‚ ì§œì˜ ì¢…ë¥˜:  78\n\n\n\n\n\n\n\n\n\n\në´ë“œë¡œê·¸ë¨\n\n\nclusters_2021_3 = hierarchical_clustering_plot(method='average', year=2021, cci_type=3, dendrogram=True)\n\nlinkage complete\n\n\n\n\n\n\nê³„ì¸µì  í´ëŸ¬ìŠ¤í„°ë§ & ìƒìŠ¹ ì¶”ì„¸ êµ°ì§‘ ì„ íƒ\n\n\ndf_shap_2021_3, df_tsne_2021_1_3 = fcluster_plot_and_filtering(clusters_2021_3, 2021, 3, t=10, ratio=0.35)\n\nêµ°ì§‘ì˜ ê°œìˆ˜: 53\nlabel 1 > 0.35 êµ°ì§‘ ë²ˆí˜¸:  Int64Index([5, 7, 10, 15], dtype='int64', name='Cluster')\në°ì´í„°ì˜ ê°œìˆ˜: 4174\nì¢…ëª©ì˜ ì¢…ë¥˜: 1163  |  ë‚ ì§œì˜ ì¢…ë¥˜:  230\n\n\n\n\n\n\n\n\n\n\nì—°ë„ë³„ ë¹ˆë„ìˆ˜ ìƒìœ„ ì£¼ & ë‚ ì§œ\n\n\nlst_data = [df_tsne_2019_1_3, df_tsne_2020_1_3, df_tsne_2021_1_3]\nvisualization_week(data_list=lst_data, week_num=5, day_num=5)\n\n\n\n\nê³¼ë§¤ë„êµ¬ê°„ì˜ ë¹ˆë„ìˆ˜ ìƒìœ„ ë‚ ì§œëŠ” 2019ë…„ 8ì›” 1ì£¼ì°¨, 2020ë…„ 3ì›” 4ì£¼ì°¨, 2021ë…„ 10ì›” 1ì£¼ì°¨ì˜€ë‹¤. CCI êµ¬ê°„ ì¤‘ ìœ ì¼í•˜ê²Œ ëª¨ë“  ì—°ë„ì—ì„œ ë†’ì€ ë‚ ì§œ ë¹ˆë„ ì°¨ì´ë¥¼ ë³´ì˜€ë‹¤.\n\n\nì—°ë„ë³„ ìƒìœ„ ë‚ ì§œì˜ ê°œë³„ ì¢…ëª© ì°¨íŠ¸ í™•ì¸\n\n\nì—°ë„ë³„ ìƒìœ„ ë‚ ì§œì˜ ëœë¤ ì¢…ëª©ì½”ë“œ ë°ì´í„°í”„ë ˆì„ ìƒì„±\n\nìƒìœ„ ë‚ ì§œë“¤ì„ ë°”ê¿”ê°€ë©° ì‹¤í–‰í•˜ê³ , ëœë¤ìœ¼ë¡œ ë‚˜ì˜¤ëŠ” ì¢…ëª©ì½”ë“œë“¤ì˜ ê°œë³„ ì¢…ëª© ì°¨íŠ¸ë¥¼ í™•ì¸í•œë‹¤.\n\n#collapse-hide\nimport datetime\ndf_tsne_2019_1_3['Date'] = df_tsne_2019_1_3['Date'].astype(str)\ndf_tsne_2020_1_3['Date'] = df_tsne_2020_1_3['Date'].astype(str)\ndf_tsne_2021_1_3['Date'] = df_tsne_2021_1_3['Date'].astype(str)\n\ndf1 = df_tsne_2019_1_3.loc[df_tsne_2019_1_3['Date'] == '2019-08-06', ['Date', 'Code']].sample(5).reset_index(drop=True)\ndf2 = df_tsne_2020_1_3.loc[df_tsne_2020_1_3['Date'] == '2020-03-19', ['Date', 'Code']].sample(5).reset_index(drop=True)\ndf3 = df_tsne_2021_1_3.loc[df_tsne_2021_1_3['Date'] == '2021-10-07', ['Date', 'Code']].sample(5).reset_index(drop=True)\n\npd.concat([df1, df2, df3], axis=1)\n\n\n\n\n\n  \n    \n      \n      Date\n      Code\n      Date\n      Code\n      Date\n      Code\n    \n  \n  \n    \n      0\n      2019-08-06\n      033100\n      2020-03-19\n      126880\n      2021-10-07\n      028300\n    \n    \n      1\n      2019-08-06\n      060480\n      2020-03-19\n      000850\n      2021-10-07\n      088130\n    \n    \n      2\n      2019-08-06\n      014190\n      2020-03-19\n      170920\n      2021-10-07\n      042110\n    \n    \n      3\n      2019-08-06\n      123860\n      2020-03-19\n      189860\n      2021-10-07\n      081000\n    \n    \n      4\n      2019-08-06\n      067990\n      2020-03-19\n      079160\n      2021-10-07\n      002690\n    \n  \n\n\n\n\n\nì°¨íŠ¸ í™•ì¸ ì˜ˆì‹œ\n\n2019ë…„ 8ì›” 6ì¼\n\n2020ë…„ 3ì›” 19ì¼\n\n2021ë…„ 10ì›” 17ì¼\n\nìœ„ì˜ ì‚¬ì§„ê³¼ ê°™ì´, ëª¨ë“  ì—°ë„ì—ì„œ ë¹„ìŠ·í•œ íŒ¨í„´ì„ ë³´ì˜€ë‹¤. í•˜ë½ì¶”ì„¸ì—ì„œ ìƒìŠ¹ì¶”ì„¸ë¡œ ì „í™˜ë˜ëŠ” Vì í˜•íƒœì˜ ê³µí†µëœ íŒ¨í„´ì—ì„œ, ê¸°ì¤€ì¼(D0)[íšŒìƒ‰ ì„ ]ì´ ê¼­ì§€ì  ê·¼ì²˜ì— ìœ„ì¹˜í•˜ì˜€ë‹¤.\n \n\n\n\n\n\nì§€ê¸ˆê¹Œì§€ ë¶„ë¥˜ ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸í•™ìŠµ, SHAP í‘œì¤€í™”, í´ëŸ¬ìŠ¤í„°ë§ ë¶„ì„ì˜ ê³¼ì •ì„ ê±°ì³ 10ì¼ ê°„ì˜ ì£¼ê°€ ì‹œê³„ì—´ ë°ì´í„°ë¡œë¶€í„° ì£¼ê°€ ìƒìŠ¹ ì¶”ì„¸ íŒ¨í„´ì„ ê²€ì¶œí•˜ì˜€ë‹¤.\nCCI êµ¬ê°„ ë³„, ì—°ë„ ë³„ë¡œ ë‚˜ëˆ„ì–´ ì—°êµ¬ë¥¼ ì§„í–‰í•˜ì˜€ëŠ”ë°,\n1) CCI êµ¬ê°„ ë³„ ê³µí†µì ìœ¼ë¡œ ë‚˜íƒ€ë‚˜ëŠ” íŒ¨í„´ì€ í•˜ë½ì¶”ì„¸ì—ì„œ ìƒìŠ¹ì¶”ì„¸ë¡œ ì „í™˜ë˜ëŠ” Vì ìƒìŠ¹ë°˜ì „í˜• íŒ¨í„´ì´ì—ˆìœ¼ë©°, ê¸°ì¤€ì¼(D0)ì˜ ìœ„ì¹˜ê°€ CCI êµ¬ê°„ì— ë”°ë¼ ë‹¤ë¥´ê²Œ ë‚˜íƒ€ë‚¬ë‹¤. íŠ¹íˆë‚˜ ê³¼ë§¤ë„êµ¬ê°„ì—ì„œì˜ íŒ¨í„´ì´ ë‹¤ë¥¸ êµ¬ê°„ì— ë¹„í•´ ê°€ì¥ ì •í™•í•˜ê³  ìœ ì‚¬í•œ ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆì—ˆë‹¤.\n2) ëª¨ë“  CCI êµ¬ê°„ì„ í†µí‹€ì–´ 2019, 2020, 2021ë…„ë„ ì¤‘ ë†’ì€ ë¹ˆë„ë¡œ ë‚˜íƒ€ë‚˜ëŠ” ë‚ ì§œì˜ í¸ì¤‘ì´ ê°€ì¥ í° ì—°ë„ëŠ” 2020ë…„ì´ì—ˆìœ¼ë©°, 2020ë…„ë„ì—ì„œ íŒ¨í„´ì˜ ëª¨ì–‘ ë˜í•œ ê°€ì¥ ìœ ì‚¬í•˜ê²Œ ë‚˜íƒ€ë‚¬ë‹¤."
  },
  {
    "objectID": "posts/c01.EDA.html#eda",
    "href": "posts/c01.EDA.html#eda",
    "title": "[cropdoctor] 1. ë…¸ì§€ì‘ë¬¼ ë°ì´í„° EDA",
    "section": "1. EDA",
    "text": "1. EDA\n\nì‚¬ìš© ë°ì´í„°ì…‹\në…¸ì§€ ì‘ë¬¼ ì§ˆë³‘ ì§„ë‹¨ ì´ë¯¸ì§€\nì¸ê³µì§€ëŠ¥ ê¸°ë°˜ì˜ ì›¹ ì„œë¹„ìŠ¤ ê°œë°œ í”„ë¡œì íŠ¸ì—ì„œ ì‘ë¬¼ì˜ ì§ˆë³‘ì„ ì§„ë‹¨ ì£¼ì œë¥¼ ì„ ì •í–ˆìŠµë‹ˆë‹¤. í•´ë‹¹ ê¸°ëŠ¥ì´ ì›¹ ì„œë¹„ìŠ¤ì˜ ë©”ì¸ì´ë©°, ê·¸ì— ë”°ë¼ ì‘ë¬¼ ì§ˆë³‘ ì§„ë‹¨ ì¸ê³µì§€ëŠ¥ì„ ê°œë°œí•©ë‹ˆë‹¤. 609GBë‚˜ ë˜ëŠ” ëŒ€ìš©ëŸ‰ ì´ë¯¸ì§€ ë°ì´í„°ì—¬ì„œ ëª¨ë“  ë°ì´í„°ë¥¼ ë‹¤ìš´ë°›ì§€ëŠ” ëª» í–ˆì§€ë§Œ, íŒ€ì›ë“¤ê³¼ ìš°ì—¬ê³¡ì ˆ ëì— í•™ìŠµì„ ì§„í–‰í•  ìˆ˜ ìˆì„ ë§Œí¼ì˜ ë°ì´í„° ë‹¤ìš´ë¡œë“œëŠ” ì„±ê³µí–ˆìŠµë‹ˆë‹¤. ê²°ë¡ ì ìœ¼ë¡œ, í”„ë¡œì íŠ¸ í™˜ê²½ì˜ ë””ìŠ¤í¬ ìµœëŒ€ ìš©ëŸ‰ì´ 300GBì¸ ê²ƒì„ ê³ ë ¤í•˜ì—¬ ì¦ê°• ë°ì´í„°ë¥¼ ëº€ ì•½ 200GB ì¤‘ ë‹¤ìš´ë¡œë“œë¥¼ ì„±ê³µí•œ ë°ì´í„° ì•½ 150GBë¥¼ ì‚¬ìš©í•˜ì—¬ ì§„í–‰í•˜ì˜€ìŠµë‹ˆë‹¤.\n\nimport os\nimport zipfile\nimport random \n\nfrom PIL import Image, ImageDraw\nimport json\nimport pickle\nfrom tqdm import tqdm\nimport numpy as np \n\nimport matplotlib.pyplot as plt\nplt.rc(\"font\", family=\"NanumGothic\", size=13) \n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nìˆ˜ì§‘í•œ ë°ì´í„°ëŠ” ë¼ë²¨ë°ì´í„°ì™€ ì›ì²œ(ì´ë¯¸ì§€) ë°ì´í„°ë¡œ ì´ë£¨ì–´ì ¸ìˆìœ¼ë©°, ë‹¤ìŒê³¼ ê°™ì€ í´ë” êµ¬ì¡°ë¡œ VMì— ì ì¬í•˜ì˜€ìŠµë‹ˆë‹¤.\ndata\n    |_ training  \n        |_ image_class   \n            |_ ê³ ì¶” ì •ìƒ\n            |_ ê³ ì¶” ì§ˆë³‘ 1 \n            |_ ...\n        |_ label  \n\n    |_ validation   \n        |_ image_class   \n            |_ ê³ ì¶” ì •ìƒ\n            |_ ê³ ì¶” ì§ˆë³‘ 1 \n            |_ ...\n        |_ label  \n    \n\n# ê²½ë¡œ ì„¤ì •  \npath_train_img = \"data/training/image_class/\"\npath_train_label = \"data/training/label/\"\n\npath_valid_img = \"data/validation/image_class/\"\npath_valid_label = \"data/validation/label/\"\n\n# ê²½ë¡œì— ë“¤ì–´ìˆëŠ” íŒŒì¼ ë¦¬ìŠ¤íŠ¸\nlst_train_img = os.listdir(path_train_img)\nlst_train_label = os.listdir(path_train_label)\n\nlst_valid_img = os.listdir(path_valid_img)\nlst_valid_label = os.listdir(path_valid_label)\n\ní´ë” êµ¬ì¡°ì— ë§ê²Œ ê²½ë¡œë¥¼ ì„¤ì •í•´ì£¼ê³ , ê·¸ ì•ˆì— ë“¤ì–´ìˆëŠ” íŒŒì¼ì´ë¦„ì„ ê° ë³€ìˆ˜ì— ë¦¬ìŠ¤íŠ¸ë¡œ ë‹´ì•„ì¤ë‹ˆë‹¤."
  },
  {
    "objectID": "posts/c01.EDA.html#ë¼ë²¨-ë°ì´í„°-ì˜ˆì‹œ",
    "href": "posts/c01.EDA.html#ë¼ë²¨-ë°ì´í„°-ì˜ˆì‹œ",
    "title": "[cropdoctor] 1. ë…¸ì§€ì‘ë¬¼ ë°ì´í„° EDA",
    "section": "ë¼ë²¨ ë°ì´í„° ì˜ˆì‹œ",
    "text": "ë¼ë²¨ ë°ì´í„° ì˜ˆì‹œ\në¨¼ì €, ë¼ë²¨ ë°ì´í„° í•˜ë‚˜ê°€ ì–´ë–»ê²Œ ìƒê²¼ëŠ”ì§€ í™•ì¸í•´ë´…ë‹ˆë‹¤.\n\nmyzip_r = zipfile.ZipFile(path_train_label+\"[á„…á…¡á„‡á…¦á†¯]á„‹á…µá‡á„†á…¡á„…á…³á†·á„‡á…§á†¼(á„á…©á„†á…¡á„á…©)_1.á„Œá…µá†¯á„‡á…§á†¼.zip\", 'r')\n\nprint(myzip_r.namelist()[:3])\n\njson_str = myzip_r.read('V006_79_1_15_07_03_12_1_2656z_20201104_19.jpg.json')\n\njson.loads(json_str)\n\n['V006_79_1_15_07_03_12_1_2656z_20201104_19.jpg.json', 'V006_79_1_15_07_03_12_1_2656z_20201104_37.jpg.json', 'V006_79_1_15_07_03_12_1_2656z_20201105_38.jpg.json']\n\n\n{'description': {'image': 'V006_79_1_15_07_03_12_1_2656z_20201104_19.jpg',\n  'date': '2020/11/04',\n  'worker': '',\n  'height': 3024,\n  'width': 4032,\n  'task': 79,\n  'type': 1,\n  'region': 5},\n 'annotations': {'disease': 15,\n  'crop': 7,\n  'area': 3,\n  'grow': 12,\n  'risk': 1,\n  'points': [{'xtl': 1746, 'ytl': 1472, 'xbr': 2528, 'ybr': 3024}]}}\n\n\nìœ„ì™€ ê°™ì´ json í˜•ì‹ìœ¼ë¡œ êµ¬ì„±ë˜ì–´ìˆìŠµë‹ˆë‹¤. íŒŒì¼ í•˜ë‚˜ë‹¹ í•˜ë‚˜ì˜ ì´ë¯¸ì§€ë¡œ ë˜ì–´ìˆê¸° ë•Œë¬¸ì—, ë¼ë²¨ ë°ì´í„°ë¥¼ ì‚¬ìš©í•  ë•Œë§ˆë‹¤ ê°ê°ì„ ë¶ˆëŸ¬ì˜¤ê¸°ì—ëŠ” ë¹„íš¨ìœ¨ì ì´ë¼ëŠ” ìƒê°ì´ ë“¤ì–´, í•„ìš”í•œ ë°ì´í„°ë“¤ë§Œ ì¶”ì¶œí•˜ì—¬ í•˜ë‚˜ì˜ ë”•ì…”ë„ˆë¦¬ì— ë‹´ëŠ” ì²˜ë¦¬ë¥¼ ì§„í–‰í•˜ê² ìŠµë‹ˆë‹¤.\nëª¨ë¸ í•™ìŠµ ì‹œ í•„ìš”í•œ ë°ì´í„°ë“¤ì„ ì¶”ë ¤ë³´ë©´ ì§ˆë³‘, ì‘ë¬¼, ë°”ìš´ë”©ë°•ìŠ¤, ì´ë¯¸ì§€ í¬ê¸° ì •ë„ ì…ë‹ˆë‹¤. ë”°ë¼ì„œ {image: {disease, crop, points, (width, height)}, â€¦} ì™€ ê°™ì€ í˜•íƒœë¡œ êµ¬ì„±ë˜ë„ë¡ ì²˜ë¦¬í•©ë‹ˆë‹¤.\n\n\n\n\nimage.png\n\n\ní•œê°€ì§€ ë” ê³ ë ¤í•  ì ì€ ì½©ê³¼ í† ë§ˆí† ì…ë‹ˆë‹¤. ë°ì´í„° ê²€ìˆ˜ë¥¼ ì§„í–‰í•˜ë˜ ì¤‘ aihub í™ˆí˜ì´ì§€ì™€ ë¼ë²¨ë§ì´ ë‹¤ë¥´ê²Œ ë˜ì–´ìˆëŠ” ì ì„ ë°œê²¬í•˜ì˜€ìŠµë‹ˆë‹¤. aihubì—ì„œëŠ” ì½©ì´ 7, í† ë§ˆí† ê°€ 8ì¸ë°, ë¼ë²¨ ë°ì´í„°ì—ì„œ í† ë§ˆí† ê°€ 7, ì½©ì´ 8 ë¡œ ë˜ì–´ìˆì—ˆìŠµë‹ˆë‹¤. ì´ ì ë„ í•¨ê»˜ ê³ ë ¤í•˜ì—¬ ë¼ë²¨ë°ì´í„°ë¥¼ ì²˜ë¦¬í•©ë‹ˆë‹¤."
  },
  {
    "objectID": "posts/c01.EDA.html#ë¼ë²¨-ë°ì´í„°-ì²˜ë¦¬",
    "href": "posts/c01.EDA.html#ë¼ë²¨-ë°ì´í„°-ì²˜ë¦¬",
    "title": "[cropdoctor] 1. ë…¸ì§€ì‘ë¬¼ ë°ì´í„° EDA",
    "section": "ë¼ë²¨ ë°ì´í„° ì²˜ë¦¬",
    "text": "ë¼ë²¨ ë°ì´í„° ì²˜ë¦¬\në¼ë²¨ ë°ì´í„°ì—ì„œ ì²˜ë¦¬í•´ì•¼ í•˜ëŠ” í•­ëª©ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.\n\në¼ë²¨ë§ ì²˜ë¦¬\n\nì½©, í† ë§ˆí†  ë¼ë²¨ ì²˜ë¦¬: aihub ë¼ë²¨ê³¼ ë‹¤ë¥¸ ì˜¤ë¥˜ë¥¼ ì²˜ë¦¬í•©ë‹ˆë‹¤.\nìˆ˜ì§‘í•œ ì‘ë¬¼ë§Œ ë‚¨ê²¨ë†“ê³ , ë¼ë²¨ ì¬ì„¤ì •: ê³ ì¶”, ë¬´, ë°°ì¶”, ì• í˜¸ë°•, ì½©, í† ë§ˆí† , í˜¸ë°•ë§Œ ìˆ˜ì§‘ ì„±ê³µí•˜ì˜€ìœ¼ë¯€ë¡œ ì´ì™¸ì˜ ë¼ë²¨ ë°ì´í„°ëŠ” ì—†ì• ì¤ë‹ˆë‹¤.\n\nì‘ë¬¼ ë³„ ì •ìƒ ë¼ë²¨ë§: ëª¨ë“  ì‘ë¬¼ì˜ ì •ìƒ ë°ì´í„°ëŠ” 0ìœ¼ë¡œ ë˜ì–´ìˆì—ˆëŠ”ë°, ì‘ë¬¼ ë³„ë¡œ êµ¬ë¶„í•˜ì—¬ ì¬ì„¤ì •í•©ë‹ˆë‹¤.\n\n\n\n\nì´ë¯¸ì§€ ë³„ ì‘ë¬¼, ì§ˆë³‘, ë°”ìš´ë”© ë°•ìŠ¤, ì´ë¯¸ì§€ ì‚¬ì´ì¦ˆ ë”•ì…”ë„ˆë¦¬ ìƒì„±\n\n{image: {disease, crop, points, (width, height)}, â€¦} ì˜ í˜•íƒœë¡œ, ë¼ë²¨ ë°ì´í„°ì—ì„œ í•„ìš”í•œ ë°ì´í„°ë§Œ ì¶”ì¶œí•˜ì—¬ í•˜ë‚˜ì˜ ë”•ì…”ë„ˆë¦¬ì— ëª¨ì•„ì¤ë‹ˆë‹¤.\n\n\n\n\nì´ë¦„ ë³€í™˜ ë”•ì…”ë„ˆë¦¬\nìš°ì„  ë¼ë²¨ ë°ì´í„°ë¥¼ ì²˜ë¦¬í•˜ê¸° ìœ„í•œ ë”•ì…”ë„ˆë¦¬ë“¤ì„ ì„ ì–¸í•©ë‹ˆë‹¤.\n\n##### ì‘ë¬¼ #####\n# ì›ë³¸ ë¼ë²¨\ncrop2name = {\n             1: \"ê³ ì¶”\", \n             2: \"ë¬´\", \n             3: \"ë°°ì¶”\", \n             4: \"ì• í˜¸ë°•\", \n             5: \"ì–‘ë°°ì¶”\", \n             6: \"ì˜¤ì´\", \n             8: \"ì½©\", \n             7:\"í† ë§ˆí† \", \n             9: \"íŒŒ\", \n             10: \"í˜¸ë°•\"}\n\n# ìˆ˜ì§‘í•œ ë°ì´í„°ë§Œ ë¼ë²¨ ì¬ì„¤ì • \ncrop2name_pre = {\n             1: 1, \n             2: 2, \n             3: 3, \n             4: 4, \n             8: 5, \n             7: 6, \n             10: 7}\n\n# ì‘ë¬¼ ì´ë¦„ê³¼ ë§¤ì¹˜ \ncrop2name_new = {\n             1: \"ê³ ì¶”\", \n             2: \"ë¬´\", \n             3: \"ë°°ì¶”\", \n             4: \"ì• í˜¸ë°•\", \n             5: \"ì½©\", \n             6: \"í† ë§ˆí† \", \n             7: \"í˜¸ë°•\"}\n\n\n##### ì§ˆë³‘ #####\n# ì›ë³¸ ì§ˆë³‘ ë¼ë²¨ \ndisease2name = {\n                0: \"ì •ìƒ\",\n                1: \"ê³ ì¶”íƒ„ì €ë³‘\",\n               2: \"ê³ ì¶”í°ê°€ë£¨ë³‘\",\n               3: \"ë¬´ê²€ì€ë¬´ëŠ¬ë³‘\", \n               4: \"ë¬´ë…¸ê· ë³‘\", \n               5: \"ë°°ì¶”ê²€ìŒì©ìŒë³‘\",\n               6: \"ë°°ì¶”ë…¸ê· ë³‘\",\n               7: \"ì• í˜¸ë°•ë…¸ê· ë³‘\",\n               8: \"ì• í˜¸ë°•í°ê°€ë£¨ë³‘\",\n               9: \"ì–‘ë°°ì¶”ê· í•µë³‘\",\n               10: \"ì–‘ë°°ì¶”ë¬´ë¦„ë³‘\",\n               11: \"ì˜¤ì´ë…¸ê· ë³‘\",\n               12: \"ì˜¤ì´í°ê°€ë£¨ë³‘\", \n               13: \"ì½©ë¶ˆë§ˆë¦„ë³‘\",\n               14: \"ì½©ì ë¬´ëŠ¬ë³‘\",\n               15: \"í† ë§ˆí† ìë§ˆë¦„ë³‘\", \n               16: \"íŒŒê²€ì€ë¬´ëŠ¬ë³‘\",\n               17: \"íŒŒë…¸ê· ë³‘\",\n               18: \"íŒŒë…¹ë³‘\",\n               19: \"í˜¸ë°•ë…¸ê· ë³‘\",\n               20: \"í˜¸ë°•í°ê°€ë£¨ë³‘\"}\n\n# ìˆ˜ì§‘í•œ ë°ì´í„°ë§Œ ë¼ë²¨ ì¬ì„¤ì • (ì •ìƒ ë°ì´í„° ë¼ë²¨ ë¹„ì›Œë†“ê¸°)\ndisease2name_pre = {\n                1: 1,\n               2: 2,\n    \n               3: 4, \n               4: 5, \n               \n               5: 7,\n               6: 8,\n               \n               7: 10,\n               8: 11,\n               \n               13: 13,\n               14: 14,\n               \n               15: 16, \n\n               19: 18,\n               20: 19}\n\n# ì •ìƒ ë°ì´í„°ë¥¼ í¬í•¨í•˜ì—¬ ì§ˆë³‘ ì´ë¦„ê³¼ ë§¤ì¹˜\ndisease2name_new = {\n               0: \"ê³ ì¶”ì •ìƒ\",\n               1: \"ê³ ì¶”íƒ„ì €ë³‘\",\n               2: \"ê³ ì¶”í°ê°€ë£¨ë³‘\",\n               \n               3: \"ë¬´ì •ìƒ\",\n               4: \"ë¬´ê²€ì€ë¬´ëŠ¬ë³‘\", \n               5: \"ë¬´ë…¸ê· ë³‘\", \n    \n               6: \"ë°°ì¶”ì •ìƒ\",\n               7: \"ë°°ì¶”ê²€ìŒì©ìŒë³‘\",\n               8: \"ë°°ì¶”ë…¸ê· ë³‘\",\n    \n               9: \"ì• í˜¸ë°•ì •ìƒ\",\n               10: \"ì• í˜¸ë°•ë…¸ê· ë³‘\",\n               11: \"ì• í˜¸ë°•í°ê°€ë£¨ë³‘\", \n               \n               12: \"ì½©ì •ìƒ\",\n               13: \"ì½©ë¶ˆë§ˆë¦„ë³‘\",\n               14: \"ì½©ì ë¬´ëŠ¬ë³‘\",\n    \n               15: \"í† ë§ˆí† ì •ìƒ\",\n               16: \"í† ë§ˆí† ìë§ˆë¦„ë³‘\", \n    \n               17: \"í˜¸ë°•ì •ìƒ\",\n               18: \"í˜¸ë°•ë…¸ê· ë³‘\",\n               19: \"í˜¸ë°•í°ê°€ë£¨ë³‘\"}\n\n\n# ì‘ë¬¼ ë¼ë²¨ì— ë”°ë¥¸ ì§ˆë³‘ì˜ ì •ìƒ ë¼ë²¨  \ncrop2normal =  {\n             1: 0, \n             2: 3, \n             3: 6, \n             4: 9, \n             5: 12, \n             6: 15, \n             7: 17}\n\n\n\n\nimageë³„ ë”•ì…”ë„ˆë¦¬ ìƒì„±\nìœ„ì—ì„œ ì„ ì–¸í•œ ë¼ë²¨ ë”•ì…”ë„ˆë¦¬ë¥¼ í™œìš©í•˜ì—¬, ì´ë¯¸ì§€ ë³„ë¡œ ì‘ë¬¼, ì§ˆë³‘ ë¼ë²¨ì„ ì²˜ë¦¬í•˜ì—¬ ë‹´ê³ , ë°”ìš´ë”© ë°•ìŠ¤, ì´ë¯¸ì§€ ì‚¬ì´ì¦ˆë„ í•¨ê»˜ ë‹´ì•„ í•˜ë‚˜ì˜ ë”•ì…”ë„ˆë¦¬ì— ëª¨ì•„ì¤ë‹ˆë‹¤.\n\n\ntrain\n\ndic_img2label_train = {}\n\nfor file in tqdm(lst_train_label[:-1]): \n    myzip_r = zipfile.ZipFile(path_train_label+file, 'r') \n    for name in myzip_r.namelist():\n        j = json.loads(myzip_r.read(name))    \n        disease, crop, points = j['annotations']['disease'], crop2name_pre[j['annotations']['crop']], j['annotations']['points'][0]\n        width, height = j['description']['width'], j['description']['height']\n        \n        if disease == 0: \n            disease = crop2normal[crop]\n        else: \n            disease = disease2name_pre[disease]\n        \n        dic_img2label_train[j['description']['image']] = {'disease': disease, 'crop': crop, 'points': points, 'size': (width, height)}\n\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 14/14 [00:04<00:00,  3.28it/s]\n\n\n\n\ntest\n\ndic_img2label_val = {}\n\nfor file in tqdm(lst_valid_label[:-1]): \n    myzip_r = zipfile.ZipFile(path_valid_label+file, 'r') \n    for name in myzip_r.namelist():\n        j = json.loads(myzip_r.read(name))    \n        disease, crop, points = j['annotations']['disease'], crop2name_pre[j['annotations']['crop']], j['annotations']['points'][0]\n        \n        if disease == 0: \n            disease = crop2normal[crop]\n        else: \n            disease = disease2name_pre[disease]\n        dic_img2label_val[j['description']['image']] = {'disease': disease, 'crop': crop, 'points': points, 'size': (width, height)}\n\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 14/14 [00:00<00:00, 18.79it/s]\n\n\nì²˜ë¦¬í•œ ë”•ì…”ë„ˆë¦¬ë¥¼ í”¼í´ë¡œ ì €ì¥í•˜ì—¬ í•„ìš”í•  ë•Œë§ˆë‹¤ ë¡œë“œí•  ìˆ˜ ìˆë„ë¡ í•©ë‹ˆë‹¤.\n\n# ì €ì¥ \n# train\nwith open(\"data_preprocessing/dic_img2label_train.pickle\",\"wb\") as fw:\n    pickle.dump(dic_img2label_train, fw)\n    \n# validation\nwith open(\"data_preprocessing/dic_img2label_val.pickle\",\"wb\") as fw:\n    pickle.dump(dic_img2label_val, fw)\n\n\n# ë¡œë“œ \n# train\nwith open(\"data_preprocessing/dic_img2label_train.pickle\",\"rb\") as fr:\n    dic_img2label_train = pickle.load(fr)\n    \n# validation\nwith open(\"data_preprocessing/dic_img2label_val.pickle\",\"rb\") as fr:\n    dic_img2label_val = pickle.load(fr)"
  },
  {
    "objectID": "posts/c01.EDA.html#ë¼ë²¨-ë°ì´í„°-ì‚´í´ë³´ê¸°",
    "href": "posts/c01.EDA.html#ë¼ë²¨-ë°ì´í„°-ì‚´í´ë³´ê¸°",
    "title": "[cropdoctor] 1. ë…¸ì§€ì‘ë¬¼ ë°ì´í„° EDA",
    "section": "ë¼ë²¨ ë°ì´í„° ì‚´í´ë³´ê¸°",
    "text": "ë¼ë²¨ ë°ì´í„° ì‚´í´ë³´ê¸°\nìœ„ì—ì„œ ìƒì„±í•œ ì´ë¯¸ì§€ ë³„ ë¼ë²¨ ë°ì´í„° ë”•ì…”ë„ˆë¦¬ë¥¼ ì‚¬ìš©í•˜ì—¬ ë°ì´í„°ì˜ ë¼ë²¨ ë¹„ìœ¨ì„ ì‚´í´ë³´ê² ìŠµë‹ˆë‹¤.\n\në…¸ì§€ì‘ë¬¼ ë¹„ìœ¨\n\nlst_crop_cnt = [0] * 7\nfor dic_value in tqdm(dic_img2label_train.values()): \n    lst_crop_cnt[dic_value['crop']-1] += 1     \n\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 78335/78335 [00:00<00:00, 2008011.02it/s]\n\n\n\nfig = plt.figure(figsize=(13, 7))\nax = fig.subplots()\n\nbars = ax.bar(range(len(lst_crop_cnt)), lst_crop_cnt, color='#8ebe8d', edgecolor = 'black')\nax.set_xticks(range(len(lst_crop_cnt)))\nax.set_xticklabels([\"ê³ ì¶”\", \"ë¬´\",  \"ë°°ì¶”\", \"ì• í˜¸ë°•\", \"ì½©\", \"í† ë§ˆí† \", \"í˜¸ë°•\"], fontsize=20)\nax.set_ylim(0, 13000)\n\nplt.title(\"ë…¸ì§€ì‘ë¬¼ ë³„ ê°œìˆ˜\", fontsize=20, pad=10)\n\nfor i, b in enumerate(bars.patches):\n    _=plt.text(b.get_x()+b.get_width()*(1/2),b.get_height()+300, \\\n            round(b.get_height()),ha='center',fontsize=15, color='k')\n\nplt.show()\n\n\n\n\nê° ì‘ë¬¼ì˜ ê°œìˆ˜ëŠ” ê· ë“±í•œ ê²ƒìœ¼ë¡œ ë³´ì…ë‹ˆë‹¤.\n\n\n\nì •ìƒ / ì§ˆë³‘ ë°ì´í„°ì˜ ë¹„ìœ¨\n\ncnt_normal = 0\nfor dic in tqdm(dic_img2label_train.values()):\n    if dic['disease'] in [0, 3, 6, 9, 12, 15, 17]: \n        cnt_normal += 1\n\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 78335/78335 [00:00<00:00, 2056499.80it/s]\n\n\n\nprint(\"ë°ì´í„° ì´ ê°œìˆ˜: \", sum(lst_disease_cnt))\nprint(\"ì •ìƒ ë°ì´í„°ì˜ ê°œìˆ˜: \", cnt_normal)\nprint(\"ì§ˆë³‘ ë°ì´í„°ì˜ ê°œìˆ˜: \", sum(lst_disease_cnt) - cnt_normal)\n\nfig = plt.figure(figsize=(5, 3))\nax = fig.subplots()\nbars = ax.bar(range(2), [cnt_normal, sum(lst_disease_cnt) - cnt_normal], color='#e0a4b2', edgecolor = 'black')\nax.set_xticks(range(2))\nax.set_xticklabels([\"ì •ìƒ\", \"ì§ˆë³‘\"], fontsize=12)\nax.set_ylim(0, 90000)\n\nplt.title(\"ì •ìƒ vs ì§ˆë³‘\", fontsize=13, pad=10)\n\nfor i, b in enumerate(bars.patches):\n    _=plt.text(b.get_x()+b.get_width()*(1/2),b.get_height()+5000, \\\n            round(b.get_height()),ha='center',fontsize=12, color='k')\n\nplt.show()\n\në°ì´í„° ì´ ê°œìˆ˜:  78335\nì •ìƒ ë°ì´í„°ì˜ ê°œìˆ˜:  71421\nì§ˆë³‘ ë°ì´í„°ì˜ ê°œìˆ˜:  6914\n\n\n\n\n\nì •ìƒ ë°ì´í„°ì™€ ì§ˆë³‘ ë°ì´í„°ì˜ ê°œìˆ˜ë¥¼ ì‹œê°í™” í•´ë³´ì•˜ì„ ë•Œ, ë¶ˆê· í˜•ì´ ì‹¬í•œ ê²ƒì„ í™•ì¸í•˜ì˜€ìŠµë‹ˆë‹¤. ë” ìì„¸íˆ ë³´ê¸° ìœ„í•´ ì‘ë¬¼ ë³„ ì •ìƒ ë°ì´í„°ì™€ ì§ˆë³‘ë°ì´í„°ë¥¼ í™•ì¸í•´ë³´ê² ìŠµë‹ˆë‹¤.\n\n\n\në ˆì´ë¸” ë¹„ìœ¨\n\nlst_disease_cnt = [0] * 20\nfor dic_value in tqdm(dic_img2label_train.values()): \n    lst_disease_cnt[dic_value['disease']] += 1    \n\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 78335/78335 [00:00<00:00, 1259471.33it/s]\n\n\n\ndic_num2disease = {\n    1: [\"ê³ ì¶”ì •ìƒ\", \"ê³ ì¶”íƒ„ì €ë³‘\", \"ê³ ì¶”í°ê°€ë£¨ë³‘\"],\n    2: [\"ë¬´ì •ìƒ\", \"ë¬´ê²€ì€ë¬´ëŠ¬ë³‘\", \"ë¬´ë…¸ê· ë³‘\"],\n    3: [\"ë°°ì¶”ì •ìƒ\", \"ë°°ì¶”ê²€ìŒì©ìŒë³‘\", \"ë°°ì¶”ë…¸ê· ë³‘\"],\n    4: [\"ì• í˜¸ë°•ì •ìƒ\", \"ì• í˜¸ë°•ë…¸ê· ë³‘\", \"ì• í˜¸ë°•í°ê°€ë£¨ë³‘\"],\n    5: [\"ì½©ì •ìƒ\", \"ì½©ë¶ˆë§ˆë¦„ë³‘\", \"ì½©ì ë¬´ëŠ¬ë³‘\"],\n    6: [\"í† ë§ˆí† ì •ìƒ\", \"í† ë§ˆí† ìë§ˆë¦„ë³‘\"],\n    7: [\"í˜¸ë°•ì •ìƒ\", \"í˜¸ë°•ë…¸ê· ë³‘\", \"í˜¸ë°•í°ê°€ë£¨ë³‘\"]\n}\n\nlst_c = ['#8b1e0d', '#8b1e0d', '#8b1e0d', 'w', 'w', 'w', '#99b563', '#99b563', '#99b563', '#d4de3a', '#d4de3a', '#d4de3a', '#4f4f4f', '#4f4f4f', '#4f4f4f', 'r', 'r', '#d57b13', '#d57b13', '#d57b13']\n\nfig = plt.figure(figsize=(15, 5))\nax = fig.subplots()\n\nbars = ax.bar(range(len(lst_disease_cnt)), lst_disease_cnt, color=lst_c, edgecolor = 'black')\nax.set_xticks(range(len(lst_disease_cnt)))\nax.set_xticklabels([\"ê³ ì¶”ì •ìƒ\", \"ê³ ì¶”íƒ„ì €ë³‘\", \"ê³ ì¶”í°ê°€ë£¨ë³‘\", \n                    \"ë¬´ì •ìƒ\", \"ë¬´ê²€ì€ë¬´ëŠ¬ë³‘\", \"ë¬´ë…¸ê· ë³‘\", \n                    \"ë°°ì¶”ì •ìƒ\", \"ë°°ì¶”ê²€ìŒì©ìŒë³‘\", \"ë°°ì¶”ë…¸ê· ë³‘\", \n                    \"ì• í˜¸ë°•ì •ìƒ\", \"ì• í˜¸ë°•ë…¸ê· ë³‘\", \"ì• í˜¸ë°•í°ê°€ë£¨ë³‘\", \n                    \"ì½©ì •ìƒ\", \"ì½©ë¶ˆë§ˆë¦„ë³‘\", \"ì½©ì ë¬´ëŠ¬ë³‘\", \n                    \"í† ë§ˆí† ì •ìƒ\", \"í† ë§ˆí† ìë§ˆë¦„ë³‘\", \n                    \"í˜¸ë°•ì •ìƒ\", \"í˜¸ë°•ë…¸ê· ë³‘\", \"í˜¸ë°•í°ê°€ë£¨ë³‘\"], fontsize=15, rotation=45)\nax.set_ylim(0, 12500)\n\nplt.title(\"ë…¸ì§€ì‘ë¬¼ ì§ˆë³‘ ë³„ ê°œìˆ˜\", fontsize=20, pad=10)\n\nfor i, b in enumerate(bars.patches):\n    _=plt.text(b.get_x()+b.get_width()*(1/2),b.get_height()+200, \\\n            round(b.get_height()),ha='center',fontsize=12, color='k')\n\nplt.show()\n\n\n\n\nì‘ë¬¼ ë³„ ì •ìƒ/ì§ˆë³‘ ë°ì´í„°ëŠ” ì´ 20ê°œì˜ ì¹´í…Œê³ ë¦¬ë¡œ, ìµœì¢…ì ì¸ ëª¨ë¸í•™ìŠµì˜ ë ˆì´ë¸”ì´ê¸°ë„ í•©ë‹ˆë‹¤.\nì‹œê°í™”í•˜ì—¬ ì‚´í´ë³´ë‹ˆ ì‘ë¬¼ ë³„ ì •ìƒ/ì§ˆë³‘ ë°ì´í„°ì˜ ë¶ˆê· í˜• ë¬¸ì œê°€ ì¡´ì¬í•˜ëŠ” ê²ƒì„ í™•ì¸í•˜ì˜€ìŠµë‹ˆë‹¤.\në”°ë¼ì„œ ë‹¤ìŒê³¼ ê°™ì€ ë°©ë²•ì„ ì‚¬ìš©í•˜ê³ ì í•©ë‹ˆë‹¤.\n1. ëœë¤ìƒ˜í”Œë§\nê° ì‘ë¬¼ë§ˆë‹¤ ì •ìƒ ë°ì´í„°ì—ì„œ 1000ì¥ì„ ëœë¤ìœ¼ë¡œ ìƒ˜í”Œë§í•©ë‹ˆë‹¤. ì´ëŠ” ë¶ˆê· í˜•ì„ ì¤„ì´ê¸° ìœ„í•´ì„œë„ ë§ì§€ë§Œ, ê·¸ì™€ ë™ì‹œì— ìš©ëŸ‰ì„ ì¤„ì´ê¸° ìœ„í•´ ì§„í–‰í•©ë‹ˆë‹¤. ì œê³µë°›ì€ GPU ì„±ëŠ¥ ë° ë””ìŠ¤í¬ ìš©ëŸ‰ìœ¼ë¡œ ì•½ 8000ì¥ì˜ ëª¨ë¸ í•™ìŠµì„ ëŒë¦¬ê¸°ì—” ì–´ë ¤ì›€ì´ ìˆì—ˆê¸°ì—, ì´ ë°©ë²•ì„ ìš°ì„  ì„ íƒí•˜ì˜€ìŠµë‹ˆë‹¤.\n2. WCE (Weighted cross entropy)\nì •ìƒ ë°ì´í„°ë¥¼ 1000ì¥ ëœë¤ ìƒ˜í”Œë§ í•œ í›„ì—ë„ ëª‡ëª‡ ë ˆì´ë¸”ì€ ë¶ˆê· í˜•ì´ ì—¬ì „íˆ ì‹¬í•  ê²ƒì…ë‹ˆë‹¤. ë”°ë¼ì„œ ëª¨ë¸ í•™ìŠµ ì‹œ WCEë¥¼ ì‚¬ìš©í•˜ì—¬ ê°€ì¤‘ì¹˜ë¥¼ ì£¼ë©´ì„œ ë¶ˆê· í˜•ì„ ê·¹ë³µí•˜ê³ ì í•©ë‹ˆë‹¤."
  },
  {
    "objectID": "posts/c01.EDA.html#ëœë¤-ìƒ˜í”Œë§",
    "href": "posts/c01.EDA.html#ëœë¤-ìƒ˜í”Œë§",
    "title": "[cropdoctor] 1. ë…¸ì§€ì‘ë¬¼ ë°ì´í„° EDA",
    "section": "ëœë¤ ìƒ˜í”Œë§",
    "text": "ëœë¤ ìƒ˜í”Œë§\në°ì´í„°ê°€ ë„ˆë¬´ ë§ê³ , ë¶ˆê· í˜•ì„ ì¤„ì´ê¸° ìœ„í•´ ì •ìƒ ë°ì´í„°ì—ì„œ 1000ì¥ì„ ëœë¤ìœ¼ë¡œ ì¶”ì¶œí•˜ì—¬ ë°ì´í„°ì˜ í¬ê¸°ë¥¼ ì¤„ì˜€ìœ¼ë©°, íŒ€ì›ë“¤ê³¼ ìˆ˜ì‘ì—…ìœ¼ë¡œ ì§„í–‰í•˜ì˜€ìŠµë‹ˆë‹¤.\nëœë¤ ìƒ˜í”Œë§ ëœ ì´ë¯¸ì§€ì˜ ë¼ë²¨ ë°ì´í„°ë§Œ ëª¨ì•„ì„œ ì•ê³¼ ë™ì¼í•˜ê²Œ ì‹œê°í™”ë¡œ í™•ì¸ í•©ë‹ˆë‹¤.\n\në¼ë²¨ ë°ì´í„° ì¬ ì²˜ë¦¬\nì•ì„œ ìƒì„±í•œ ë”•ì…”ë„ˆë¦¬ dic_img2label_trainì—ì„œ ëœë¤ ìƒ˜í”Œë§ëœ ì´ë¯¸ì§€ë§Œ ì¶”ì¶œí•˜ì—¬ dic_img2label_train_sampling ë”•ì…”ë„ˆë¦¬ì— ë„£ì–´ì£¼ê³ , ë˜‘ê°™ì´ pickle íŒŒì¼ë¡œ ì €ì¥í•´ì¤ë‹ˆë‹¤.\n\nlst_crop_folder_train = os.listdir(path_train_img)\nlst_crop_folder_valid = os.listdir(path_valid_img)\n\ndic_img2label_train_sampling = {}\ndic_img2label_valid_sampling = {}\n\nfor folder in tqdm(lst_crop_folder_train): \n    for img in os.listdir(path_train_img + folder):  \n        if img == '.ipynb_checkpoints': \n            continue \n        dic_img2label_train_sampling[img] = dic_img2label_train[img]\n\nfor folder in tqdm(lst_crop_folder_valid): \n    for img in os.listdir(path_valid_img + folder):  \n        if img == '.ipynb_checkpoints': \n            continue \n        dic_img2label_valid_sampling[img] = dic_img2label_val[img]\n\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:00<00:00, 951.73it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:00<00:00, 1587.10it/s]\n\n\n\n# ì €ì¥ \n# train\nwith open(\"data_preprocessing/dic_img2label_train_sampling.pickle\",\"wb\") as fw:\n    pickle.dump(dic_img2label_train_sampling, fw)\n    \n# validation\nwith open(\"data_preprocessing/dic_img2label_valid_sampling.pickle\",\"wb\") as fw:\n    pickle.dump(dic_img2label_valid_sampling, fw)\n\n\n# ë¡œë“œ \n# train\nwith open(\"data_preprocessing/dic_img2label_train_sampling.pickle\",\"rb\") as fr:\n    dic_img2label_train_sampling = pickle.load(fr)\n    \n# validation\nwith open(\"data_preprocessing/dic_img2label_valid_sampling.pickle\",\"rb\") as fr:\n    dic_img2label_valid_sampling = pickle.load(fr)\n\n\n\n\në…¸ì§€ì‘ë¬¼ ë¹„ìœ¨\n\nlst_crop_cnt = [0] * 7\nfor dic_value in tqdm(dic_img2label_train_sampling.values()): \n    lst_crop_cnt[dic_value['crop']-1] += 1     \n\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13915/13915 [00:00<00:00, 1197745.45it/s]\n\n\n\nfig = plt.figure(figsize=(13, 7))\nax = fig.subplots()\n\nbars = ax.bar(range(len(lst_crop_cnt)), lst_crop_cnt, color='#8ebe8d', edgecolor = 'black')\nax.set_xticks(range(len(lst_crop_cnt)))\nax.set_xticklabels([\"ê³ ì¶”\", \"ë¬´\",  \"ë°°ì¶”\", \"ì• í˜¸ë°•\", \"ì½©\", \"í† ë§ˆí† \", \"í˜¸ë°•\"], fontsize=20)\nax.set_ylim(0, 3100)\n\nplt.title(\"ëœë¤ ìƒ˜í”Œë§ í›„: ë…¸ì§€ì‘ë¬¼ ë³„ ê°œìˆ˜\", fontsize=20, pad=10)\n\nfor i, b in enumerate(bars.patches):\n    _=plt.text(b.get_x()+b.get_width()*(1/2),b.get_height()+50, \\\n            round(b.get_height()),ha='center',fontsize=15, color='k')\n\nplt.show()\n\n\n\n\nì •ìƒ ë°ì´í„°ì˜ ê°œìˆ˜ë¥¼ ì¤„ì´ë‹ˆ, ì§ˆë³‘ ë°ì´í„°ì˜ ë¶ˆê· í˜•ì´ ì „ì²´ ë°ì´í„°ì—ì„œë„ ë“œëŸ¬ë‚˜ëŠ” ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\n\n\nì •ìƒ / ë¹„ì •ìƒ ë°ì´í„°ì˜ ë¹„ìœ¨\n\ncnt_normal = 0\nfor dic in tqdm(dic_img2label_train_sampling.values()):\n    if dic['disease'] in [0, 3, 6, 9, 12, 15, 17]: \n        cnt_normal += 1\n\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13915/13915 [00:00<00:00, 2021955.31it/s]\n\n\n\nprint(\"ë°ì´í„° ì´ ê°œìˆ˜: \", sum(lst_disease_cnt))\nprint(\"ì •ìƒ ë°ì´í„°ì˜ ê°œìˆ˜: \", cnt_normal)\nprint(\"ì§ˆë³‘ ë°ì´í„°ì˜ ê°œìˆ˜: \", sum(lst_disease_cnt) - cnt_normal)\n\nfig = plt.figure(figsize=(5, 3))\nax = fig.subplots()\nbars = ax.bar(range(2), [cnt_normal, sum(lst_disease_cnt) - cnt_normal], color='#e0a4b2', edgecolor = 'black')\nax.set_xticks(range(2))\nax.set_xticklabels([\"ì •ìƒ\", \"ì§ˆë³‘\"], fontsize=12)\nax.set_ylim(0, 9000)\n\nplt.title(\"ëœë¤ ìƒ˜í”Œë§ í›„: ì •ìƒ vs ì§ˆë³‘\", fontsize=13, pad=10)\n\nfor i, b in enumerate(bars.patches):\n    _=plt.text(b.get_x()+b.get_width()*(1/2),b.get_height()+500, \\\n            round(b.get_height()),ha='center',fontsize=12, color='k')\n\nplt.show()\n\në°ì´í„° ì´ ê°œìˆ˜:  13915\nì •ìƒ ë°ì´í„°ì˜ ê°œìˆ˜:  7001\nì§ˆë³‘ ë°ì´í„°ì˜ ê°œìˆ˜:  6914\n\n\n\n\n\në°ì´í„°ë¥¼ ëª¨ë‘ í•©ì³¤ì„ ë•ŒëŠ” ì •ìƒ ë°ì´í„°ì™€ ì§ˆë³‘ ë°ì´í„°ì˜ ë¶ˆê· í˜•ì´ í™•ì—°íˆ ì¤„ì–´ë“  ê²ƒì²˜ëŸ¼ ë³´ì´ì§€ë§Œ ìœ„ì—ì„œ í™•ì¸í•œ ê²ƒê³¼ ê°™ì´ ê·¸ë ‡ì§€ ì•ŠìŠµë‹ˆë‹¤. ë°ì´í„°ì˜ ì´ ê°œìˆ˜ë¥¼ ë³´ë©´, ëœë¤ìƒ˜í”Œë§ ì „ 78335ê°œì—ì„œ 13915ê°œë¡œ ëŒ€í­ ì¤„ì–´ë“  ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\n\n\në ˆì´ë¸” ë¹„ìœ¨\n\nlst_disease_cnt = [0] * 20\nfor dic_value in tqdm(dic_img2label_train_sampling.values()): \n    lst_disease_cnt[dic_value['disease']] += 1  \n\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13915/13915 [00:00<00:00, 346903.51it/s]\n\n\n\ndic_num2disease = {\n    1: [\"ê³ ì¶”ì •ìƒ\", \"ê³ ì¶”íƒ„ì €ë³‘\", \"ê³ ì¶”í°ê°€ë£¨ë³‘\"],\n    2: [\"ë¬´ì •ìƒ\", \"ë¬´ê²€ì€ë¬´ëŠ¬ë³‘\", \"ë¬´ë…¸ê· ë³‘\"],\n    3: [\"ë°°ì¶”ì •ìƒ\", \"ë°°ì¶”ê²€ìŒì©ìŒë³‘\", \"ë°°ì¶”ë…¸ê· ë³‘\"],\n    4: [\"ì• í˜¸ë°•ì •ìƒ\", \"ì• í˜¸ë°•ë…¸ê· ë³‘\", \"ì• í˜¸ë°•í°ê°€ë£¨ë³‘\"],\n    5: [\"ì½©ì •ìƒ\", \"ì½©ë¶ˆë§ˆë¦„ë³‘\", \"ì½©ì ë¬´ëŠ¬ë³‘\"],\n    6: [\"í† ë§ˆí† ì •ìƒ\", \"í† ë§ˆí† ìë§ˆë¦„ë³‘\"],\n    7: [\"í˜¸ë°•ì •ìƒ\", \"í˜¸ë°•ë…¸ê· ë³‘\", \"í˜¸ë°•í°ê°€ë£¨ë³‘\"]\n}\n\nlst_c = ['#8b1e0d', '#8b1e0d', '#8b1e0d', 'w', 'w', 'w', '#99b563', '#99b563', '#99b563', '#d4de3a', '#d4de3a', '#d4de3a', '#4f4f4f', '#4f4f4f', '#4f4f4f', 'r', 'r', '#d57b13', '#d57b13', '#d57b13']\n\nfig = plt.figure(figsize=(15, 5))\nax = fig.subplots()\n\nbars = ax.bar(range(len(lst_disease_cnt)), lst_disease_cnt, color=lst_c, edgecolor = 'black')\nax.set_xticks(range(len(lst_disease_cnt)))\nax.set_xticklabels([\"ê³ ì¶”ì •ìƒ\", \"ê³ ì¶”íƒ„ì €ë³‘\", \"ê³ ì¶”í°ê°€ë£¨ë³‘\", \n                    \"ë¬´ì •ìƒ\", \"ë¬´ê²€ì€ë¬´ëŠ¬ë³‘\", \"ë¬´ë…¸ê· ë³‘\", \n                    \"ë°°ì¶”ì •ìƒ\", \"ë°°ì¶”ê²€ìŒì©ìŒë³‘\", \"ë°°ì¶”ë…¸ê· ë³‘\", \n                    \"ì• í˜¸ë°•ì •ìƒ\", \"ì• í˜¸ë°•ë…¸ê· ë³‘\", \"ì• í˜¸ë°•í°ê°€ë£¨ë³‘\", \n                    \"ì½©ì •ìƒ\", \"ì½©ë¶ˆë§ˆë¦„ë³‘\", \"ì½©ì ë¬´ëŠ¬ë³‘\", \n                    \"í† ë§ˆí† ì •ìƒ\", \"í† ë§ˆí† ìë§ˆë¦„ë³‘\", \n                    \"í˜¸ë°•ì •ìƒ\", \"í˜¸ë°•ë…¸ê· ë³‘\", \"í˜¸ë°•í°ê°€ë£¨ë³‘\"], fontsize=15, rotation=45)\nax.set_ylim(0, 1100)\n\nplt.title(\"ëœë¤ ìƒ˜í”Œë§ í›„: ë…¸ì§€ì‘ë¬¼ ë¼ë²¨ ë³„ ê°œìˆ˜\", fontsize=20, pad=10)\n\nfor i, b in enumerate(bars.patches):\n    _=plt.text(b.get_x()+b.get_width()*(1/2),b.get_height()+20, \\\n            round(b.get_height()),ha='center',fontsize=12, color='k')\n\nplt.show()\n\n\n\n\nì •ìƒ ì´ë¯¸ì§€ê°€ ëª¨ë‘ 1000ì¥ìœ¼ë¡œ ì¤„ì–´ë“  ê²ƒ í™•ì¸í•˜ì˜€ìœ¼ë©°, ì—­ì‹œ ì•„ì§ ë¶ˆê· í˜•ì´ ì¡´ì¬í•©ë‹ˆë‹¤. ì´ëŠ” ì•ì„œ ë§í–ˆë“¯, ëª¨ë¸ í•™ìŠµ ì‹œ WCE ê¸°ë²•ì„ ì‚¬ìš©í•˜ë„ë¡ í•©ë‹ˆë‹¤."
  },
  {
    "objectID": "posts/c01.EDA.html#ì´ë¯¸ì§€-í™•ì¸",
    "href": "posts/c01.EDA.html#ì´ë¯¸ì§€-í™•ì¸",
    "title": "[cropdoctor] 1. ë…¸ì§€ì‘ë¬¼ ë°ì´í„° EDA",
    "section": "ì´ë¯¸ì§€ í™•ì¸",
    "text": "ì´ë¯¸ì§€ í™•ì¸\nì´ë¯¸ì§€ì˜ ë¼ë²¨ë°ì´í„°ë¥¼ í™œìš©í•˜ì—¬ ë ˆì´ë¸” ë§¤ì¹˜ ë° ë°”ìš´ë”© ë°•ìŠ¤ë¥¼ í•¨ê»˜ ê·¸ë ¤ ë°ì´í„°ë¥¼ í™•ì¸í•˜ëŠ” í•¨ìˆ˜ë¥¼ ìƒì„±í•˜ì˜€ìŠµë‹ˆë‹¤. ì¸ìì— ì›í•˜ëŠ” ë ˆì´ë¸”ì˜ ìˆ«ìë¥¼ ë„£ìœ¼ë©´ ê·¸ì— ëŒ€í•œ ì´ë¯¸ì§€ë¥¼ ëœë¤ìœ¼ë¡œ 9ì¥ ë½‘ì•„ì˜µë‹ˆë‹¤.\n\nlst_train_img_folder = sorted([folder for folder in sorted(lst_train_img) if 'ipynb' not in folder])\n\n# ë¼ë²¨ ë³„ ëœë¤ ì´ë¯¸ì§€ 9ì¥ ë½‘ì•„ì˜¤ëŠ” í•¨ìˆ˜ \ndef show_images_by_label(folder_num):\n    folder = lst_train_img_folder[folder_num]\n    lst_forder_img = os.listdir(path_train_img + folder)\n    lst_nums = random.sample(range(len(lst_forder_img)), 9)\n\n    fig = plt.figure(figsize=(13, 13))\n    axes = fig.subplots(3, 3).flatten()\n    fig.suptitle(f\"{disease2name_new[folder_num]}\", fontsize=20)\n\n    for i, num in enumerate(lst_nums): \n        img_name = lst_forder_img[num]\n        img = Image.open(path_train_img + folder + '/' + img_name)\n\n        d1 = dic_img2label_train[img_name]\n\n        crop, disease, points = d1['crop'], d1['disease'], d1['points']\n\n        draw = ImageDraw.Draw(img)\n        draw.rectangle([(points['xtl'], points['ytl']), (points['xbr'], points['ybr'])], outline=(255, 0, 255), width=30)\n\n        axes[i].imshow(np.array(img))\n\n\nshow_images_by_label(1)\n\n\n\n\n\nshow_images_by_label(3)\n\n\n\n\n\nshow_images_by_label(5)\n\n\n\n\n\nshow_images_by_label(7)\n\n\n\n\n\nshow_images_by_label(9)\n\n\n\n\n\nshow_images_by_label(11)\n\n\n\n\n\nshow_images_by_label(19)\n\n\n\n\nì´ë¯¸ì§€ ë°ì´í„°ë¥¼ ì‚´í´ë³´ì•˜ì„ ë•Œ, ì§ˆë³‘ ì‘ë¬¼ì— ëŒ€í•˜ì—¬ ë°”ìš´ë”© ë°•ìŠ¤ ì²˜ë¦¬ê°€ ì˜ ë˜ì–´ìˆìŒì„ í™•ì¸í–ˆìŠµë‹ˆë‹¤. image classification ë¿ë§Œ ì•„ë‹ˆë¼, object detection ìœ¼ë¡œ ëª¨ë¸í•™ìŠµì„ ì§„í–‰í•˜ëŠ” ê²ƒë„ ê³ ë ¤í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë‹¤ë§Œ, ì‘ë¬¼ì„ ì´ë£¨ëŠ” ìš”ì†Œê°€ ì, ì¤„ê¸°, ê½ƒ ë“±ì´ ìˆë‹¤ë³´ë‹ˆ ê°™ì€ ë ˆì´ë¸” ì•ˆì—ì„œë„ ë‹¤ë¥¸ íŠ¹ì§•ì˜ ì´ë¯¸ì§€ë¡œ ì¸ì‹í•  ìˆ˜ ìˆë‹¤ëŠ” ì ì´ ìš°ë ¤ë˜ì—ˆìŠµë‹ˆë‹¤.\nì§€ê¸ˆê¹Œì§€ ë…¸ì§€ì‘ë¬¼ ë°ì´í„°ì— ëŒ€í•œ EDAë¥¼ ì§„í–‰í•˜ì˜€ìŠµë‹ˆë‹¤. EDAë¥¼ í†µí•´ ë¼ë²¨ ë°ì´í„°ë¥¼ ì²˜ë¦¬í•˜ì˜€ê³ , ë°ì´í„°ì˜ ë¶„í¬ê°€ ì–´ë–»ê²Œ ë˜ì–´ìˆëŠ”ì§€ í™•ì¸í•  ìˆ˜ ìˆì—ˆìŠµë‹ˆë‹¤. ë˜í•œ ì–´ë–¤ ë°©í–¥ìœ¼ë¡œ ì¸ê³µì§€ëŠ¥ ëª¨ë¸ í•™ìŠµì„ í•´ì•¼í•˜ëŠ”ì§€ ê³ ë¯¼í•´ë³¼ ìˆ˜ ìˆì—ˆìŠµë‹ˆë‹¤."
  },
  {
    "objectID": "posts/c02.aihub_model_evaluate.html#tar-í™•ì¥ì-ëª¨ë¸-docker-image-ë¶ˆëŸ¬ì˜¤ê¸°",
    "href": "posts/c02.aihub_model_evaluate.html#tar-í™•ì¥ì-ëª¨ë¸-docker-image-ë¶ˆëŸ¬ì˜¤ê¸°",
    "title": "[cropdoctor] 2. tar í™•ì¥ì ëª¨ë¸ docker image ë¶ˆëŸ¬ì˜¤ê¸°",
    "section": "2. tar í™•ì¥ì ëª¨ë¸ docker image ë¶ˆëŸ¬ì˜¤ê¸°",
    "text": "2. tar í™•ì¥ì ëª¨ë¸ docker image ë¶ˆëŸ¬ì˜¤ê¸°\n\nimport pickle \nimport os \nfrom tqdm import tqdm\nimport random \n\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torchvision.models as models\nfrom torchsummary import summary\n \nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n    \nfrom PIL import Image, ImageDraw\nimport albumentations\n\nimport matplotlib.pyplot as plt \nplt.rc(\"font\", family=\"NanumGothic\", size=13) \n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\n01. docker ë¡œ tar íŒŒì¼ ë¶ˆëŸ¬ì˜¤ê¸°\naihubì—ì„œ ì˜ˆì‹œ ëª¨ë¸ì„ í•˜ë‚˜ ì œê³µí•´ì£¼ì–´ì„œ ê·¸ê²ƒì„ ë² ì´ìŠ¤ë¼ì¸ ëª¨ë¸ë¡œ ì„ ì •í•˜ê³ ì í–ˆìŠµë‹ˆë‹¤. í•´ë‹¹ ëª¨ë¸ì„ ì‚¬ìš©í•˜ê¸° ìœ„í•´, tar í™•ì¥ìë¡œ ì €ì¥ë˜ì–´ ìˆëŠ” aihubì˜ ì˜ˆì‹œ ëª¨ë¸ì„ dockerë¡œ ë¶ˆëŸ¬ì™€ì•¼ í–ˆìŠµë‹ˆë‹¤.\n\n1) ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸° ì˜¤ë¥˜\ní”„ë¡œì íŠ¸ì—ì„œ ì œê³µë°›ì€ VMì˜ ë¦¬ëˆ…ìŠ¤ í„°ë¯¸ë„ì—ì„œ ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ëŠ” ê²ƒì„ ì‹œë„í•˜ì˜€ìŠµë‹ˆë‹¤. >docker load -i 73.tar\nCannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?\ní•˜ì§€ë§Œ ìœ„ì™€ ê°™ì€ ì˜¤ë¥˜ê°€ ë‚¬ìŠµë‹ˆë‹¤.\në”°ë¼ì„œ ë°ëª¬ì´ ì‹¤í–‰ì¤‘ì¸ì§€ í™•ì¸í•˜ê¸° ìœ„í•˜ì—¬ ë‹¤ìŒê³¼ ê°™ì€ ëª…ë ¹ì–´ë¥¼ ì‹¤í–‰í•˜ì˜€ìŠµë‹ˆë‹¤.\n\nsudo systemctl status docker\n\nSystem has not been booted with systemd as init system (PID 1). Canâ€™t operate. Failed to connect to bus: Host is down ë˜ ë‹¤ë¥¸ ì˜¤ë¥˜ë¥¼ ë³´ì•˜ìŠµë‹ˆë‹¤.\nêµ¬ê¸€ë§ê³¼ chat gptì— ì—´ì‹¬íˆ ì§ˆë¬¸í•˜ë©´ì„œ í•´ê²°ì„ ì‹œë„í•˜ì˜€ì§€ë§Œ ì„±ê³µí•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.  ê·¸ ë‹¹ì‹œ ë…¸ì…˜ì— ë©”ëª¨í•´ë’€ë˜ ì˜¤ë¥˜ì˜ ì›ì¸ì…ë‹ˆë‹¤.\në§ˆì¹¨ë‚´ ìš´ì˜ì§„ìª½ì—ì„œ dockerë¥¼ ì§€ì›í•˜ì§€ ì•ŠëŠ” VMì´ë¼ëŠ” ë§ì„ ì „í•´ë“£ê³ , ë‹¤ë¥¸ ë°©ë²•ì„ íƒí–ˆìŠµë‹ˆë‹¤. í—ˆíƒˆí–ˆì§€ë§Œ í•´ê²° ì•ˆë˜ë˜ê²Œ ì •ìƒì´ì–´ì„œ í•œí¸ìœ¼ë¡œëŠ” ë‹¤í–‰ì´ì—ˆìŠµë‹ˆë‹¤.\n\n2) ë¡œì»¬ì—ì„œ ë¶ˆëŸ¬ì™€ì„œ VMì— ì˜¬ë¦¬ê¸°\nVMì—ì„œ ëª¨ë¸ì„ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìœ¼ë‹ˆ, ë¡œì»¬ git bashì—ì„œ ëª…ë ¹ì–´ë¥¼ ì‹¤í–‰í•˜ëŠ” ë°©ë²•ì„ ì„ íƒí–ˆìŠµë‹ˆë‹¤.\n\n\në„ì»¤ ì´ë¯¸ì§€ ë¶ˆëŸ¬ì˜¤ê¸°\n\n\ndocker load -i 73.tar\n\n\n\n\nimage.png\n\n\nëª¨ë¸ ë¶ˆëŸ¬ì˜¬ ë‹¹ì‹œ ì»´í“¨í„°ê°€ ë©ˆì¶”ê³  í™”ë©´ì´ ê¹Œë§£ê²Œ ë³€í–ˆë˜ ê¸°ì–µì´ ë‚©ë‹ˆë‹¤. ë‹¤í–‰íˆ ê·¸ ì™€ì¤‘ì—ë„ git bashëŠ” êº¼ì§€ì§€ ì•Šê³  ê³„ì† ëª¨ë¸ì´ ë¡œë“œë˜ê³  ìˆì—ˆìŠµë‹ˆë‹¤.\n\n\në„ì»¤ ì´ë¯¸ì§€ í™•ì¸í•˜ê¸°\n\n\ndocker images\n\n\n\n\nimage.png\n\n\në„ì»¤ ì´ë¯¸ì§€ê°€ ì˜ ë¶ˆëŸ¬ì™€ì§„ ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\n\në„ì»¤ ì»¨í…Œì´ë„ˆ ì‹¤í–‰ì‹œí‚¤ê¸°\n\n\ndocker run -it laonpeople/79_classification:v0.2 ls\n\n\n\n\nimage.png\n\n\në„ì»¤ ì»¨í…Œì´ë„ˆë¥¼ ì‹¤í–‰ì‹œí‚¤ê³ , ê·¸ ì•ˆì—ì„œ íŒŒì¼ ëª©ë¡ì„ í™•ì¸í•´ ë³´ì•˜ìŠµë‹ˆë‹¤. ë“œë””ì–´ aihubì—ì„œ ì œê³µí•´ì¤€ ëª¨ë¸ íŒŒì¼ë“¤ì„ í™•ì¸í•˜ëŠ” ë° ì„±ê³µí–ˆìŠµë‹ˆë‹¤.\n\n\në„ì»¤ ì»¨í…Œì´ë„ˆ -> ë¡œì»¬ ë³µì‚¬í•˜ê¸°\n\në¡œì»¬ì—ì„œ ëª¨ë¸ì„ ë¡œë“œí–ˆê¸° ë•Œë¬¸ì— í”„ë¡œì íŠ¸ ì‘ì—…ì„ ì§„í–‰í•˜ëŠ” VMìœ¼ë¡œ íŒŒì¼ë“¤ì„ ì˜®ê²¨ì•¼ í–ˆìŠµë‹ˆë‹¤. ë”°ë¼ì„œ ë„ì»¤ ì»¨í…Œì´ë„ˆ -> ë¡œì»¬ -> í”„ë¡œì íŠ¸ VM ì˜ ìˆœì„œë¡œ íŒŒì¼ì„ ì˜®ê¸°ê¸°ë¡œ í–ˆìŠµë‹ˆë‹¤.\n\ndocker run -d â€“name con laonpeople/79_classification:v0.2\n\nìš°ì„  ë„ì»¤ ì»¨í…Œì´ë„ˆë¥¼ -d ì˜µì…˜ìœ¼ë¡œ ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì‹¤í–‰ì‹œí‚µë‹ˆë‹¤. ì´ ë•Œ, â€“name ì˜µì…˜ìœ¼ë¡œ ì»¨í…Œì´ë„ˆ ì´ë¦„ì„ conìœ¼ë¡œ ì§€ì •í•´ì£¼ì—ˆìŠµë‹ˆë‹¤.\n\ndocker cp con:/ C:/Users/sooki/Downloads/docker\n\nê·¸ ë‹¤ìŒ, docker cp [ì»¨í…Œì´ë„ˆì´ë¦„]:[íŒŒì¼ìœ„ì¹˜] [ë¡œì»¬íŒŒì¼ìœ„ì¹˜] ëª…ë ¹ì–´ë¥¼ ì‚¬ìš©í•˜ì—¬ ë¡œì»¬ì— ë¬´ì‚¬íˆ íŒŒì¼ë“¤ì„ ì˜®ê²¼ìŠµë‹ˆë‹¤.\nì´ì œ ë¡œì»¬ì—ì„œ VM jupyter-lab í™˜ê²½ì— ë“œë˜ê·¸ì•¤ë“œëìœ¼ë¡œ íŒŒì¼ì„ ìµœì¢…ì ìœ¼ë¡œ ì˜®ê²¨ì£¼ì—ˆìŠµë‹ˆë‹¤.\n\n\nVMì—ì„œ í•„ìš”í•œ íŒ¨í‚¤ì§€ ì„¤ì¹˜í•˜ê¸°\n\n\npip install -r requirements.txt\n\nrequirements.txt íŒŒì¼ì— í•„ìš”í•œ íŒ¨í‚¤ì§€ê°€ ë“¤ì–´ìˆì—ˆìŠµë‹ˆë‹¤. ì´ë¥¼ vmì—ì„œ ëª¨ë‘ ì„¤ì¹˜í•´ì£¼ë©´, aihub ëª¨ë¸ ë¶ˆëŸ¬ì˜¬ ì¤€ë¹„ë¥¼ ëª¨ë‘ ë§ˆì¹˜ê²Œ ë©ë‹ˆë‹¤.\n\n\nì´ë¦„ ë³€í™˜ ë”•ì…”ë„ˆë¦¬\n\ndisease2name = {\n                0: \"ì •ìƒ\",\n                1: \"ê³ ì¶”íƒ„ì €ë³‘\",\n               2: \"ê³ ì¶”í°ê°€ë£¨ë³‘\",\n               3: \"ë¬´ê²€ì€ë¬´ëŠ¬ë³‘\", \n               4: \"ë¬´ë…¸ê· ë³‘\", \n               5: \"ë°°ì¶”ê²€ìŒì©ìŒë³‘\",\n               6: \"ë°°ì¶”ë…¸ê· ë³‘\",\n               7: \"ì• í˜¸ë°•ë…¸ê· ë³‘\",\n               8: \"ì• í˜¸ë°•í°ê°€ë£¨ë³‘\",\n               9: \"ì–‘ë°°ì¶”ê· í•µë³‘\",\n               10: \"ì–‘ë°°ì¶”ë¬´ë¦„ë³‘\",\n               11: \"ì˜¤ì´ë…¸ê· ë³‘\",\n               12: \"ì˜¤ì´í°ê°€ë£¨ë³‘\", \n               13: \"ì½©ë¶ˆë§ˆë¦„ë³‘\",\n               14: \"ì½©ì ë¬´ëŠ¬ë³‘\",\n               15: \"í† ë§ˆí† ìë§ˆë¦„ë³‘\", \n               16: \"íŒŒê²€ì€ë¬´ëŠ¬ë³‘\",\n               17: \"íŒŒë…¸ê· ë³‘\",\n               18: \"íŒŒë…¹ë³‘\",\n               19: \"í˜¸ë°•ë…¸ê· ë³‘\",\n               20: \"í˜¸ë°•í°ê°€ë£¨ë³‘\"}\n\n\n\n\n\n02. ëª¨ë¸ ë¶ˆëŸ¬ì™€ì„œ ì„±ëŠ¥ í‰ê°€í•´ë³´ê¸°\n\nconfig íŒŒì¼ì„ ë³´ë‹ˆ 04ì‘ë¬¼ì— ëŒ€í•´ì„œ 3ê°œì˜ í´ë˜ìŠ¤ë¡œ ë¶„ë¥˜í•˜ëŠ” ëª¨ë¸ì¸ ê²ƒìœ¼ë¡œ ë³´ì…ë‹ˆë‹¤. ì¦‰ ì• í˜¸ë°• ì— ëŒ€í•´ì„œë§Œ ì •ìƒ/ ì§ˆë³‘1 / ì§ˆë³‘2 ì´ë ‡ê²Œ ë¶„ë¥˜í•˜ëŠ” ëª¨ë¸ì…ë‹ˆë‹¤. ë ˆì´ë¸”ì´ ë‹¤ë¥´ë¯€ë¡œ í•™ìŠµì— ì‚¬ìš©í•˜ì§€ëŠ” ì•Šê³ , ì„±ëŠ¥ì´ ì–¼ë§ˆì •ë„ ë‚˜ì˜¤ëŠ”ì§€ë§Œ í™•ì¸í•´ë³´ê² ìŠµë‹ˆë‹¤.\n\n\n\nëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°\n\nmodel_path = \"docker/weights/sample.pt\"\n  \ndevice = torch.device(\"cuda\")\n\nmodel_weights = torch.load(model_path, map_location=device)[\"model_state_dict\"]\n\n\n\n\ní•™ìŠµëœ ëª¨ë¸ êµ¬ì¡°\n\nimport torch.nn as nn\nimport torchvision.models as models\n\ndict_backbone = {'resnet50' : models.resnet50}\n\ndef get_model(model_name='resnet50', num_classes=3, pretrained=False): # use pretrained backbone\n    assert model_name in dict_backbone.keys()\n    \n    network = dict_backbone[model_name](pretrained=pretrained)\n    network.fc = nn.Linear(network.fc.in_features, num_classes)\n    \n    return network\n\n\nmodel = get_model()\nmodel.load_state_dict(model_weights)\nmodel = model.to(device)\n\n\n\n\nëª¨ë¸ ìš”ì•½\n\nsummary(model, input_size=(3, 512, 512))\n\n----------------------------------------------------------------\n        Layer (type)               Output Shape         Param #\n================================================================\n            Conv2d-1         [-1, 64, 112, 112]           9,408\n       BatchNorm2d-2         [-1, 64, 112, 112]             128\n              ReLU-3         [-1, 64, 112, 112]               0\n         MaxPool2d-4           [-1, 64, 56, 56]               0\n            Conv2d-5           [-1, 64, 56, 56]           4,096\n       BatchNorm2d-6           [-1, 64, 56, 56]             128\n              ReLU-7           [-1, 64, 56, 56]               0\n            Conv2d-8           [-1, 64, 56, 56]          36,864\n       BatchNorm2d-9           [-1, 64, 56, 56]             128\n             ReLU-10           [-1, 64, 56, 56]               0\n           Conv2d-11          [-1, 256, 56, 56]          16,384\n      BatchNorm2d-12          [-1, 256, 56, 56]             512\n           Conv2d-13          [-1, 256, 56, 56]          16,384\n      BatchNorm2d-14          [-1, 256, 56, 56]             512\n             ReLU-15          [-1, 256, 56, 56]               0\n       Bottleneck-16          [-1, 256, 56, 56]               0\n           Conv2d-17           [-1, 64, 56, 56]          16,384\n      BatchNorm2d-18           [-1, 64, 56, 56]             128\n             ReLU-19           [-1, 64, 56, 56]               0\n           Conv2d-20           [-1, 64, 56, 56]          36,864\n      BatchNorm2d-21           [-1, 64, 56, 56]             128\n             ReLU-22           [-1, 64, 56, 56]               0\n           Conv2d-23          [-1, 256, 56, 56]          16,384\n      BatchNorm2d-24          [-1, 256, 56, 56]             512\n             ReLU-25          [-1, 256, 56, 56]               0\n       Bottleneck-26          [-1, 256, 56, 56]               0\n           Conv2d-27           [-1, 64, 56, 56]          16,384\n      BatchNorm2d-28           [-1, 64, 56, 56]             128\n             ReLU-29           [-1, 64, 56, 56]               0\n           Conv2d-30           [-1, 64, 56, 56]          36,864\n      BatchNorm2d-31           [-1, 64, 56, 56]             128\n             ReLU-32           [-1, 64, 56, 56]               0\n           Conv2d-33          [-1, 256, 56, 56]          16,384\n      BatchNorm2d-34          [-1, 256, 56, 56]             512\n             ReLU-35          [-1, 256, 56, 56]               0\n       Bottleneck-36          [-1, 256, 56, 56]               0\n           Conv2d-37          [-1, 128, 56, 56]          32,768\n      BatchNorm2d-38          [-1, 128, 56, 56]             256\n             ReLU-39          [-1, 128, 56, 56]               0\n           Conv2d-40          [-1, 128, 28, 28]         147,456\n      BatchNorm2d-41          [-1, 128, 28, 28]             256\n             ReLU-42          [-1, 128, 28, 28]               0\n           Conv2d-43          [-1, 512, 28, 28]          65,536\n      BatchNorm2d-44          [-1, 512, 28, 28]           1,024\n           Conv2d-45          [-1, 512, 28, 28]         131,072\n      BatchNorm2d-46          [-1, 512, 28, 28]           1,024\n             ReLU-47          [-1, 512, 28, 28]               0\n       Bottleneck-48          [-1, 512, 28, 28]               0\n           Conv2d-49          [-1, 128, 28, 28]          65,536\n      BatchNorm2d-50          [-1, 128, 28, 28]             256\n             ReLU-51          [-1, 128, 28, 28]               0\n           Conv2d-52          [-1, 128, 28, 28]         147,456\n      BatchNorm2d-53          [-1, 128, 28, 28]             256\n             ReLU-54          [-1, 128, 28, 28]               0\n           Conv2d-55          [-1, 512, 28, 28]          65,536\n      BatchNorm2d-56          [-1, 512, 28, 28]           1,024\n             ReLU-57          [-1, 512, 28, 28]               0\n       Bottleneck-58          [-1, 512, 28, 28]               0\n           Conv2d-59          [-1, 128, 28, 28]          65,536\n      BatchNorm2d-60          [-1, 128, 28, 28]             256\n             ReLU-61          [-1, 128, 28, 28]               0\n           Conv2d-62          [-1, 128, 28, 28]         147,456\n      BatchNorm2d-63          [-1, 128, 28, 28]             256\n             ReLU-64          [-1, 128, 28, 28]               0\n           Conv2d-65          [-1, 512, 28, 28]          65,536\n      BatchNorm2d-66          [-1, 512, 28, 28]           1,024\n             ReLU-67          [-1, 512, 28, 28]               0\n       Bottleneck-68          [-1, 512, 28, 28]               0\n           Conv2d-69          [-1, 128, 28, 28]          65,536\n      BatchNorm2d-70          [-1, 128, 28, 28]             256\n             ReLU-71          [-1, 128, 28, 28]               0\n           Conv2d-72          [-1, 128, 28, 28]         147,456\n      BatchNorm2d-73          [-1, 128, 28, 28]             256\n             ReLU-74          [-1, 128, 28, 28]               0\n           Conv2d-75          [-1, 512, 28, 28]          65,536\n      BatchNorm2d-76          [-1, 512, 28, 28]           1,024\n             ReLU-77          [-1, 512, 28, 28]               0\n       Bottleneck-78          [-1, 512, 28, 28]               0\n           Conv2d-79          [-1, 256, 28, 28]         131,072\n      BatchNorm2d-80          [-1, 256, 28, 28]             512\n             ReLU-81          [-1, 256, 28, 28]               0\n           Conv2d-82          [-1, 256, 14, 14]         589,824\n      BatchNorm2d-83          [-1, 256, 14, 14]             512\n             ReLU-84          [-1, 256, 14, 14]               0\n           Conv2d-85         [-1, 1024, 14, 14]         262,144\n      BatchNorm2d-86         [-1, 1024, 14, 14]           2,048\n           Conv2d-87         [-1, 1024, 14, 14]         524,288\n      BatchNorm2d-88         [-1, 1024, 14, 14]           2,048\n             ReLU-89         [-1, 1024, 14, 14]               0\n       Bottleneck-90         [-1, 1024, 14, 14]               0\n           Conv2d-91          [-1, 256, 14, 14]         262,144\n      BatchNorm2d-92          [-1, 256, 14, 14]             512\n             ReLU-93          [-1, 256, 14, 14]               0\n           Conv2d-94          [-1, 256, 14, 14]         589,824\n      BatchNorm2d-95          [-1, 256, 14, 14]             512\n             ReLU-96          [-1, 256, 14, 14]               0\n           Conv2d-97         [-1, 1024, 14, 14]         262,144\n      BatchNorm2d-98         [-1, 1024, 14, 14]           2,048\n             ReLU-99         [-1, 1024, 14, 14]               0\n      Bottleneck-100         [-1, 1024, 14, 14]               0\n          Conv2d-101          [-1, 256, 14, 14]         262,144\n     BatchNorm2d-102          [-1, 256, 14, 14]             512\n            ReLU-103          [-1, 256, 14, 14]               0\n          Conv2d-104          [-1, 256, 14, 14]         589,824\n     BatchNorm2d-105          [-1, 256, 14, 14]             512\n            ReLU-106          [-1, 256, 14, 14]               0\n          Conv2d-107         [-1, 1024, 14, 14]         262,144\n     BatchNorm2d-108         [-1, 1024, 14, 14]           2,048\n            ReLU-109         [-1, 1024, 14, 14]               0\n      Bottleneck-110         [-1, 1024, 14, 14]               0\n          Conv2d-111          [-1, 256, 14, 14]         262,144\n     BatchNorm2d-112          [-1, 256, 14, 14]             512\n            ReLU-113          [-1, 256, 14, 14]               0\n          Conv2d-114          [-1, 256, 14, 14]         589,824\n     BatchNorm2d-115          [-1, 256, 14, 14]             512\n            ReLU-116          [-1, 256, 14, 14]               0\n          Conv2d-117         [-1, 1024, 14, 14]         262,144\n     BatchNorm2d-118         [-1, 1024, 14, 14]           2,048\n            ReLU-119         [-1, 1024, 14, 14]               0\n      Bottleneck-120         [-1, 1024, 14, 14]               0\n          Conv2d-121          [-1, 256, 14, 14]         262,144\n     BatchNorm2d-122          [-1, 256, 14, 14]             512\n            ReLU-123          [-1, 256, 14, 14]               0\n          Conv2d-124          [-1, 256, 14, 14]         589,824\n     BatchNorm2d-125          [-1, 256, 14, 14]             512\n            ReLU-126          [-1, 256, 14, 14]               0\n          Conv2d-127         [-1, 1024, 14, 14]         262,144\n     BatchNorm2d-128         [-1, 1024, 14, 14]           2,048\n            ReLU-129         [-1, 1024, 14, 14]               0\n      Bottleneck-130         [-1, 1024, 14, 14]               0\n          Conv2d-131          [-1, 256, 14, 14]         262,144\n     BatchNorm2d-132          [-1, 256, 14, 14]             512\n            ReLU-133          [-1, 256, 14, 14]               0\n          Conv2d-134          [-1, 256, 14, 14]         589,824\n     BatchNorm2d-135          [-1, 256, 14, 14]             512\n            ReLU-136          [-1, 256, 14, 14]               0\n          Conv2d-137         [-1, 1024, 14, 14]         262,144\n     BatchNorm2d-138         [-1, 1024, 14, 14]           2,048\n            ReLU-139         [-1, 1024, 14, 14]               0\n      Bottleneck-140         [-1, 1024, 14, 14]               0\n          Conv2d-141          [-1, 512, 14, 14]         524,288\n     BatchNorm2d-142          [-1, 512, 14, 14]           1,024\n            ReLU-143          [-1, 512, 14, 14]               0\n          Conv2d-144            [-1, 512, 7, 7]       2,359,296\n     BatchNorm2d-145            [-1, 512, 7, 7]           1,024\n            ReLU-146            [-1, 512, 7, 7]               0\n          Conv2d-147           [-1, 2048, 7, 7]       1,048,576\n     BatchNorm2d-148           [-1, 2048, 7, 7]           4,096\n          Conv2d-149           [-1, 2048, 7, 7]       2,097,152\n     BatchNorm2d-150           [-1, 2048, 7, 7]           4,096\n            ReLU-151           [-1, 2048, 7, 7]               0\n      Bottleneck-152           [-1, 2048, 7, 7]               0\n          Conv2d-153            [-1, 512, 7, 7]       1,048,576\n     BatchNorm2d-154            [-1, 512, 7, 7]           1,024\n            ReLU-155            [-1, 512, 7, 7]               0\n          Conv2d-156            [-1, 512, 7, 7]       2,359,296\n     BatchNorm2d-157            [-1, 512, 7, 7]           1,024\n            ReLU-158            [-1, 512, 7, 7]               0\n          Conv2d-159           [-1, 2048, 7, 7]       1,048,576\n     BatchNorm2d-160           [-1, 2048, 7, 7]           4,096\n            ReLU-161           [-1, 2048, 7, 7]               0\n      Bottleneck-162           [-1, 2048, 7, 7]               0\n          Conv2d-163            [-1, 512, 7, 7]       1,048,576\n     BatchNorm2d-164            [-1, 512, 7, 7]           1,024\n            ReLU-165            [-1, 512, 7, 7]               0\n          Conv2d-166            [-1, 512, 7, 7]       2,359,296\n     BatchNorm2d-167            [-1, 512, 7, 7]           1,024\n            ReLU-168            [-1, 512, 7, 7]               0\n          Conv2d-169           [-1, 2048, 7, 7]       1,048,576\n     BatchNorm2d-170           [-1, 2048, 7, 7]           4,096\n            ReLU-171           [-1, 2048, 7, 7]               0\n      Bottleneck-172           [-1, 2048, 7, 7]               0\nAdaptiveAvgPool2d-173           [-1, 2048, 1, 1]               0\n          Linear-174                    [-1, 3]           6,147\n================================================================\nTotal params: 23,514,179\nTrainable params: 23,514,179\nNon-trainable params: 0\n----------------------------------------------------------------\nInput size (MB): 0.57\nForward/backward pass size (MB): 286.55\nParams size (MB): 89.70\nEstimated Total Size (MB): 376.82\n----------------------------------------------------------------\n\n\nëª¨ë¸ì˜ êµ¬ì¡°ë¥¼ í™•ì¸í•´ë³´ë‹ˆ resnet ê¸°ë°˜ì˜ outputì´ 3ê°œì¸ ì• í˜¸ë°• ë¶„ë¥˜ê¸° ëª¨ë¸ì´ì—ˆìŠµë‹ˆë‹¤.\n\n\n\nìƒ˜í”Œ ë°ì´í„°ì…‹ êµ¬ì¶• (ì• í˜¸ë°•)\ndocker/transforms.py ì—ì„œ ì§ì ‘ í™•ì¸í•´ë³´ë‹ˆ, í•™ìŠµëœ ëª¨ë¸ì˜ ì´ë¯¸ì§€ ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ì€ ë‹¤ìŒê³¼ ê°™ì•˜ìŠµë‹ˆë‹¤. ë˜‘ê°™ì´ test_augë³€ìˆ˜ì— ë„£ì–´ ë°ì´í„°ì…‹ì„ êµ¬ì¶•í•˜ì˜€ìŠµë‹ˆë‹¤.\n\ntest_aug = albumentations.Compose([\n    albumentations.Resize(512, 512),\n    albumentations.Normalize(mean=[0.485, 0.456, 0.406],\n                             std=[0.229, 0.224, 0.225],\n                             max_pixel_value=255.0,\n                             p=1.0)], p=1.0)\n\n\n# validation\nwith open(\"data_preprocessing/dic_img2label_val.pickle\",\"rb\") as fr:\n    dic_img2label_val = pickle.load(fr)\n\n\n# ì• í˜¸ë°• ë°ì´í„°ë§Œ ë¶ˆëŸ¬ì˜¤ê¸° \nlst_valid_img = \"data/validation/images/ì• í˜¸ë°•\"\n\n\ndict_label = {0: 0, 7: 1, 8: 2}\n\nvalidX = []\nvalidY = []\nlst_img = []\n\nfor img_name in tqdm(lst_valid_img): \n    np_img = np.array(Image.open(path_valid_img + img_name))\n    transformed_img = test_aug(image=np_img)['image']\n    \n    validX.append(transformed_img) \n    validY.append(dict_label[dic_img2label_val[img_name]['disease']])\n    lst_img.append(img_name)\n\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1479/1479 [02:07<00:00, 11.59it/s]\n\n\nvalidation ë°ì´í„°ì˜ ë…ë¦½ë³€ìˆ˜, ì¢…ì†ë³€ìˆ˜ ë¦¬ìŠ¤íŠ¸ë¥¼ ìƒì„±í•˜ê³ , í›„ì— ë°”ìš´ë”© ë°•ìŠ¤ë¥¼ ê·¸ë¦¬ê¸° ìœ„í•˜ì—¬ ì´ë¯¸ì§€ ì´ë¦„ ë¦¬ìŠ¤íŠ¸ë„ í•¨ê»˜ ìƒì„±í•˜ì˜€ìŠµë‹ˆë‹¤.\n\n\n\nëª¨ë¸ í‰ê°€\n\nbatch_size = 32\nvalidX_batches = [validX[i:i+batch_size] for i in range(0, len(validX), batch_size)]\nvalidY_batches = [validY[i:i+batch_size] for i in range(0, len(validY), batch_size)]\n\n\nnp.array(validX_batches[0]).shape\n\n(32, 512, 512, 3)\n\n\në°°ì¹˜ ì‚¬ì´ì¦ˆë¥¼ 32ë¡œ í•˜ì˜€ê³ , í¬ê¸°ë¥¼ í™•ì¸í•˜ë‹ˆ (32, 512, 512, 3)ê³¼ ê°™ì´ ì •ìƒì ìœ¼ë¡œ ì ìš©ëœ ê²ƒì„ í™•ì¸í•˜ì˜€ìŠµë‹ˆë‹¤.\n\nlst_preds = [] \n\nwith torch.no_grad():\n    for i, (X_batch, Y_batch) in enumerate(tqdm(zip(validX_batches, validY_batches))):\n        X_batch = torch.tensor(X_batch).to(device)\n        Y_batch = torch.tensor(Y_batch).to(device) \n        \n        new_shape = (len(X_batch), 3, 512, 512)\n        X_batch = X_batch.permute(0, 3, 1, 2)  # channelì„ ë§¨ ì•ìœ¼ë¡œ ë³´ë‚´ê¸° ìœ„í•´ permute\n        X_batch = X_batch.view(new_shape)\n        \n        preds = model(X_batch)\n        \n        _, pred_labels = torch.max(preds, dim=1) \n        \n        # ë¯¸ë‹ˆë°°ì¹˜ ë§ˆë‹¤ì˜ preds ì¶”ê°€ \n        lst_preds.extend(pred_labels.cpu().tolist()) \n        \n\n# ì „ì²´ ë°ì´í„°ì˜ í‰ê°€ ì§€í‘œ ê³„ì‚° \naccuracy = accuracy_score(validY, lst_preds)\nprecision = precision_score(validY, lst_preds, average='macro')\nrecall = recall_score(validY, lst_preds, average='macro')\nf1 = f1_score(validY, lst_preds, average='macro')\n\nprint('Accuracy:', accuracy)\nprint('Precision:', precision)\nprint('Recall:', recall)\nprint('F1 score:', f1)\n\n47it [12:01, 15.36s/it]\n\n\nAccuracy: 0.530764029749831\nPrecision: 0.429514860306709\nRecall: 0.7757727491003014\nF1 score: 0.3874735945413221\n\n\n\n\n\naihubì˜ ì• í˜¸ë°• ë¶„ë¥˜ê¸° ëª¨ë¸ì„ í‰ê°€í•´ë³¸ ê²°ê³¼ ì •í™•ë„ ì•½ 53%, ë‚˜ë¨¸ì§€ ì§€í‘œë„ ê·¸ë¦¬ ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì´ì§€ëŠ” ì•Šì•˜ìŠµë‹ˆë‹¤. ì´í›„ì— í•™ìŠµì„ ì§„í–‰í•  ë•ŒëŠ”, ì´ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë” ë†’ì€ ì§€í‘œ ê²°ê³¼ê°€ ë‚˜ì˜¤ë„ë¡ í•™ìŠµì„ ì§„í–‰í•˜ëŠ” ê²ƒì„ ëª©í‘œë¡œ ì¡ì•˜ìŠµë‹ˆë‹¤.\n\n\n\nì´ë¯¸ì§€ë¡œ í™•ì¸í•´ë³´ê¸°\n\nlst_nums = random.sample(range(len(lst_img)), 9)\n\ndict_name = {0: 0, 1: 7, 2: 8}\n\nfig = plt.figure(figsize=(13, 13))\naxes = fig.subplots(3, 3).flatten()\n\nfor i, num in enumerate(lst_nums): \n    img_name = lst_img[num]\n    img = Image.open(path_valid_img + img_name)\n\n    d1 = dic_img2label_val[img_name]\n\n    crop, disease, points = d1['crop'], d1['disease'], d1['points']\n    pred = dict_name[lst_preds[num]]\n\n    draw = ImageDraw.Draw(img)\n    draw.rectangle([(points['xtl'], points['ytl']), (points['xbr'], points['ybr'])], outline=(255, 0, 255), width=30)\n\n    axes[i].set_title(f\"ì‹¤ì œ: {disease2name[disease]}  ||  ì˜ˆì¸¡: {disease2name[pred]}\")\n    axes[i].imshow(np.array(img))\n\n\n\n\nì‹¤ì œ ì´ë¯¸ì§€ë¥¼ ëœë¤ìœ¼ë¡œ 9ì¥ì„ ë½‘ì•„ë´¤ì„ ë•Œ, ê½¤ ì˜ ì˜ˆì¸¡í•˜ëŠ” ë“¯ í•©ë‹ˆë‹¤. ì •í™•ë„ëŠ” 50%ì§€ë§Œ, ìš´ì´ ì¢‹ê²Œ ì˜ ë½‘íŒ ê²ƒ ê°™ìŠµë‹ˆë‹¤. ë˜, ì• í˜¸ë°•ì´ë¼ë„ ë¶€ìœ„ê°€ ë‹¤ë¥´ë‹¤ëŠ” ì ë„ í™•ì¸í•˜ì˜€ìŠµë‹ˆë‹¤. ìë„ ìˆê³  ì¤„ê¸°ë„ ìˆê³ , ê½ƒë„ ìˆê³ , ì—´ë§¤ë„ ìˆê¸°ì—, ëª¨ë¸ í•™ìŠµì´ ì œëŒ€ë¡œ ë˜ì§€ ì•Šì€ ì›ì¸ë„ ìˆì–´ë³´ì…ë‹ˆë‹¤.\nì§€ê¸ˆê¹Œì§€ AIHUB ìƒ˜í”Œ ëª¨ë¸ì„ í™•ì¸í•´ ë³´ì•˜ê³ , ë‹¤ìŒ ê¸€ì—ì„œëŠ” EDAì—ì„œ ì²˜ë¦¬í•´ ë†“ì€ 20ê°œì˜ ë ˆì´ë¸”ì— ëŒ€í•œ ëª¨ë¸ í•™ìŠµì„ ì§ì ‘ ì§„í–‰í•´ë³´ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤."
  },
  {
    "objectID": "posts/c03.image_classification.html#ì‘ë¬¼-ì§ˆë³‘-ì§„ë‹¨-ëª¨ë¸-í•™ìŠµ-ë°-í‰ê°€-mobilenetv2",
    "href": "posts/c03.image_classification.html#ì‘ë¬¼-ì§ˆë³‘-ì§„ë‹¨-ëª¨ë¸-í•™ìŠµ-ë°-í‰ê°€-mobilenetv2",
    "title": "[cropdoctor] 3. ì‘ë¬¼ ì§ˆë³‘ ì§„ë‹¨ ëª¨ë¸ í•™ìŠµ ë° í‰ê°€ (mobilenetV2)",
    "section": "3. ì‘ë¬¼ ì§ˆë³‘ ì§„ë‹¨ ëª¨ë¸ í•™ìŠµ ë° í‰ê°€ (mobilenetV2)",
    "text": "3. ì‘ë¬¼ ì§ˆë³‘ ì§„ë‹¨ ëª¨ë¸ í•™ìŠµ ë° í‰ê°€ (mobilenetV2)\n\nimport pickle \nimport os \nfrom tqdm import tqdm\nimport random  \nimport time   \n    \nfrom PIL import Image, ImageDraw, ImageFile \n\nimport albumentations\n\nimport matplotlib.pyplot as plt               \nplt.rc(\"font\", family=\"NanumGothic\", size=13) \n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\nfrom sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torchvision.models as models\nfrom torchsummary import summary\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision.transforms as transforms\nimport torchvision.datasets as datasets\nfrom torchvision.datasets import ImageFolder\nfrom torch.utils.data import DataLoader\nfrom torch.utils.tensorboard import SummaryWriter\nfrom torch.utils.data.sampler import WeightedRandomSampler\nimport torchvision.transforms as transforms\n\nImageFile.LOAD_TRUNCATED_IMAGES = True"
  },
  {
    "objectID": "posts/c03.image_classification.html#ëª¨ë¸-í•™ìŠµ",
    "href": "posts/c03.image_classification.html#ëª¨ë¸-í•™ìŠµ",
    "title": "[cropdoctor] 3. ì‘ë¬¼ ì§ˆë³‘ ì§„ë‹¨ ëª¨ë¸ í•™ìŠµ ë° í‰ê°€ (mobilenetV2)",
    "section": "01. ëª¨ë¸ í•™ìŠµ",
    "text": "01. ëª¨ë¸ í•™ìŠµ\nì´ë²ˆ ê¸€ì—ì„œëŠ” ë…¸ì§€ì‘ë¬¼ì˜ ì§ˆë³‘ì„ ì§„ë‹¨í•˜ëŠ” ëª¨ë¸ì„ í•™ìŠµí•©ë‹ˆë‹¤. ì´ë²ˆ ê¸€ì—ì„œëŠ” ë² ì´ìŠ¤ë¼ì¸ ëª¨ë¸ë¡œ, Image Classification ë°©ë²•ì„ ì‚¬ìš©í•©ë‹ˆë‹¤. ê·¸ ë‹¤ìŒ, ì‚¬ìš©í•˜ëŠ” ë°ì´í„°ì…‹ì˜ ë¼ë²¨ ë°ì´í„°ì— ë°”ìš´ë”© ë°•ìŠ¤ê°€ í¬í•¨ë˜ì–´ ìˆìœ¼ë¯€ë¡œ, Object Detection ëª¨ë¸ ë˜í•œ í•™ìŠµí•´ ë³¼ ì˜ˆì •ì…ë‹ˆë‹¤.\në¨¼ì €, Image Classification ì „ì´í•™ìŠµì„ ì§„í–‰í•˜ë©°, ì‚¬ì „í•™ìŠµ ëª¨ë¸ì€ mobilenet v2ì„ ì‚¬ìš©í•©ë‹ˆë‹¤. mobilenet V2ëŠ” ê²½ëŸ‰í™”ëœ ì•„í‚¤í…ì²˜ë¡œ, íŒŒë¼ë¯¸í„° ìˆ˜ë¥¼ ì¤„ì´ê³  ì—°ì‚°ëŸ‰ì„ ìµœì í™”ëœ ëª¨ë¸ì…ë‹ˆë‹¤. ì‘ì€ ëª¨ë¸ í¬ê¸°ì™€ ì ì€ ì—°ì‚° ìš”êµ¬ë¡œ ì œí•œëœ ìì›ì„ ê°€ì§„ í™˜ê²½ì—ì„œë„ íš¨ìœ¨ì ìœ¼ë¡œ ì‹¤í–‰ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì‘ì€ ëª¨ë¸ í¬ê¸°ì™€ ë‚®ì€ ì—°ì‚° ìš”êµ¬ë¥¼ ìœ ì§€í•˜ë©´ì„œë„ ì •í™•í•œ ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ì–»ì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ, í”„ë¡œì íŠ¸ì—ì„œ ì œê³µë°›ì€ VMì˜ ì„±ëŠ¥ì—ì„œ ë¬´ë¦¬ ì—†ì´ ëŒì•„ê°€ê²Œ í•˜ë©´ì„œë„, ë†’ì€ ë¶„ë¥˜ ì„±ëŠ¥ì„ ì œê³µí•´ì£¼ë¯€ë¡œ í•´ë‹¹ ëª¨ë¸ì„ ì„ ì •í•˜ì˜€ìŠµë‹ˆë‹¤.\nì‹œê°„ì´ë‚˜ ìš©ëŸ‰ ë“± ë¬¼ë¦¬ì  ìì›ì´ ì—¬ìœ ë¡œì› ë‹¤ë©´ ImageNetì˜ ì—¬ëŸ¬ ëª¨ë¸ë¡œ í•™ìŠµì‹œì¼œ ì„±ëŠ¥ì„ ë¹„êµí•´ë³´ê³  ì‹¶ì—ˆì§€ë§Œ, ê·¸ëŸ´ë§Œí•œ ì—¬ìœ ëŠ” ì—†ì—ˆê¸°ì— mobilenet V2 ëª¨ë¸ë¡œ ê°„ë‹¨í•œ íŒŒë¼ë¯¸í„° ì¡°ì •ì„ í†µí•œ ë¹„êµë§Œ ì§„í–‰í•©ë‹ˆë‹¤.\n\n\n\nimage.png\n\n\nìœ„ì˜ ë„ì‹í™” ì‚¬ì§„ê³¼ ê°™ì´, ì´ë¯¸ì§€ì˜ input ì‚¬ì´ì¦ˆëŠ” (224, 224, 3)ìœ¼ë¡œ ë‘ì—ˆê³ , outputì€ ìš°ë¦¬ ë°ì´í„° ë ˆì´ë¸” ê°œìˆ˜ì— ë§ê²Œ (1, 20)ìœ¼ë¡œ ì„¤ì •í•˜ì—¬ ëª¨ë¸ì„ êµ¬ì„±í•˜ì˜€ìŠµë‹ˆë‹¤.\n\n\nëª¨ë¸ í•™ìŠµ ì½”ë“œ\nmobilenetV2_1.py\n\n# Device ì •ì˜\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# í•˜ì´í¼ íŒŒë¼ë¯¸í„° ì •ì˜\nnum_classes = 20\nbatch_size = 64\nlearning_rate = 0.001\nnum_epochs = 10\n\n# ì‚¬ì „í•™ìŠµëœ mobilenet V2 ë¶ˆëŸ¬ì˜¤ê¸° \nmodel = torch.hub.load('pytorch/vision:v0.9.0', 'mobilenet_v2', pretrained=True)\n\n# ë§ˆì§€ë§‰ FC ë ˆì´ì–´ë¥¼ ìš°ë¦¬ ë°ì´í„°ì…‹ì— ë§ê²Œ ì¡°ì •í•´ì£¼ê¸° \nnum_ftrs = model.classifier[-1].in_features\nmodel.classifier[-1] = nn.Linear(num_ftrs, num_classes)\nmodel = model.to(device)\n\n# loss function ê³¼ optimizer ë¥¼ ì„¤ì • \ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=learning_rate)\n\n\n# ì…ë ¥ ì´ë¯¸ì§€ ë°ì´í„°ì…‹ ì „ì²˜ë¦¬ (ì‚¬ì´ì¦ˆ 224, 224, tensor íƒ€ì… ë³€ê²½, ìŠ¤ì¼€ì¼ í‘œì¤€í™”)\ndata_transforms = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])\n\n\n\n# ë°ì´í„°ì…‹ ê²½ë¡œ ì„¤ì • ë° ë°ì´í„°ì…‹ ë¶ˆëŸ¬ì˜¤ê¸° \npath_train_img = \"data/training/image_class/\"\npath_valid_img = \"data/validation/image_class/\"\n\ntrain_dataset = ImageFolder(root=path_train_img, transform=data_transforms) # ë°ì´í„° ì¦ê°• O\ntest_dataset = ImageFolder(root=path_valid_img, transform=data_transforms) # test_dataset -> val_dataset, ë°ì´í„° ì¦ê°• X\ntrain_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\ntest_loader = DataLoader(dataset=test_dataset, batch_size=1, shuffle=True) # test -> val, batch = 1\n\n\n\n# TensorBoard writerë¥¼ í†µí•´ í•™ìŠµê³¼ì •ì„ ì‹œê°í™”í•´ë³´ê¸°\nwriter = SummaryWriter()\n\n# ëª¨ë¸ í•™ìŠµ\ntotal_step = len(train_loader)\nstart_time = time.time()\nfor epoch in range(num_epochs):\n    model.train() # í•™ìŠµ ëª¨ë“œë¡œ ì„¤ì • \n    for i, (images, labels) in tqdm(enumerate(train_loader), total=len(train_loader), leave=False):\n        images = images.to(device)\n        labels = labels.to(device)\n        \n        # ìˆœì „íŒŒ\n        outputs = model(images)\n        loss = criterion(outputs, labels)\n\n        # ì—­ì „íŒŒ ë° optimize\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        # TensorBoardì—ì„œ accuracy ì™€ loss ì¶”ì   \n        _, predicted = torch.max(outputs.data, 1)\n        total = labels.size(0)\n        correct = (predicted == labels).sum().item()\n        acc = 100 * correct / total\n        writer.add_scalar('Train/Loss', loss.item(), epoch * total_step + i)\n        writer.add_scalar('Train/Accuracy', acc, epoch * total_step + i)\n    writer.flush()\n    \n    \n    model.eval() # í‰ê°€ ëª¨ë“œë¡œ ì„¤ì • \n    # validation ë°ì´í„°ì…‹ìœ¼ë¡œ ëª¨ë¸ í‰ê°€í•˜ê¸° \n    with torch.no_grad():\n        correct = 0\n        total = 0\n        for i, (images, labels) in tqdm(enumerate(test_loader), total=len(test_loader), leave=False):\n            images = images.to(device)\n            labels = labels.to(device)\n            outputs = model(images)\n            _, predicted = torch.max(outputs.data, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n\n        test_acc = 100 * correct / total\n        writer.add_scalar('Test/Accuracy', test_acc, epoch)\n        writer.flush()\n\n    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}, Training Accuracy: {acc:.2f}%, Test Accuracy: {test_acc:.2f}%')\n\nend_time = time.time()\nprint(f'Training complete in {(end_time - start_time):.0f} seconds')\n\n\n# ëª¨ë¸ ì •ë³´ê°€ ë‹´ê¸´ ê°€ì¤‘ì¹˜ íŒŒì¼ì„ ckpt í™•ì¥ìë¡œ ì €ì¥\ntorch.save(model.state_dict(), 'model/mobilenet_v2_1.ckpt')\nprint('Model saved as mobilenet_v2_1.ckpt')\n\n\n# Tensorboard writer ì¢…ë£Œ \nwriter.close()\n\n\n\n\nğŸ’¡ tmux ì‚¬ìš©\nì œê³µë°›ì€ VM í™˜ê²½ì—ì„œ 1 epoch ë‹¹ ì•½ 1ì‹œê°„ ì •ë„ì˜ ì‹œê°„ì´ ê±¸ë ¸ìŠµë‹ˆë‹¤. 10 epochë§Œ ëŒë ¤ë„ 10ì‹œê°„ ì´ìƒì´ ê±¸ë ¸ê¸°ì—, ì„œë²„ê°€ ì¬ë¶€íŒ…ë˜ì§€ ì•ŠëŠ” ì´ìƒ í•™ìŠµ ì¤‘ì¸ ì„¸ì…˜ì´ ì¢…ë£Œë˜ì§€ ì•Šì„ ìˆ˜ ìˆëŠ” tmuxë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸í•™ìŠµì„ ì§„í–‰í•˜ì˜€ìŠµë‹ˆë‹¤.\n1. ì„¤ì¹˜\n\nsudo apt-get install tmux\n\n\n2. ì„¸ì…˜ ì‹œì‘\n\ntmux new -s NAME\n\nìœ„ì™€ ê°™ì´ tmux ì„¸ì…˜ì„ ì‹œì‘í•©ë‹ˆë‹¤.\n\n3. ëª¨ë¸ í•™ìŠµ ì‹¤í–‰\n\npython mobilenetV2_1.py\n\nì‹œì‘ëœ tmux ì„¸ì…˜ ì•ˆì˜ í„°ë¯¸ë„ì—ì„œ ìœ„ì˜ ëª…ë ¹ì–´ë¡œ ëª¨ë¸ í•™ìŠµì„ ì§„í–‰ì‹œì¼°ìŠµë‹ˆë‹¤."
  },
  {
    "objectID": "posts/c03.image_classification.html#ëª¨ë¸-í‰ê°€",
    "href": "posts/c03.image_classification.html#ëª¨ë¸-í‰ê°€",
    "title": "[cropdoctor] 3. ì‘ë¬¼ ì§ˆë³‘ ì§„ë‹¨ ëª¨ë¸ í•™ìŠµ ë° í‰ê°€ (mobilenetV2)",
    "section": "02. ëª¨ë¸ í‰ê°€",
    "text": "02. ëª¨ë¸ í‰ê°€\n\n\nì´ë¦„ ë³€í™˜ ë”•ì…”ë„ˆë¦¬\n\ndisease2name_new = {\n               0: \"ê³ ì¶”ì •ìƒ\",\n               1: \"ê³ ì¶”íƒ„ì €ë³‘\",\n               2: \"ê³ ì¶”í°ê°€ë£¨ë³‘\",\n               \n               3: \"ë¬´ì •ìƒ\",\n               4: \"ë¬´ê²€ì€ë¬´ëŠ¬ë³‘\", \n               5: \"ë¬´ë…¸ê· ë³‘\", \n    \n               6: \"ë°°ì¶”ì •ìƒ\",\n               7: \"ë°°ì¶”ê²€ìŒì©ìŒë³‘\",\n               8: \"ë°°ì¶”ë…¸ê· ë³‘\",\n    \n               9: \"ì• í˜¸ë°•ì •ìƒ\",\n               10: \"ì• í˜¸ë°•ë…¸ê· ë³‘\",\n               11: \"ì• í˜¸ë°•í°ê°€ë£¨ë³‘\", \n               \n               12: \"ì½©ì •ìƒ\",\n               13: \"ì½©ë¶ˆë§ˆë¦„ë³‘\",\n               14: \"ì½©ì ë¬´ëŠ¬ë³‘\",\n    \n               15: \"í† ë§ˆí† ì •ìƒ\",\n               16: \"í† ë§ˆí† ìë§ˆë¦„ë³‘\", \n    \n               17: \"í˜¸ë°•ì •ìƒ\",\n               18: \"í˜¸ë°•ë…¸ê· ë³‘\",\n               19: \"í˜¸ë°•í°ê°€ë£¨ë³‘\"}\n\nlst_label_name = [\"ê³ ì¶”ì •ìƒ\", \"ê³ ì¶”íƒ„ì €ë³‘\", \"ê³ ì¶”í°ê°€ë£¨ë³‘\", \n                    \"ë¬´ì •ìƒ\", \"ë¬´ê²€ì€ë¬´ëŠ¬ë³‘\", \"ë¬´ë…¸ê· ë³‘\", \n                    \"ë°°ì¶”ì •ìƒ\", \"ë°°ì¶”ê²€ìŒì©ìŒë³‘\", \"ë°°ì¶”ë…¸ê· ë³‘\", \n                    \"ì• í˜¸ë°•ì •ìƒ\", \"ì• í˜¸ë°•ë…¸ê· ë³‘\", \"ì• í˜¸ë°•í°ê°€ë£¨ë³‘\", \n                    \"ì½©ì •ìƒ\", \"ì½©ë¶ˆë§ˆë¦„ë³‘\", \"ì½©ì ë¬´ëŠ¬ë³‘\", \n                    \"í† ë§ˆí† ì •ìƒ\", \"í† ë§ˆí† ìë§ˆë¦„ë³‘\", \n                    \"í˜¸ë°•ì •ìƒ\", \"í˜¸ë°•ë…¸ê· ë³‘\", \"í˜¸ë°•í°ê°€ë£¨ë³‘\"]\n\n\n\n\nëª¨ë¸ ìš”ì•½\n\nmodel_path = \"model/mobilenet_v2_1.ckpt\"\n  \ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nmodel_weights = torch.load(model_path, map_location=device)\n\ní•™ìŠµí•˜ì—¬ ì €ì¥ëœ ckpt ê°€ì¤‘ì¹˜ íŒŒì¼ì„ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.\n\n\n\nëª¨ë¸ ìƒì„±\n\nmodel = torch.hub.load('pytorch/vision:v0.9.0', 'mobilenet_v2', pretrained=False)\nnum_ftrs = model.classifier[-1].in_features\nmodel.classifier[-1] = nn.Linear(num_ftrs, 20)\nmodel.load_state_dict(model_weights)\nmodel = model.to(device)\n\nUsing cache found in /home/elicer/.cache/torch/hub/pytorch_vision_v0.9.0\n\n\në¶ˆëŸ¬ì˜¨ ëª¨ë¸ì˜ ê°€ì¤‘ì¹˜ íŒŒì¼ì„ í•™ìŠµí•œ ëª¨ë¸ êµ¬ì¡°ì— ë§ê²Œ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.\n\n\n\nëª¨ë¸ ìš”ì•½\n\nsummary(model, input_size=(3, 224, 224))\n\n----------------------------------------------------------------\n        Layer (type)               Output Shape         Param #\n================================================================\n            Conv2d-1         [-1, 32, 112, 112]             864\n       BatchNorm2d-2         [-1, 32, 112, 112]              64\n             ReLU6-3         [-1, 32, 112, 112]               0\n            Conv2d-4         [-1, 32, 112, 112]             288\n       BatchNorm2d-5         [-1, 32, 112, 112]              64\n             ReLU6-6         [-1, 32, 112, 112]               0\n            Conv2d-7         [-1, 16, 112, 112]             512\n       BatchNorm2d-8         [-1, 16, 112, 112]              32\n  InvertedResidual-9         [-1, 16, 112, 112]               0\n           Conv2d-10         [-1, 96, 112, 112]           1,536\n      BatchNorm2d-11         [-1, 96, 112, 112]             192\n            ReLU6-12         [-1, 96, 112, 112]               0\n           Conv2d-13           [-1, 96, 56, 56]             864\n      BatchNorm2d-14           [-1, 96, 56, 56]             192\n            ReLU6-15           [-1, 96, 56, 56]               0\n           Conv2d-16           [-1, 24, 56, 56]           2,304\n      BatchNorm2d-17           [-1, 24, 56, 56]              48\n InvertedResidual-18           [-1, 24, 56, 56]               0\n           Conv2d-19          [-1, 144, 56, 56]           3,456\n      BatchNorm2d-20          [-1, 144, 56, 56]             288\n            ReLU6-21          [-1, 144, 56, 56]               0\n           Conv2d-22          [-1, 144, 56, 56]           1,296\n      BatchNorm2d-23          [-1, 144, 56, 56]             288\n            ReLU6-24          [-1, 144, 56, 56]               0\n           Conv2d-25           [-1, 24, 56, 56]           3,456\n      BatchNorm2d-26           [-1, 24, 56, 56]              48\n InvertedResidual-27           [-1, 24, 56, 56]               0\n           Conv2d-28          [-1, 144, 56, 56]           3,456\n      BatchNorm2d-29          [-1, 144, 56, 56]             288\n            ReLU6-30          [-1, 144, 56, 56]               0\n           Conv2d-31          [-1, 144, 28, 28]           1,296\n      BatchNorm2d-32          [-1, 144, 28, 28]             288\n            ReLU6-33          [-1, 144, 28, 28]               0\n           Conv2d-34           [-1, 32, 28, 28]           4,608\n      BatchNorm2d-35           [-1, 32, 28, 28]              64\n InvertedResidual-36           [-1, 32, 28, 28]               0\n           Conv2d-37          [-1, 192, 28, 28]           6,144\n      BatchNorm2d-38          [-1, 192, 28, 28]             384\n            ReLU6-39          [-1, 192, 28, 28]               0\n           Conv2d-40          [-1, 192, 28, 28]           1,728\n      BatchNorm2d-41          [-1, 192, 28, 28]             384\n            ReLU6-42          [-1, 192, 28, 28]               0\n           Conv2d-43           [-1, 32, 28, 28]           6,144\n      BatchNorm2d-44           [-1, 32, 28, 28]              64\n InvertedResidual-45           [-1, 32, 28, 28]               0\n           Conv2d-46          [-1, 192, 28, 28]           6,144\n      BatchNorm2d-47          [-1, 192, 28, 28]             384\n            ReLU6-48          [-1, 192, 28, 28]               0\n           Conv2d-49          [-1, 192, 28, 28]           1,728\n      BatchNorm2d-50          [-1, 192, 28, 28]             384\n            ReLU6-51          [-1, 192, 28, 28]               0\n           Conv2d-52           [-1, 32, 28, 28]           6,144\n      BatchNorm2d-53           [-1, 32, 28, 28]              64\n InvertedResidual-54           [-1, 32, 28, 28]               0\n           Conv2d-55          [-1, 192, 28, 28]           6,144\n      BatchNorm2d-56          [-1, 192, 28, 28]             384\n            ReLU6-57          [-1, 192, 28, 28]               0\n           Conv2d-58          [-1, 192, 14, 14]           1,728\n      BatchNorm2d-59          [-1, 192, 14, 14]             384\n            ReLU6-60          [-1, 192, 14, 14]               0\n           Conv2d-61           [-1, 64, 14, 14]          12,288\n      BatchNorm2d-62           [-1, 64, 14, 14]             128\n InvertedResidual-63           [-1, 64, 14, 14]               0\n           Conv2d-64          [-1, 384, 14, 14]          24,576\n      BatchNorm2d-65          [-1, 384, 14, 14]             768\n            ReLU6-66          [-1, 384, 14, 14]               0\n           Conv2d-67          [-1, 384, 14, 14]           3,456\n      BatchNorm2d-68          [-1, 384, 14, 14]             768\n            ReLU6-69          [-1, 384, 14, 14]               0\n           Conv2d-70           [-1, 64, 14, 14]          24,576\n      BatchNorm2d-71           [-1, 64, 14, 14]             128\n InvertedResidual-72           [-1, 64, 14, 14]               0\n           Conv2d-73          [-1, 384, 14, 14]          24,576\n      BatchNorm2d-74          [-1, 384, 14, 14]             768\n            ReLU6-75          [-1, 384, 14, 14]               0\n           Conv2d-76          [-1, 384, 14, 14]           3,456\n      BatchNorm2d-77          [-1, 384, 14, 14]             768\n            ReLU6-78          [-1, 384, 14, 14]               0\n           Conv2d-79           [-1, 64, 14, 14]          24,576\n      BatchNorm2d-80           [-1, 64, 14, 14]             128\n InvertedResidual-81           [-1, 64, 14, 14]               0\n           Conv2d-82          [-1, 384, 14, 14]          24,576\n      BatchNorm2d-83          [-1, 384, 14, 14]             768\n            ReLU6-84          [-1, 384, 14, 14]               0\n           Conv2d-85          [-1, 384, 14, 14]           3,456\n      BatchNorm2d-86          [-1, 384, 14, 14]             768\n            ReLU6-87          [-1, 384, 14, 14]               0\n           Conv2d-88           [-1, 64, 14, 14]          24,576\n      BatchNorm2d-89           [-1, 64, 14, 14]             128\n InvertedResidual-90           [-1, 64, 14, 14]               0\n           Conv2d-91          [-1, 384, 14, 14]          24,576\n      BatchNorm2d-92          [-1, 384, 14, 14]             768\n            ReLU6-93          [-1, 384, 14, 14]               0\n           Conv2d-94          [-1, 384, 14, 14]           3,456\n      BatchNorm2d-95          [-1, 384, 14, 14]             768\n            ReLU6-96          [-1, 384, 14, 14]               0\n           Conv2d-97           [-1, 96, 14, 14]          36,864\n      BatchNorm2d-98           [-1, 96, 14, 14]             192\n InvertedResidual-99           [-1, 96, 14, 14]               0\n          Conv2d-100          [-1, 576, 14, 14]          55,296\n     BatchNorm2d-101          [-1, 576, 14, 14]           1,152\n           ReLU6-102          [-1, 576, 14, 14]               0\n          Conv2d-103          [-1, 576, 14, 14]           5,184\n     BatchNorm2d-104          [-1, 576, 14, 14]           1,152\n           ReLU6-105          [-1, 576, 14, 14]               0\n          Conv2d-106           [-1, 96, 14, 14]          55,296\n     BatchNorm2d-107           [-1, 96, 14, 14]             192\nInvertedResidual-108           [-1, 96, 14, 14]               0\n          Conv2d-109          [-1, 576, 14, 14]          55,296\n     BatchNorm2d-110          [-1, 576, 14, 14]           1,152\n           ReLU6-111          [-1, 576, 14, 14]               0\n          Conv2d-112          [-1, 576, 14, 14]           5,184\n     BatchNorm2d-113          [-1, 576, 14, 14]           1,152\n           ReLU6-114          [-1, 576, 14, 14]               0\n          Conv2d-115           [-1, 96, 14, 14]          55,296\n     BatchNorm2d-116           [-1, 96, 14, 14]             192\nInvertedResidual-117           [-1, 96, 14, 14]               0\n          Conv2d-118          [-1, 576, 14, 14]          55,296\n     BatchNorm2d-119          [-1, 576, 14, 14]           1,152\n           ReLU6-120          [-1, 576, 14, 14]               0\n          Conv2d-121            [-1, 576, 7, 7]           5,184\n     BatchNorm2d-122            [-1, 576, 7, 7]           1,152\n           ReLU6-123            [-1, 576, 7, 7]               0\n          Conv2d-124            [-1, 160, 7, 7]          92,160\n     BatchNorm2d-125            [-1, 160, 7, 7]             320\nInvertedResidual-126            [-1, 160, 7, 7]               0\n          Conv2d-127            [-1, 960, 7, 7]         153,600\n     BatchNorm2d-128            [-1, 960, 7, 7]           1,920\n           ReLU6-129            [-1, 960, 7, 7]               0\n          Conv2d-130            [-1, 960, 7, 7]           8,640\n     BatchNorm2d-131            [-1, 960, 7, 7]           1,920\n           ReLU6-132            [-1, 960, 7, 7]               0\n          Conv2d-133            [-1, 160, 7, 7]         153,600\n     BatchNorm2d-134            [-1, 160, 7, 7]             320\nInvertedResidual-135            [-1, 160, 7, 7]               0\n          Conv2d-136            [-1, 960, 7, 7]         153,600\n     BatchNorm2d-137            [-1, 960, 7, 7]           1,920\n           ReLU6-138            [-1, 960, 7, 7]               0\n          Conv2d-139            [-1, 960, 7, 7]           8,640\n     BatchNorm2d-140            [-1, 960, 7, 7]           1,920\n           ReLU6-141            [-1, 960, 7, 7]               0\n          Conv2d-142            [-1, 160, 7, 7]         153,600\n     BatchNorm2d-143            [-1, 160, 7, 7]             320\nInvertedResidual-144            [-1, 160, 7, 7]               0\n          Conv2d-145            [-1, 960, 7, 7]         153,600\n     BatchNorm2d-146            [-1, 960, 7, 7]           1,920\n           ReLU6-147            [-1, 960, 7, 7]               0\n          Conv2d-148            [-1, 960, 7, 7]           8,640\n     BatchNorm2d-149            [-1, 960, 7, 7]           1,920\n           ReLU6-150            [-1, 960, 7, 7]               0\n          Conv2d-151            [-1, 320, 7, 7]         307,200\n     BatchNorm2d-152            [-1, 320, 7, 7]             640\nInvertedResidual-153            [-1, 320, 7, 7]               0\n          Conv2d-154           [-1, 1280, 7, 7]         409,600\n     BatchNorm2d-155           [-1, 1280, 7, 7]           2,560\n           ReLU6-156           [-1, 1280, 7, 7]               0\n         Dropout-157                 [-1, 1280]               0\n          Linear-158                   [-1, 20]          25,620\n================================================================\nTotal params: 2,249,492\nTrainable params: 2,249,492\nNon-trainable params: 0\n----------------------------------------------------------------\nInput size (MB): 0.57\nForward/backward pass size (MB): 152.86\nParams size (MB): 8.58\nEstimated Total Size (MB): 162.02\n----------------------------------------------------------------\n\n\ninputì´ (3, 224, 224)ì´ê³ , outputì´ (1, 20)ì¸ mobilenet V2 ì „ì´í•™ìŠµ ëª¨ë¸ì˜ êµ¬ì¡°ì„ì„ í™•ì¸í•©ë‹ˆë‹¤.\n\n\n\nëª¨ë¸ í‰ê°€\nëª¨ë¸ í•™ìŠµ ì‹œ ì—í­ë§ˆë‹¤ ì •í™•ë„ë¥¼ ì¶œë ¥í•´ì£¼ì—ˆëŠ”ë°, í•´ë‹¹ ì½”ë“œì™€ ê±°ì˜ ìœ ì‚¬í•©ë‹ˆë‹¤. ëª‡ê°€ì§€ ì¶”ê°€ëœ ì ì€, ì •í™•ë„ ì™¸ì˜ ì •ë°€ë„, ì¬í˜„ìœ¨, f1 score ì§€í‘œ ê³„ì‚°ê³¼, í˜¼ë™í–‰ë ¬ì˜ ì‹œê°í™”ì…ë‹ˆë‹¤.\n\n\në°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬\n\nvalid_transform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])\n\n\npath_valid_img = \"data/validation/image_class/\"\n\nvalid_dataset = ImageFolder(root=path_valid_img, transform=valid_transform)\nvalid_loader = DataLoader(dataset=valid_dataset, batch_size=1)\n\ní•™ìŠµí•  ë•Œ train ë°ì´í„°ì™€ ë§ˆì°¬ê°€ì§€ë¡œ 224x224ë¡œ ë¦¬ì‚¬ì´ì§•í•˜ë©°, ìŠ¤ì¼€ì¼ì„ í‘œì¤€í™” í•˜ì—¬ validation ì´ë¯¸ì§€ ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.\n\n\n\nëª¨ë¸ ì˜ˆì¸¡\n\nmodel.eval() # ì¤‘ìš” \nwith torch.no_grad():\n    lst_pred = []\n    lst_labels = []\n\n    correct = 0\n    total = 0\n    \n    for i, (images, labels) in tqdm(enumerate(valid_loader), total=len(valid_loader)):\n        images = images.to(device)\n        labels = labels.to(device)\n        outputs = model(images)\n        _, predicted = torch.max(outputs.data, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n        lst_pred.extend(predicted.cpu().tolist())\n        lst_labels.extend(labels.cpu().tolist())\n\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7873/7873 [19:50<00:00,  6.62it/s]\n\n\në¶ˆëŸ¬ì˜¨ ëª¨ë¸ê³¼ ë°ì´í„°ë¡œ ì˜ˆì¸¡ì„ ì§„í–‰í•©ë‹ˆë‹¤. ë°‘ì—ì„œ í‰ê°€ì§€í‘œì™€ í˜¼ë™í–‰ë ¬ì„ êµ¬í•˜ê¸° ìœ„í•´ ì˜ˆì¸¡ ê°’ê³¼ ì •ë‹µê°’ì„ ê°ê° lst_pred, lst_labels ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€í•©ë‹ˆë‹¤.\n\n\n\ní‰ê°€ ì§€í‘œ\n\n# ì „ì²´ validation ë°ì´í„°ì…‹ì˜ í‰ê°€ ì§€í‘œ ê³„ì‚° \naccuracy = accuracy_score(lst_labels, lst_pred)\nprecision = precision_score(lst_labels, lst_pred, average='macro')\nrecall = recall_score(lst_labels, lst_pred, average='macro')\nf1 = f1_score(lst_labels, lst_pred, average='macro')\n\n\nprint('Accuracy:', accuracy)\nprint('Precision:', precision)\nprint('Recall:', recall)\nprint('F1 score:', f1)\n\n\nfig = plt.figure(figsize=(5, 3))\nax = fig.subplots()\nbars = ax.bar(range(4), [accuracy, precision, recall, f1], color='#e0a4b2', edgecolor = 'black')\nax.set_xticks(range(4))\nax.set_xticklabels([\"Accuracy\", \"Precision\", \"Recall\", \"F1 score\"], fontsize=12)\nax.set_ylim(0, 1.1)\n\nplt.title(\"mobilenet_v2_1 í‰ê°€ì§€í‘œ\", fontsize=13, pad=10)\n\nfor i, b in enumerate(bars.patches):\n    _=plt.text(b.get_x()+b.get_width()*(1/2),b.get_height()+0.05, \\\n            round(b.get_height(), 2),ha='center',fontsize=12, color='k')\n\nplt.show()\n\nAccuracy: 0.9350946272069097\nPrecision: 0.846919676923046\nRecall: 0.8981016401529176\nF1 score: 0.8572522290077309\n\n\n\n\n\nì •í™•ë„ 0.94, ì •ë°€ë„ 0.85, ì¬í˜„ìœ¨ 0.9, F1 score 0.86ìœ¼ë¡œ, í‰ê°€ì§€í‘œì˜ ê°’ì€ ê½¤ ë†’ì€ ìˆ˜ì¹˜ë¥¼ ë³´ì˜€ìŠµë‹ˆë‹¤.\n\n\n\ní˜¼ë™í–‰ë ¬, confusion matrix\n\nimport seaborn as sns \n\n\nplt.figure(figsize=(15, 15))\ncm = confusion_matrix(lst_labels, lst_pred)\n_=sns.heatmap(cm, annot=True, fmt='d', cmap='Reds', square=True)\n\n\n\nplt.xticks(np.arange(len(lst_label_name))+0.5, lst_label_name, rotation=45)\nplt.yticks(np.arange(len(lst_label_name))+0.5, lst_label_name, rotation=45)\n\n\n# ê·¸ë˜í”„ ì œëª©, xì¶• ë¼ë²¨, yì¶• ë¼ë²¨ ì„¤ì •\nplt.title('Confusion Matrix')\nplt.xlabel('Predicted Label')\nplt.ylabel('True Label')\n\n# ê·¸ë˜í”„ ì¶œë ¥\nplt.show()\n\n\n\n\n40ê°œ ì´ìƒ ì˜ ëª» ì˜ˆì¸¡í•œ ë ˆì´ë¸”ì€ ê³ ì¶” ì •ìƒ -> ì½©ì •ìƒ, ì• í˜¸ë°• ì •ìƒ -> ì½© ì •ìƒìœ¼ë¡œ ì˜ˆì¸¡í•œ ê°’ë“¤ì…ë‹ˆë‹¤. ì„¸ ì‘ë¬¼ì€ ì´íŒŒë¦¬ë‚˜ ì¤„ê¸°ê°€ ì´ˆë¡ìƒ‰ìœ¼ë©°, ë¹„ìŠ·í•˜ê²Œ ìƒê²¼ë‹¤ëŠ” ì ì—ì„œ ì˜¤ì˜ˆì¸¡ì˜ ê°€ëŠ¥ì„±ì´ ìˆì„ ê²ƒìœ¼ë¡œ ì˜ˆìƒë©ë‹ˆë‹¤.\n\n\n\n\nì´ë¯¸ì§€ë¡œ í™•ì¸í•´ë³´ê¸°\n\n# validation\nwith open(\"data_preprocessing/dic_img2label_valid_sampling.pickle\",\"rb\") as fr:\n    dic_img2label_valid_sampling = pickle.load(fr)\n\n\ndef get_prediction():\n    model.eval()\n    path_valid_img = \"./datasets/validation/images\"\n    lst_img = os.listdir(path_valid_img)\n    lst_random_img = random.sample(lst_img, 9)\n\n    fig = plt.figure(figsize=(13, 13))\n    axes = fig.subplots(3, 3).flatten()\n    fig.suptitle(\"ì‹¤ì œ vs ì˜ˆì¸¡ ë¹„êµ\", fontsize=20)\n    \n    \n    for i, img in enumerate(lst_random_img): \n        \n        image = Image.open(path_valid_img+ \"/\" + img)\n        preprocessed_image = valid_transform(image) \n\n        pred_image = preprocessed_image.view(1, 3, 224, 224).to(device) \n\n\n        pred = model(pred_image).data.cuda()\n        _, disease_pred = torch.max(pred, dim=1)\n        disease_pred = int(disease_pred.cpu())\n\n\n        d1 = dic_img2label_valid_sampling[img]\n\n        crop, disease, points = d1['crop'], d1['disease'], d1['points']\n\n        draw = ImageDraw.Draw(image)\n        draw.rectangle([(points['xtl'], points['ytl']), (points['xbr'], points['ybr'])], outline=(255, 0, 255), width=30)\n\n        axes[i].imshow(np.array(image))\n        axes[i].set_title(f\"real: {disease2name_new[disease]}, pred: {disease2name_new[disease_pred]}\", fontsize=15)\n\n\nget_prediction()\n\n\n\n\nì´ë¯¸ì§€ë¡œ ëœë¤ ì‚¬ì§„ì„ ë½‘ì•„ ì§ì ‘ í™•ì¸í•´ë³´ì•˜ëŠ”ë°, ì•ì„œ ì–¸ê¸‰í•œ ì• í˜¸ë°• ì •ìƒê³¼ ê³ ì¶”ì •ìƒì´ ì½©ì •ìƒìœ¼ë¡œ ì˜ˆì¸¡ë˜ëŠ” ì˜ˆì‹œê°€ ëª¨ë‘ ë‚˜ì™€ì„œ í™•ì¸í•´ë³¼ ìˆ˜ ìˆì—ˆìŠµë‹ˆë‹¤. ë§ê²Œ ì˜ˆì¸¡í•œ ê³ ì¶” ì •ìƒì˜ ê²½ìš° ìì˜ ì‚¬ì§„ì€ ëª¨ë‘ ì˜ ë§í˜”ì§€ë§Œ, ì˜ ëª» ì˜ˆì¸¡í•œ ë‘ ê²½ìš°ëŠ” ëª¨ë‘ ì¤„ê¸° ì‚¬ì§„ì´ì—ˆê³ , ì½© ì •ìƒì˜ ëª¨ìŠµê³¼ ë¹„ìŠ·í•œ ëª¨ì–‘ìƒˆë¥¼ ë„ëŠ” ê²ƒìœ¼ë¡œ ë³´ì…ë‹ˆë‹¤."
  },
  {
    "objectID": "posts/c03.image_classification.html#ë°ì´í„°-ì¦ê°•-ë°-ë°ì´í„°-ë¶ˆê· í˜•-ì²˜ë¦¬",
    "href": "posts/c03.image_classification.html#ë°ì´í„°-ì¦ê°•-ë°-ë°ì´í„°-ë¶ˆê· í˜•-ì²˜ë¦¬",
    "title": "[cropdoctor] 3. ì‘ë¬¼ ì§ˆë³‘ ì§„ë‹¨ ëª¨ë¸ í•™ìŠµ ë° í‰ê°€ (mobilenetV2)",
    "section": "03. ë°ì´í„° ì¦ê°• ë° ë°ì´í„° ë¶ˆê· í˜• ì²˜ë¦¬",
    "text": "03. ë°ì´í„° ì¦ê°• ë° ë°ì´í„° ë¶ˆê· í˜• ì²˜ë¦¬\nEDAì—ì„œ ë¶ˆê· í˜•ê³¼ ìš©ëŸ‰ì„ ì¤„ì´ê¸° ìœ„í•´ ëœë¤ìƒ˜í”Œë§ìœ¼ë¡œ, ì•½ 8ë§Œì¥ì˜ ì´ë¯¸ì§€ ë°ì´í„°ë¥¼ ì•½ 15000ì¥ìœ¼ë¡œ ì¤„ì˜€ìŠµë‹ˆë‹¤. ë°ì´í„°ì˜ ìˆ˜ê°€ ê°ì†Œí•œ ê²ƒê³¼, ì•„ì§ ë‚¨ì•„ìˆëŠ” ë¶ˆê· í˜•ì„ ì²˜ë¦¬í•˜ê¸° ìœ„í•´ í•™ìŠµ ì‹œ ë°ì´í„° ì¦ê°•ê³¼, WCE ê¸°ë²•ì„ ì¶”ê°€í•´ë³´ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤.\nìœ„ì˜ mobilenetv2_1.pyì™€ ëª¨ë‘ ë™ì¼í•˜ë©°, ì•„ë˜ ë‘ ë¶€ë¶„ë§Œ ì¶”ê°€í•˜ì—¬ í•™ìŠµí•˜ì˜€ìŠµë‹ˆë‹¤.\n\n\në°ì´í„° ì¦ê°• ì¶”ê°€\n\ntrain_transform = transforms.Compose([\n    transforms.RandomRotation(degrees=30),\n    transforms.RandomResizedCrop(size=(224, 224)),\n    transforms.RandomHorizontalFlip(p=0.5),\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])\n\n\nval_transform = transforms.Compose([ # ì´ë¯¸ì§€ í¬ê¸°, ì •ê·œí™”ë§Œ ì§„í–‰ \n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])\n\ní•™ìŠµ ë°ì´í„°ì…‹ì—ë§Œ transformì„ ì ìš©í•˜ì—¬ ë°ì´í„°ë¥¼ ì¦ê°•í•©ë‹ˆë‹¤. ì´ ë•Œ, ì‘ë¬¼ ì´ë¯¸ì§€ëŠ” ìƒ‰ê¹”ì— ì˜í–¥ì„ í¬ê²Œ ë°›ìœ¼ë¯€ë¡œ, Colorjitterì™€ ê°™ì€ ìƒ‰ê¹” ë³€í˜•ì€ ì£¼ì§€ ì•Šë„ë¡ í•©ë‹ˆë‹¤.\n\n\n\nWCE ì¶”ê°€\n\nnSamples = [1000, 973, 915, 1000, 470, 227, 1000, 802, 458, 1000, 543, 484, 1000, 538, 854, 1000, 216, 1001, 208, 226]\nnormedWeights = [1 - (x / sum(nSamples)) for x in nSamples]\nnormedWeights = torch.FloatTensor(normedWeights).to(device)\n\ncriterion = nn.CrossEntropyLoss(weight=normedWeights)\n\nê° í´ë˜ìŠ¤ ë³„ ê°œìˆ˜ë¥¼ ë¦¬ìŠ¤íŠ¸ì— ì ì–´ì£¼ê³ , ê·¸ì— ë”°ë¥¸ ê°€ì¤‘ì¹˜ë¥¼ ì£¼ëŠ” WCE ì‘ì—…ì„ ì¶”ê°€í•´ì£¼ì—ˆìŠµë‹ˆë‹¤.\n\n\n\nëª¨ë¸ í‰ê°€ ê²°ê³¼\ní•˜ì´í¼ íŒŒë¼ë¯¸í„°ëŠ” ìœ„ì™€ ëª¨ë‘ ë™ì¼í–ˆê³ , ê°™ì€ ë°©ë²•ìœ¼ë¡œ ëª¨ë¸ì„ í‰ê°€í•œ ê²°ê³¼ì…ë‹ˆë‹¤.\n\ní‰ê°€ì§€í‘œ ê²°ê³¼\n\n\n\nimage.png\n\n\n\n\ní˜¼ë™í–‰ë ¬\n\n\n\nimage.png\n\n\n\n\nì´ë¯¸ì§€ í™•ì¸\n\n\n\nimage.png\n\n\nëª¨ë¸ í‰ê°€ì§€í‘œë‚˜ í˜¼ë™í–‰ë ¬ì„ ë³´ë©´ ë² ì´ìŠ¤ ëª¨ë¸ì˜ ì„±ëŠ¥ì´ ë” ì¢‹ì€ ê²°ê³¼ê°€ ë‚˜ì™”ìŠµë‹ˆë‹¤. ì™¸ë¶€ ë°ì´í„°ì…‹ì— ë” ê°•ê±´í•´ì§€ê²Œ ë§Œë“¤ê¸° ìœ„í•´ ë‘ë²ˆì§¸ ë°©ë²•ì˜ ì—í­ì„ ëŠ˜ë ¤ í•™ìŠµí•´ë³´ëŠ” ê²ƒë„ ì‹œë„í•´ë³´ê³  ì‹¶ìŠµë‹ˆë‹¤.\n\nì‚¬ì‹¤ í”„ë¡œì íŠ¸ ì‹œê°„ì€ ì œí•œë˜ì–´ìˆê³ , í•™ìŠµì€ í•œ ë²ˆì— 10ì‹œê°„ ì´ìƒì´ ì†Œìš”ë˜ì—ˆê¸° ë•Œë¬¸ì— epoch 10ìœ¼ë¡œ ë°–ì— í•˜ì§€ ëª» í–ˆì§€ë§Œ, í„± ì—†ì´ ë¶€ì¡±í•œ ìˆ˜ë¼ëŠ” ìƒê°ì´ ë“¤ì—ˆìŠµë‹ˆë‹¤. ì‹œê°„ì´ë‚˜ ìš©ëŸ‰ ë“± ë¬¼ë¦¬ì  ìì›ì˜ ì—¬ìœ ê°€ ëœë‹¤ë©´ epochì„ 50~100 ì •ë„ë¡œ ëŠ˜ë ¤ í•™ìŠµí•´ë³´ê³ ì‹¶ì—ˆìŠµë‹ˆë‹¤. ë˜í•œ epoch ë¿ë§Œ ì•„ë‹ˆë¼ ë‹¤ë¥¸ í•˜ì´í¼ íŒŒë¼ë¯¸í„° ì¡°ì •ì´ë‚˜, ë‹¤ë¥¸ ì‚¬ì „í•™ìŠµ ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ì‹¤í—˜í•´ë³´ì§€ ëª» í•œ ì ì´ ì•„ì‰¬ì›€ìœ¼ë¡œ ë‚¨ì•˜ìŠµë‹ˆë‹¤. ê·¸ë ‡ì§€ë§Œ ì´ì „ì— ìƒ˜í”Œë¡œ ì œê³µë°›ì€ aihub ëª¨ë¸ì˜ í‰ê°€ì§€í‘œì™€ ë¹„êµí•´ë´¤ì„ ë•Œ, ì„±ëŠ¥ì´ í›¨ì”¬ ê´œì°®ê²Œ ë‚˜ì™”ê³ , ì´ë¯¸ì§€ë¡œ ì§ì ‘ í™•ì¸í•´ ë³´ì•˜ì„ ë•Œë„ ì‘ë¬¼ì˜ ì§ˆë³‘ì„ ê½¤ ì˜ ë§íˆëŠ” ê²ƒìœ¼ë¡œ ë³´ì´ëŠ” ê²ƒìœ¼ë¡œ ë§Œì¡±í•˜ë ¤ê³  í•©ë‹ˆë‹¤. ë‹¤ìŒ ê¸€ì—ì„œëŠ” object detection ëª¨ë¸ë¡œ í•™ìŠµì„ ì§„í–‰í•œ í›„, ì–´ë–¤ ëª¨ë¸ì„ ì›¹ ì„œë¹„ìŠ¤ì— ì„œë¹™í•  ê²ƒì¸ì§€ ì„ íƒí•˜ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤."
  },
  {
    "objectID": "posts/c04.object_detection.html#ì‘ë¬¼-ì§ˆë³‘-ë°ì´í„°ì…‹ìœ¼ë¡œ-ì»¤ìŠ¤í…€-yolo-ëª¨ë¸-í•™ìŠµ",
    "href": "posts/c04.object_detection.html#ì‘ë¬¼-ì§ˆë³‘-ë°ì´í„°ì…‹ìœ¼ë¡œ-ì»¤ìŠ¤í…€-yolo-ëª¨ë¸-í•™ìŠµ",
    "title": "[cropdoctor] 4. ì‘ë¬¼ ì§ˆë³‘ ë°ì´í„°ì…‹ìœ¼ë¡œ ì»¤ìŠ¤í…€ YOLO ëª¨ë¸ í•™ìŠµ",
    "section": "4. ì‘ë¬¼ ì§ˆë³‘ ë°ì´í„°ì…‹ìœ¼ë¡œ ì»¤ìŠ¤í…€ YOLO ëª¨ë¸ í•™ìŠµ",
    "text": "4. ì‘ë¬¼ ì§ˆë³‘ ë°ì´í„°ì…‹ìœ¼ë¡œ ì»¤ìŠ¤í…€ YOLO ëª¨ë¸ í•™ìŠµ\n\nimport os\nimport zipfile\nimport random \n\nfrom PIL import Image, ImageDraw\nimport json\nimport pickle\nfrom tqdm import tqdm\nimport numpy as np \n\nimport matplotlib.pyplot as plt\nplt.rc(\"font\", family=\"NanumGothic\", size=13) \n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision.transforms as transforms\nimport torchvision.datasets as datasets\nfrom torch.utils.data import DataLoader\nfrom torch.utils.tensorboard import SummaryWriter\nfrom torch.utils.data.sampler import WeightedRandomSampler\nimport time\nfrom tqdm import tqdm\nimport torchvision.transforms as transforms\nfrom torchvision.datasets import ImageFolder\nimport os\nfrom PIL import ImageFile\nImageFile.LOAD_TRUNCATED_IMAGES = True\nfrom imblearn.over_sampling import SMOTE\nimport torchvision.datasets as datasets\nfrom torchvision.transforms import transforms\nimport numpy as np \n\n2023-05-10 13:15:09.260609: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n2023-05-10 13:15:09.314742: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\nTo enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2023-05-10 13:15:10.776130: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n\n\n \n\n00. ë°ì´í„°ì…‹ í´ë”êµ¬ì¡° ì„¤ê³„\nê¸°ì¡´ì˜ image classification pytorch ëª¨ë¸ì˜ í´ë”êµ¬ì¡°ëŠ” ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤. í´ë˜ìŠ¤ ë³„ í´ë”ë¥¼ ìƒì„±í•˜ê³ , ImageFolderì™€ DataLoader í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ë°ì´í„°ì…‹ì„ ë¶ˆëŸ¬ì™”ìŠµë‹ˆë‹¤.\n\nimage classifiaction (pytorch) : í´ë˜ìŠ¤ ë³„ í´ë” ìƒì„±\ndata\n  |_ training \n      |_ image_class \n          |_ ê³ ì¶”\n          |_ ê³ ì¶”ì§ˆë³‘1 \n          |_ ... \n\n\n  |_ vadlidation\n      |_ image_class \n          |_ ê³ ì¶”\n          |_ ê³ ì¶”ì§ˆë³‘1 \n          |_ ...         \n\nyolo : ì „ì²´ ì´ë¯¸ì§€ ëª¨ìœ¼ê³ , ë¼ë²¨ë°ì´í„°(ë¼ë²¨, ë°”ìš´ë”©ë°•ìŠ¤ ì •ë³´) ë”°ë¡œ ìƒì„±\ndatasets\n    |_ training \n        |_ images\n            |_ ì‚¬ì§„ì´ë¦„.jpg\n            |_ ...\n\n        |_ labels\n            |_ ì‚¬ì§„ì´ë¦„.txt\n            |_ ... \n\n\n    |_ validation \n        |_ images\n            |_ ì‚¬ì§„ì´ë¦„.jpg\n            |_ ...\n\n        |_ labels\n            |_ ì‚¬ì§„ì´ë¦„.txt\n            |_ ...         \ní•˜ì§€ë§Œ yoloëŠ” ìœ„ì™€ ê°™ì´ ë‹¤ë¥¸ í´ë”êµ¬ì¡°ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. images/label í´ë”ë¥¼ ê°ê° íƒ€ë¡œ ìƒì„±í•©ë‹ˆë‹¤. ì´ë¯¸ì§€ ë°ì´í„°ë¥¼ í´ë˜ìŠ¤ ë³„ë¡œ ëª¨ìœ¼ëŠ” ê²ƒì´ ì•„ë‹ˆë¼, ì „ì²´ ì´ë¯¸ì§€ ë°ì´í„°ë¥¼ í•œ í´ë”ì— ëª¨ìœ¼ê³ , label í´ë”ì—ëŠ” ì´ë¯¸ì§€ íŒŒì¼ê³¼ ë™ì¼í•œ ì´ë¦„ì˜ txt íŒŒì¼ì„ ìƒì„±í•˜ê³ , txt íŒŒì¼ ì•ˆì— label ì •ë³´ë¥¼ ë‹´ìŠµë‹ˆë‹¤.\n\nğŸ“• reference\nyolo í•™ìŠµ ~ ì˜ˆì¸¡ ì°¸ê³  ë§í¬\n \n\në°ì´í„°ì…‹ í´ë” ê²½ë¡œ ì„¤ì •\n\n# ê²½ë¡œ ì„¤ì •  \npath_train_img = \"datasets/training/images/\"\npath_train_label = \"datasets/training/labels/\"\n\npath_valid_img = \"datasets/validation/images/\"\npath_valid_label = \"datasets/validation/labels/\"\n\nìœ„ì—ì„œ ì„¤ê³„í•œ ëŒ€ë¡œ í´ë”ë¥¼ ìƒì„±í•˜ê³ , ê·¸ì— ë§ê²Œ ê²½ë¡œë¥¼ ì„¤ì •í•´ì¤ë‹ˆë‹¤.\n\n \n\n\n\n01. í˜„ì¬ annotation ë°ì´í„°ì˜ bboxë¥¼ yolo bbox í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•˜ê¸°\nyolo bbox í˜•ì‹ ì°¸ê³  ë§í¬\n\n\në ˆì´ë¸” ì •ë³´ ë”•ì…”ë„ˆë¦¬ ë¶ˆëŸ¬ì˜¤ê¸°\n\n# train\nwith open(\"data_preprocessing/dic_img2label_train_sampling.pickle\",\"rb\") as fr:\n    dic_img2label_train_sampling = pickle.load(fr)\n    \n# validation\nwith open(\"data_preprocessing/dic_img2label_valid_sampling.pickle\",\"rb\") as fr:\n    dic_img2label_valid_sampling = pickle.load(fr)\n\nyolo í•™ìŠµì„ ìœ„í•´ ê¸°ì¡´ì˜ bboxë¥¼ yolo bbox í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•´ì•¼ í•©ë‹ˆë‹¤. ì´ë¥¼ ìœ„í•´ EDAì—ì„œ ìƒì„±í•´ ë†“ì€ ë ˆì´ë¸” ì •ë³´ê°€ ë‹´ê²¨ì ¸ ìˆëŠ” ë”•ì…”ë„ˆë¦¬ë¥¼ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.\n\n\n\nbbox ë³€í™˜ í•¨ìˆ˜\n\n# í˜„ì¬ bbox: (x_min, y_min, x_max, y_max) ì¢Œìƒë‹¨, ìš°í•˜ë‹¨ ê¼­ì§“ì  ì¢Œí‘œ \n# yolo bbox: (x_center, y_center, width, height)\ndef bbox_to_yolo_bbox(bbox, w, h): \n    # xmin, ymin, xmax, ymax\n    if w < 10: \n        w, h = 3024, 3024 \n        \n    x_center = ((bbox['xbr'] + bbox['xtl']) / 2) / w\n    y_center = ((bbox['ybr'] + bbox['ytl']) / 2) / h\n    width = (bbox['xbr'] - bbox['xtl']) / w\n    height = (bbox['ybr'] - bbox['ytl']) / h\n    return [x_center, y_center, width, height]\n\n\n\n\ntrain datasets\n\nfor img, values in tqdm(dic_img2label_train_sampling.items()):\n    filename = os.path.splitext(img)[0]\n    \n    yolo_bbox = bbox_to_yolo_bbox(values[\"points\"], values[\"size\"][0], values[\"size\"][1])\n    bbox_str = \" \".join([str(b) for b in yolo_bbox])\n    label = values[\"disease\"]\n    result = f\"{label} {bbox_str}\"\n       \n    if result: \n        with open(os.path.join(path_train_label, f\"{filename}.txt\"), \"w\", encoding='utf-8') as f: \n            f.write(result)\n\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13915/13915 [00:01<00:00, 12558.49it/s]\n\n\n\n\n\nvalidation datasets\n\nfor img, values in tqdm(dic_img2label_valid_sampling.items()):\n    filename = os.path.splitext(img)[0]\n    \n    yolo_bbox = bbox_to_yolo_bbox(values[\"points\"], values[\"size\"][0], values[\"size\"][1])\n    bbox_str = \" \".join([str(b) for b in yolo_bbox])\n    label = values[\"disease\"]\n    result = f\"{label} {bbox_str}\"\n       \n    if result: \n        with open(os.path.join(path_valid_label, f\"{filename}.txt\"), \"w\", encoding='utf-8') as f: \n            f.write(result)\n\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7873/7873 [00:00<00:00, 15471.80it/s]\n\n\ntrain, validation ë°ì´í„°ì…‹ ëª¨ë‘ yolo bboxì— ë§ê²Œ ë³€í™˜ì„ í•´ì£¼ì—ˆê³ , datasets/training/labels, datasets/validation/labels í´ë”ì— ì´ë¯¸ì§€ ë³„ txt íŒŒì¼ë¡œ ìƒì„±í•´ì£¼ì—ˆìŠµë‹ˆë‹¤. ì´ ë•Œ txt íŒŒì¼ì˜ ë‚´ìš©ì€ label bboxì •ë³´ ìˆœì„œë¡œ ì‘ì„±í•´ì¤ë‹ˆë‹¤.\n\n\n \n\n\n\n02. ë°ì´í„°ì…‹ ê²½ë¡œê°€ ì íŒ txt íŒŒì¼ ìƒì„±í•˜ê¸°\n\n# ê²½ë¡œì— ë“¤ì–´ìˆëŠ” íŒŒì¼ ë¦¬ìŠ¤íŠ¸\nlst_train_img = os.listdir(path_train_img)\nlst_valid_img = os.listdir(path_valid_img)\n\n\nlst_train_data = [\"/home/elicer/\" + path_train_img + img for img in lst_train_img]\nlst_valid_data = [\"/home/elicer/\" + path_valid_img + img for img in lst_valid_img]\n\n\n# train.txt\nwith open(\"train.txt\", 'w') as f:\n    f.write('\\n'.join(lst_train_data) + '\\n')\n\n# valid.txt\nwith open(\"valid.txt\", 'w') as f:\n    f.write('\\n'.join(lst_valid_data) + '\\n')\n\nì „ì²´ ì´ë¯¸ì§€ íŒŒì¼ ê²½ë¡œê°€ ë‹´ê¸´ txt íŒŒì¼ë„ ìƒì„±í•©ë‹ˆë‹¤.\n\n\n \n\n\n03. yaml íŒŒì¼ ìƒì„±í•˜ê¸°\n\nimport yaml\n\n\nyaml_data = {\n    \"names\": [\"Pepper\", \"Pepper anthrax\", \"Pepper white powder bottle\",\n\"Radish\", \"Radish Black-and-white disease\", \"Radish Bacteria-free disease\",\n\"Cabbage\", \"Cabbage black rot disease\", \"Cabbage roe disease\",\n\"Zucchini\", \"Zucchini nosocomial disease\", \"Zucchini white powder disease\",\n\"Bean\", \"Bean fire disease\", \"Bean dot disease\",\n\"Tomato\", \"Tomato leaf blight\",\n\"Pumpkin\", \"Pumpkin roe disease\", \"Pumpkin white powder disease\"],\n    \"nc\":20, \n    \"path\": \"/\",\n    \"train\": \"./datasets/train.txt\",\n    \"val\": \"./datasets/valid.txt\",\n}\n\nwith open(\"custom.yaml\", \"w\") as f: \n    yaml.dump(yaml_data, f)\n\nnameì— í´ë˜ìŠ¤ì˜ ì´ë¦„ì„ ì˜ë¬¸ìœ¼ë¡œ ì ì–´ ë¦¬ìŠ¤íŠ¸ë¡œ ë„£ì–´ì£¼ê³ , ncì— í´ë˜ìŠ¤ì˜ ê°œìˆ˜ë¥¼ ë„£ì–´ì¤ë‹ˆë‹¤. ê·¸ ë‹¤ìŒ train, valì— ì•ì„œ ìƒì„±í•œ txt íŒŒì¼ì˜ ê²½ë¡œë¥¼ ë„£ì–´ì£¼ê³  yamlíŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤.\n\n\n \n\n\n04. train ì§„í–‰í•˜ê¸°\n\ní•™ìŠµì„ ìœ„í•´ ìƒì„±í•œ íŒŒì¼ ì •ë¦¬\n1) images í´ë” ì•ˆì— ì „ì²´ ì´ë¯¸ì§€ íŒŒì¼\n2) labels í´ë” ì•ˆì— ì´ë¯¸ì§€ íŒŒì¼ì— ëŒ€ì‘ë˜ëŠ” ë ˆì´ë¸” ì •ë³´ txt íŒŒì¼\n3) ì „ì²´ ì´ë¯¸ì§€ íŒŒì¼ì˜ ê²½ë¡œê°€ ëª¨ë‘ ì íŒ txt íŒŒì¼\n4) í´ë˜ìŠ¤ ì´ë¦„, ê°œìˆ˜, 3ë²ˆì—ì„œ ìƒì„±í•œ txt íŒŒì¼ ê²½ë¡œë¥¼ í¬í•¨í•œ yaml íŒŒì¼\n\nì•„ë˜ì˜ ëª…ë ¹ì–´ë¥¼ í„°ë¯¸ë„ ì—ì„œ ì‹¤í–‰\n\ngit clone https://github.com/ultralytics/yolov5.git\ncd yolov5\npip install -qr requirements.txt\n\n!python train.py --batch 64 --epochs 20 --data ../custom.yaml --device 0 --weights yolov5s.pt --name test\nì œê°€ ìˆ˜í–‰í•œ í•™ìŠµ í™˜ê²½ì—ì„œëŠ” 1ì—í­ ë‹¹ 1ì‹œê°„ 30ë¶„ ì •ë„ ê±¸ë ¸ìŠµë‹ˆë‹¤. ë”°ë¼ì„œ ë§ˆì°¬ê°€ì§€ë¡œ tmuxë¥¼ ì‚¬ìš©í•˜ì—¬ í•™ìŠµì„ ì§„í–‰í•˜ì˜€ìŠµë‹ˆë‹¤.\nyolo í•™ìŠµì„ ìˆ˜í–‰í•˜ë©´ ìë™ìœ¼ë¡œ ëª¨ë¸ í‰ê°€ ê²°ê³¼ë¥¼ ëª‡ê°€ì§€ ì œê³µí•´ì¤ë‹ˆë‹¤.\n\n\nì‹¤ì œ vs ì˜ˆì¸¡ Image ë¹„êµ\n\nlabel\n\n\n\n\nimage.png\n\n\n\npred\n\n\n\n\nimage.png\n\n\nìœ„ì˜ ë„ì¶œëœ ê²°ê³¼ ì‚¬ì§„ì„ ë³´ë©´, predì˜ bboxê°€ ì˜¤íˆë ¤ ìì„ ë” ì˜ ì˜ˆì¸¡í•œ ê²ƒìœ¼ë¡œ ë³´ì…ë‹ˆë‹¤.\n \n\n\nPrecision-Confidence Curve / Precision-Recall Curve / Recall-Confidence Curve í™•ì¸\n  \n \n\n\ntensorboard í‰ê°€ì§€í‘œ í™•ì¸\n\n\n\nimage.png\n\n\n\n\n\nimage.png\n\n\n\n\n\nimage.png\n\n\n20 ì—í­ ë™ì•ˆ ì´ìƒì ìœ¼ë¡œ í‰ê°€ì§€í‘œëŠ” ì˜¬ë¼ê°€ê³  lossê°’ì€ ì¤„ì–´ë“¤ì§€ë§Œ, validationì˜ obj_lossê°’ì€ ì¦ê°€í•˜ëŠ” ì¶”ì´ë¥¼ ë³´ì…ë‹ˆë‹¤. ìœ„ì˜ 3ê°€ì§€ì˜ curve ê·¸ë˜í”„ë‚˜, ì´ tensorboardë¡œ ë¯¸ë£¨ì–´ ë³´ì•„, 20 ì—í­ìœ¼ë¡œëŠ” í•™ìŠµì´ ëœ ëœ ê²ƒ ê°™ë‹¤ëŠ” ìƒê°ì´ ë“­ë‹ˆë‹¤. epochì„ 100 ì´ìƒìœ¼ë¡œ ì„¤ì •í•˜ê³  ëŒë¦¬ë©´ ë” ê´œì°®ì€ ê²°ê³¼ë¥¼ ë³´ì¼ ìˆ˜ ìˆì„ ê²ƒì´ë¼ ì˜ˆìƒë©ë‹ˆë‹¤.\n\nì´ë ‡ê²Œ ì¸ê³µì§€ëŠ¥ ê¸°ë°˜ì˜ ì›¹ì„œë¹„ìŠ¤ ê°œë°œ CropDoctor í”„ë¡œì íŠ¸ì—ì„œ ëª¨ë¸ í•™ìŠµ ë¶€ë¶„ì„ ë§ˆì³¤ìŠµë‹ˆë‹¤. ì‹œê°„ì˜ ì—¬ìœ ê°€ ìˆì—ˆë‹¤ë©´, ë” ë§ì€ ëª¨ë¸ í•™ìŠµê³¼ ì‹¤í—˜ì„ í†µí•´ ê²°ì •í•˜ê³  ì‹¶ì€ ë§ˆìŒì´ ìˆì—ˆì§€ë§Œ, image classificationê³¼ object detection ëª¨ë¸ í•™ìŠµì„ ë¹„êµ í•™ìŠµí•´ë´¤ë‹¤ëŠ” ì ì— ì˜ì˜ë¥¼ ë‘ë ¤ê³  í•©ë‹ˆë‹¤. ê²°ë¡ ì ìœ¼ë¡œ ì›¹ ì„œë¹„ìŠ¤ì—ì„œëŠ” ì„±ëŠ¥ë„ ì¢‹ê³ , ì•„í‚¤í…ì²˜ê°€ ê°€ë²¼ìš´ Image Classificationì˜ mobilenetV2 ëª¨ë¸ë¡œ ì„œë¹™í•˜ê¸°ë¡œ ê²°ì •í•˜ì˜€ìŠµë‹ˆë‹¤."
  },
  {
    "objectID": "posts/s00.demo.html",
    "href": "posts/s00.demo.html",
    "title": "[stockait] stockait ì‚¬ìš©í•˜ì—¬ ì£¼ê°€ ì˜ˆì¸¡ ëª¨ë¸ í•™ìŠµí•˜ê¸°",
    "section": "",
    "text": "ë³¸ ê¸€ì—ì„œëŠ” ì§ì ‘ ê°œë°œí•œ íŒŒì´ì¬ ë¼ì´ë¸ŒëŸ¬ë¦¬ stockait ì‚¬ìš©ë²•ì„ ì •ë¦¬í•©ë‹ˆë‹¤. stockaitëŠ” ì£¼ê°€ ë¹…ë°ì´í„° ì—°êµ¬ë¥¼ ìœ„í•œ í†µí•© ë¼ì´ë¸ŒëŸ¬ë¦¬ë¡œ, ë°ì´í„° ìˆ˜ì§‘ë¶€í„° ë°ì´í„° ì „ì²˜ë¦¬, ëª¨ë¸ í•™ìŠµ ëª¨ë¸ í‰ê°€, ìˆ˜ìµë¥  ê³„ì‚°ê¹Œì§€ ëª¨ë“  ê³¼ì •ì„ ì´ ë¼ì´ë¸ŒëŸ¬ë¦¬ì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\në¼ì´ë¸ŒëŸ¬ë¦¬ì˜ ì „ì²´ì ì¸ íë¦„ ë° ì‚¬ìš©ë²•ì„ ë‹¤ìŒì˜ ê³¼ì •ìœ¼ë¡œ ì •ë¦¬í•˜ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤.\në°ì´í„° ìˆ˜ì§‘ - ë°ì´í„° ì „ì²˜ë¦¬ - íŠ¸ë ˆì´ë” ì •ì˜ - íŠ¸ë ˆì´ë” ì‚¬ìš©(ëª¨ë¸ í•™ìŠµ & í‰ê°€) - ìˆ˜ìµë¥  ì‹œë®¬ë ˆì´ì…˜\n\nimport pandas as pd\nimport os\nimport sys\nimport stockait as sai"
  },
  {
    "objectID": "posts/s01.trader_definition.html#íŠ¸ë ˆì´ë”-ì •ì˜í•˜ê¸°",
    "href": "posts/s01.trader_definition.html#íŠ¸ë ˆì´ë”-ì •ì˜í•˜ê¸°",
    "title": "[stockait] stockaitì—ì„œ traderì˜ ê°œë…ê³¼ ì‚¬ìš©ë²•",
    "section": "íŠ¸ë ˆì´ë” ì •ì˜í•˜ê¸°",
    "text": "íŠ¸ë ˆì´ë” ì •ì˜í•˜ê¸°\nstockaitëŠ” ëª¨ë¸í•™ìŠµ ê³¼ì •ì—ì„œ íŠ¸ë ˆì´ë”ë¼ëŠ” ê°œë…ì´ ë‚˜ì˜µë‹ˆë‹¤. ê²Œì„ ìºë¦­í„°ê°€ ì¥ë¹„ë¥¼ ì¥ì°©í•˜ëŠ” ê²ƒ ì²˜ëŸ¼ í•˜ë‚˜ì˜ íŠ¸ë ˆì´ë” ì•ˆì— ëª¨ë¸í•™ìŠµì— í•„ìš”í•œ ë°ì´í„°ì…‹ì„ ì €ì¥í•˜ê³ , ëª¨ë¸ì˜ ì •ë³´, ì£¼ì‹ ë§¤ë§¤ì™€ ê´€ë ¨ëœ ì •ë³´ë¥¼ ì €ì¥í•´ë†“ìŠµë‹ˆë‹¤. ê·¸ í›„ì— ìºë¦­í„°ê°€ ë™ì‘ì„ ìˆ˜í–‰í•˜ëŠ” ê²ƒê³¼ ê°™ì´ íŠ¸ë ˆì´ë”ë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ í•™ìŠµì„ ì§„í–‰í•˜ê³ , ì£¼ì‹ ë§¤ë§¤ ì‹œë®¬ë ˆì´ì…˜ê¹Œì§€ ìˆ˜í–‰í•´ë³¼ ìˆ˜ ìˆëŠ” ê°œë…ì…ë‹ˆë‹¤.\në‹¤ìŒì€ íŠ¸ë ˆì´ë” ê¸°ëŠ¥ì— ëŒ€í•œ ì„¤ëª…ì…ë‹ˆë‹¤.\n \n\níŠ¸ë ˆì´ë”ì˜ ê¸°ëŠ¥\níŠ¸ë ˆì´ë”ëŠ” ë‹¤ìŒì˜ ì„¸ê°€ì§€ ê¸°ëŠ¥ì„ ê°–ê³ ìˆìŠµë‹ˆë‹¤.\n\n\n\nimage.png\n\n\në§¤ìˆ˜ ë§¤ë„ ì •ë³´ë¥¼ í¬í•¨í•œ íŠ¸ë ˆì´ë” ê°ì²´ë¥¼ ì •ì˜í•˜ë©°, ë°ì´í„°ì…‹ì„ ì €ì¥í•©ë‹ˆë‹¤. ì¶”ê°€ì ìœ¼ë¡œ, íŠ¸ë ˆì´ë”ì˜ ì´ë¦„ê³¼ ë ˆì´ë¸” ì •ë³´ê°€ ì €ì¥ë©ë‹ˆë‹¤.\n \n\n\nì²«ì§¸, íŠ¸ë ˆì´ë” ê°ì²´ ì •ì˜\n\n\n\nimage.png\n\n\n\ntrader.buyer: buyer ê°ì²´ ì•ˆì— ë‘ê°œì˜ í•˜ìœ„ ë§¤ìˆ˜ ê°ì²´ë¥¼ ë¦¬ìŠ¤íŠ¸ í˜•íƒœë¡œ í¬í•¨ë©ë‹ˆë‹¤. ([conditional_buyer, machine learning_buyer])\ntrader.seller: sellerê°ì²´ ì•ˆì— í•˜ìœ„ ë§¤ë„ ê°ì²´ë¥¼ í¬í•¨í•©ë‹ˆë‹¤. (Subseller).\n\n\n\n1. conditional_buyer\në°ì´í„°ì…‹ í•„í„°ë§ ì¡°ê±´ìœ¼ë¡œ ë§¤ìˆ˜ ê²°ì •ì„ í•˜ëŠ” ê°ì²´ì…ë‹ˆë‹¤.\n\nConditional_buyer.condition: Add a dataset filtering condition to the condition method. (For example, only data with a transaction price (end price x transaction volume) of more than 1 billion won.) condition ë©”ì„œë“œì— ë°ì´í„°ì…‹ í•„í„°ë§ ì¡°ê±´ì„ í•¨ìˆ˜ë¡œ ë„£ì–´ì¤ë‹ˆë‹¤. (ì˜ˆë¥¼ ë“¤ì–´, ê±°ë˜ëŒ€ê¸ˆ (ì¢…ê°€xê±°ë˜ëŸ‰)ì´ 1ì–µì› ì´ìƒì¸ ë°ì´í„°ë“¤)\n\n\n\n\n2. machinelearning_buyer\në¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ë¡œ ë§¤ìˆ˜ ê²°ì •ì„ í•˜ëŠ” ê°ì²´ì…ë‹ˆë‹¤.\n\nmachinelearning_buyer.algorithm: algorithm ë©”ì„œë“œì— ìœ ì €ê°€ ì •ì˜í•œ ëª¨ë¸ì„ ë„£ì–´ì¤ë‹ˆë‹¤. sklearn íŒ¨í‚¤ì§€ë‚˜ pytorch, tensorflow ë“±ìœ¼ë¡œ ì •ì˜í•œ ë”¥ëŸ¬ë‹ ëª¨ë¸ ë“± ê³¼ ê°™ì´ ëª¨ë“  ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ì„ ì¶”ê°€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\n\n\n\n3. SubSeller\nëª¨ë“  ë‚ ì§œì— ëŒ€í•˜ì—¬ ë§¤ë„ë¥¼ ê²°ì •í•˜ëŠ” ê°ì²´ì…ë‹ˆë‹¤.\n\n \n\n\n\në‘˜ì§¸, ë°ì´í„°ì…‹ ì €ì¥\n\n\n\nimage.png\n\n\níŠ¸ë ˆì´ë” ê°ì²´ ì•ˆì— ë°ì´í„°ì…‹ì„ ì €ì¥í•©ë‹ˆë‹¤. train data, test dataë¥¼ ë„£ì–´ì£¼ê³ , ë§Œì•½ í‘œì¤€í™”ë¥¼ ì§„í–‰í–ˆë‹¤ë©´ train data scaled, test data scaled ê¹Œì§€ ë„£ì–´ì¤ë‹ˆë‹¤. ê·¸ëŸ¼ í•™ìŠµì— í•„ìš”í•œ ë°ì´í„°ì…‹ë“¤ì„ ìƒì„±í•˜ì—¬ ê°ì²´ì— ìœ„ì™€ ê°™ì´ ì €ì¥ë©ë‹ˆë‹¤.\n \n\n\nì…‹ì§¸, íŠ¸ë ˆì´ë”ì˜ ì •ë³´ ì €ì¥\n\n\n\nimage.png\n\n\n\nTrader.name: íŠ¸ë ˆì´ë”ë¥¼ êµ¬ë³„í•˜ëŠ” ì´ë¦„ì…ë‹ˆë‹¤.\nTrader.label: ì¢…ì† ë³€ìˆ˜ì˜ íƒ€ì…ì„ ì €ì¥í•©ë‹ˆë‹¤. (regression: reg, classification: class&0.02)\n\nstockaitì—ì„œ ì§€ì •í•œ default ì¢…ì†ë³€ìˆ˜ëŠ” ë‹¤ìŒ ë‚  ì¢…ê°€ ë³€í™”ìœ¨ (next_change)ì…ë‹ˆë‹¤.\në”°ë¼ì„œ regresionìœ¼ë¡œ ì„¤ì •í•˜ë©´ ì¢…ì†ë³€ìˆ˜ëŠ” next_change,\nclass&0.02 ì™€ ê°™ì´ ì„¤ì •í•˜ë©´ next_change, ì¦‰ ë‹¤ìŒ ë‚  ì¢…ê°€ ë³€í™”ìœ¨ì´ 0.02 ì´ìƒì´ë©´ 1, ê·¸ë ‡ì§€ ì•Šìœ¼ë©´ 0 ìœ¼ë¡œ ì´ì§„ë¶„ë¥˜ í•´ì¤ë‹ˆë‹¤. (ë‹¤ìŒë‚ ì˜ ì¢…ê°€ê°€ 0.02 ì´ìƒ ì˜¬ëëŠ”ì§€ ì•„ë‹Œì§€)\n\n\n \n\nê·¸ ë‹¤ìŒìœ¼ë¡œ, íŠ¸ë ˆì´ë”ë¥¼ ì •ì˜í•˜ëŠ” ì˜ˆì‹œë¥¼ ë³´ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤.\níŠ¸ë ˆì´ë”ë¥¼ ì •ì˜í•  ë•Œ ë‹¤ìŒì˜ ê³¼ì •ì„ ë”°ë¼ ì§„í–‰í•˜ë„ë¡ í•©ë‹ˆë‹¤.\n\nì²˜ìŒìœ¼ë¡œ, íŠ¸ë ˆì´ë”ë¥¼ ë‹´ì„ ë¹ˆ ë¦¬ìŠ¤íŠ¸ë¥¼ í•˜ë‚˜ ì„ ì–¸í•©ë‹ˆë‹¤.\n\nlst_trader = [] \n\në‹¤ìŒì€ ì´ë¯¸ ìƒì„±ë˜ì–´ìˆëŠ” ë¨¸ì‹ ëŸ¬ë‹ íŒ¨í‚¤ì§€ë¡œ íŠ¸ë ˆì´ë”ë¥¼ ì •ì˜í•˜ëŠ” ì˜ˆì‹œì…ë‹ˆë‹¤.\n\n\n\nex1) LightGBM\n\nfrom lightgbm import LGBMClassifier\n\n# conditional_buyer: Object that determines acquisition based on data filtering conditions \nb1_lg = sai.ConditionalBuyer()\n\ndef sampling1(df): # Create a conditional function\n    condition1 = (-0.3 <= df.D0_Change) & (df.D0_Change <= 0.3) # Remove exceptions that exceed upper and lower limits\n    condition2 = df.D0_trading_value >= 1000000000 # condition 1: Transaction amount of more than 1 billion won \n    condition3 = (-0.05 >= df.D0_Change) | (0.05 <= df.D0_Change) # condition 2: Today's stock price change rate is more than 5%\n    condition = condition1 & condition2 & condition3\n    return condition\n\nb1_lg.condition = sampling1  # Define the condition function directly (sampling1) and store it in the condition property \n\n\n# machinelearning_buyer: Object that determines acquisition by machine learning model\nb2_lg = sai.MachinelearningBuyer()\n\n# Save user-defined models to algorithm properties\nscale_pos_weight = round(72/28 , 2)\nparams = {  'random_state' : 42,\n            'scale_pos_weight' : scale_pos_weight,\n            'learning_rate' : 0.1, \n            'num_iterations' : 1000,\n            'max_depth' : 4,\n            'n_jobs' : 30,\n            'boost_from_average' : False,\n            'objective' : 'binary' }\n\nb2_lg.algorithm =  LGBMClassifier( **params )\n\n\n# SubSeller: Object that determines selling all of the following days\nsell_all = sai.SubSeller() \n\n\n# Trader Object   \nt1 = sai.Trader()\nt1.name = 'saiLightGBM' # Trader's name\nt1.label = 'class&0.02' # Set the Trader dependent variable (do not set if it is regression analysis) \nt1.buyer = sai.Buyer([b1_lg, b2_lg]) # [ conditional buyer, machinelearning buyer ] \nt1.seller = sai.Seller(sell_all)\n\nlst_trader.append(t1)\n\n\n\n\nex2) XGBoost\n\nfrom xgboost import XGBClassifier\n\nb1_xgb = sai.ConditionalBuyer() \n\ndef sampling2(df): \n    condition1 = (-0.3 <= df.D0_Change) & (df.D0_Change <= 0.3) \n    condition2 = df.D0_trading_value >= 1000000000 \n    condition3 = (-0.05 >= df.D0_Change) | (0.05 <= df.D0_Change) \n    condition = condition1 & condition2 & condition3\n    return condition\n\nb1_xgb.condition = sampling2\n\n\nb2_xgb = sai.MachinelearningBuyer()  \n\nscale_pos_weight = round(72/28 , 2)\nb2_xgb.algorithm = XGBClassifier(random_state = 42,\n                   n_jobs=30,\n                   scale_pos_weight=scale_pos_weight,\n                   learning_rate=0.1,\n                   max_depth=4,\n                   n_estimators=1000,\n                   )  \n\nsell_all = sai.SubSeller()\n\n\nt2 = sai.Trader()\nt2.name = 'saiXGboost' \nt2.label = 'class&0.02' \nt2.buyer = sai.Buyer([b1_xgb, b2_xgb])\nt2.seller = sai.Seller(sell_all) \n\nlst_trader.append(t2) \n\n\n\n\nex3) LogisticRegression\n\nfrom sklearn.linear_model import LogisticRegression\n\nb1_lr = sai.ConditionalBuyer()\n\ndef sampling3(df):  \n    condition1 = (-0.3 <= df.D0_Change) & (df.D0_Change <= 0.3) \n    condition2 = df.D0_trading_value >= 1000000000 \n    condition3 = (-0.05 >= df.D0_Change) | (0.05 <= df.D0_Change) \n    condition = condition1 & condition2 & condition3\n    return condition\n\nb1_lr.condition = sampling3\n\n\nb2_lr = sai.MachinelearningBuyer()  \n\nb2_lr.algorithm = LogisticRegression()\n\n\nsell_all = sai.SubSeller() \n\n\nt3 = sai.Trader()\nt3.name = 'saiLogisticRegression'  \nt3.label = 'class&0.02' \nt3.buyer = sai.Buyer([b1_lr, b2_lr]) \nt3.seller = sai.Seller(sell_all)\n\nlst_trader.append(t3) \n\n\n\n\nex4) Support Vector Machine\n\nfrom sklearn.svm import SVC\n\nb1_sv = sai.ConditionalBuyer()\n\ndef sampling4(df):  \n    condition1 = (-0.3 <= df.D0_Change) & (df.D0_Change <= 0.3) \n    condition2 = df.D0_trading_value >= 1000000000 \n    condition3 = (-0.05 >= df.D0_Change) | (0.05 <= df.D0_Change) \n    condition = condition1 & condition2 & condition3\n    return condition\n\nb1_sv.condition = sampling4 \n\n\nb2_sv = sai.MachinelearningBuyer()  \n\nb2_sv.algorithm = SVC() \n\n\nsell_all = sai.SubSeller() \n\n\nt4 = sai.Trader()\nt4.name = 'saiSupportVectorMachine'  \nt4.label = 'class&0.02' \nt4.buyer = sai.Buyer([b1_sv, b2_sv]) \nt4.seller = sai.Seller(sell_all)\n\nlst_trader.append(t4) \n\n\n\n\nex5) Decision Tree\n\nfrom sklearn.tree import DecisionTreeClassifier\n\nb1_dt = sai.ConditionalBuyer()\n\ndef sampling5(df):  \n    condition1 = (-0.3 <= df.D0_Change) & (df.D0_Change <= 0.3) \n    condition2 = df.D0_trading_value >= 1000000000 \n    condition3 = (-0.05 >= df.D0_Change) | (0.05 <= df.D0_Change) \n    condition = condition1 & condition2 & condition3\n    return condition\n\nb1_dt.condition = sampling5 \n\n\nb2_dt = sai.MachinelearningBuyer()  \n\nb2_dt.algorithm = DecisionTreeClassifier() \n\n\nsell_all = sai.SubSeller() \n\n\nt5 = sai.Trader()\nt5.name = 'saiDecisionTree'  \nt5.label = 'class&0.02' \nt5.buyer = sai.Buyer([b1_dt, b2_dt]) \nt5.seller = sai.Seller(sell_all)\n\nlst_trader.append(t5) \n\n\n\n\nex6) RandomForest\n\nfrom sklearn.ensemble import RandomForestClassifier\n\nb1_rf = sai.ConditionalBuyer()\n\ndef sampling6(df):  \n    condition1 = (-0.3 <= df.D0_Change) & (df.D0_Change <= 0.3) \n    condition2 = df.D0_trading_value >= 1000000000 \n    condition3 = (-0.05 >= df.D0_Change) | (0.05 <= df.D0_Change) \n    condition = condition1 & condition2 & condition3\n    return condition\n\nb1_rf.condition = sampling6 \n\n\nb2_rf = sai.MachinelearningBuyer()  \n\nb2_rf.algorithm = RandomForestClassifier() \n\n\nsell_all = sai.SubSeller() \n\n\nt6 = sai.Trader()\nt6.name = 'saiDecisionTree'  \nt6.label = 'class&0.02' \nt6.buyer = sai.Buyer([b1_rf, b2_rf]) \nt6.seller = sai.Seller(sell_all)\n\nlst_trader.append(t6) \n\n \n\në‹¤ìŒì€ kerasë¥¼ ì‚¬ìš©í•˜ì—¬ LSTM ëª¨ë¸ì„ ì •ì˜í•˜ëŠ” ë”¥ëŸ¬ë‹ ëª¨ë¸ì˜ ì˜ˆì‹œì…ë‹ˆë‹¤. ìœ„ì˜ ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ê³¼ ë‹¤ë¥¸ ì ì€, data_transformì´ë¼ëŠ” ì†ì„±ì„ í•˜ë‚˜ ë” ì¶”ê°€í•´ì„œ ì •ì˜í•´ì£¼ì–´ì•¼ í•©ë‹ˆë‹¤.\n\n\nex7) LSTM\nâ­ ë”¥ëŸ¬ë‹ ëª¨ë¸ì„ ì‚¬ìš©í•˜ê¸° ìœ„í•´ì„œëŠ” 2ì°¨ì› êµ¬ì¡°ë¡œ ë°ì´í„°ì…‹ ë³€í™˜ì„ ìˆ˜í–‰í•˜ëŠ” transform í•¨ìˆ˜ë¥¼ ì •ì˜í•˜ê³  data_transform ì†ì„±ì— ë„£ì–´ì¤ë‹ˆë‹¤. ì•„ë˜ ì˜ˆì‹œì—ì„œëŠ” 1x480 ë°ì´í„°ì…‹ì„ 10x48ë¡œ ë³€í™˜ì‹œì¼œì£¼ê³ , ëª¨ë¸ êµ¬ì¡°ë¥¼ ì •ì˜í•  ë•Œ input_shapeì„ ê·¸ì— ë§ê²Œ (10, 48)ë¡œ ì§€ì •í•´ì£¼ì—ˆìŠµë‹ˆë‹¤. ì‚¬ìš©í•˜ëŠ” ë°ì´í„°ì…‹ì˜ êµ¬ì¡°ë¥¼ ì œëŒ€ë¡œ íŒŒì•…í•˜ê³ , ë³€í™˜ í›„ì˜ input shapeì„ ì œëŒ€ë¡œ ì„¤ì •í•´ì£¼ëŠ” ê²ƒì´ ì¤‘ìš”í•©ë‹ˆë‹¤.\n\nfrom tensorflow import keras\n\nb1_ls = sai.ConditionalBuyer()\n\ndef sampling7(df): \n    condition1 = (-0.3 <= df.D0_Change) & (df.D0_Change <= 0.3) \n    condition2 = df.D0_trading_value >= 1000000000 \n    condition3 = (-0.05 >= df.D0_Change) | (0.05 <= df.D0_Change) \n    condition = condition1 & condition2 & condition3\n    return condition\n\nb1_ls.condition = sampling7 \n\n\nb2_ls = sai.MachinelearningBuyer()\n\n# â­ User-defined functions (users who want deep learning modeling)\ndef transform(data): # A function that converts into a two-dimensional structure / data: list (lst_time_series)\n    data_2d = []\n    n_col = int(len(data[0]) / 10) \n    for row in data:      \n        data_2d.append([])\n        for i in range(0, len(row), n_col):\n            data_2d[-1].append(row[i:i+n_col])\n    \n    return np.array(data_2d)\n    \n\n# Directly define a two-dimensional structure transformation function (transform) and store it in the data_transform property\nb2_ls.data_transform = transform \n\nmodel = keras.models.Sequential()\nmodel.add(keras.layers.InputLayer(input_shape=(10, 48)))\nmodel.add(keras.layers.LSTM(128, activation='relu', return_sequences=True))\nmodel.add(keras.layers.LSTM(64, activation='relu', return_sequences=True))\nmodel.add(keras.layers.Dropout(0.2))\nmodel.add(keras.layers.LSTM(64, activation='relu', return_sequences=True))\nmodel.add(keras.layers.LSTM(64, activation='relu', return_sequences=True))\nmodel.add(keras.layers.LSTM(64, activation='relu', return_sequences=True))\nmodel.add(keras.layers.LSTM(32, activation='relu', return_sequences=False))\nmodel.add(keras.layers.Dropout(0.2))\nmodel.add(keras.layers.Dense(1, activation='sigmoid'))\n    \nmodel.compile(optimizer=keras.optimizers.Adam(\n    learning_rate=keras.optimizers.schedules.ExponentialDecay(0.05,decay_steps=100000,decay_rate=0.96)), \n    loss=\"binary_crossentropy\",\n    metrics=['accuracy'])\n\nb2_ls.algorithm =  model\n\n\nsell_all = sai.SubSeller() \n\n\nt7 = sai.Trader()\nt7.name = 'saiLSTM' \nt7.label = 'class&0.02' \nt7.buyer = sai.Buyer([b1_ls, b2_ls]) \nt7.seller = sai.Seller(sell_all)\n\nlst_trader.append(t7)\n\níŠ¸ë ˆì´ë”ë¥¼ ì •ì˜í•˜ê³ , ì²˜ìŒì— ìƒì„±í–ˆë˜ lst_trader ë¦¬ìŠ¤íŠ¸ì— ê°ê°ì˜ íŠ¸ë ˆì´ë”ë¥¼ ë§ˆì§€ë§‰ ì¤„ì—ì„œ append í•´ì¤ë‹ˆë‹¤.\níŠ¸ë ˆì´ë”ë¥¼ ì •ì˜í•˜ê³ , ëª¨ë“  íŠ¸ë ˆì´ë”ë“¤ì´ ëª¨ì•„ì§„ lst_traderë¥¼ ì‚¬ìš©í•˜ì—¬ ì •ì˜í•œ ì—¬ëŸ¬ê°œì˜ ëª¨ë¸ë¡œ ëª¨ë¸ í•™ìŠµ ë° ì£¼ì‹ ë§¤ë§¤ë¥¼ ì‹¤í—˜í•´ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. stockait ì‚¬ìš©í•˜ì—¬ ì—¬ëŸ¬ ëª¨ë¸ í•™ìŠµí•´ë³´ê¸° ê¸€ì— ì˜ˆì‹œë¥¼ ë³¼ ìˆ˜ ìˆìœ¼ë©°, ì¢‹ì€ ì°¸ê³  ìë£Œê°€ ë  ê²ƒì…ë‹ˆë‹¤."
  },
  {
    "objectID": "posts/s02.different_models.html",
    "href": "posts/s02.different_models.html",
    "title": "[stockait] stockait ì‚¬ìš©í•˜ì—¬ ì£¼ê°€ ì˜ˆì¸¡ ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ ì‹¤í—˜í•˜ê¸°",
    "section": "",
    "text": "import stockait as sai\nimport pandas as pd\n\në³¸ ê¸€ì€ stockait ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•˜ì—¬ ê°™ì€ ë°©ì‹ìœ¼ë¡œ ì „ì²˜ë¦¬í•œ ë°ì´í„°ì…‹ìœ¼ë¡œ ê°ê° ë‹¤ë¥¸ ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ì„ í•™ìŠµí•˜ì—¬ ì–´ë–¤ íŠ¸ë ˆì´ë”ì˜ ìˆ˜ìµë¥ ì´ ê°€ì¥ ë†’ê²Œ ë‚˜ì˜¤ëŠ”ì§€ ì‹¤í—˜í•´ë³´ëŠ” ì˜ˆì œì…ë‹ˆë‹¤.\n\n \n\n1. ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°\nâ­ ë³¸ ê¸€ì—ì„œ ì‚¬ìš©ë˜ëŠ” ì£¼ê°€ ë°ì´í„°ì…‹ ì „ì²˜ë¦¬ëŠ” ìƒëµí•˜ê³ , [stockait] stockait ì‚¬ìš©í•˜ì—¬ ì£¼ê°€ ì˜ˆì¸¡ ëª¨ë¸ í•™ìŠµí•˜ê¸° ê¸€ê³¼ ê°™ì€ ë°©ì‹ìœ¼ë¡œ ì „ì²˜ë¦¬í•˜ì—¬ ì €ì¥í•œ ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì™€ì„œ ì‚¬ìš©í•˜ê² ìŠµë‹ˆë‹¤.\n\ndf_time_series = pd.read_parquet(\"../../../../cha0716/time_series_0305.parquet\")\ndf_time_series_scaled = pd.read_parquet(\"../../../../cha0716/time_series_scaled_0305.parquet\")\n\ndf_time_series = df_time_series[~(df_time_series[\"Code\"].isin([\"33626K\", \"33637k\", '33637K']))]\ndf_time_series_scaled = df_time_series_scaled[~(df_time_series_scaled[\"Code\"].isin([\"33626K\", \"33637k\", '33637K']))]\n\ndf_time_series['Code'] = df_time_series['Code'].astype(str).str.zfill(6)\ndf_time_series_scaled['Code'] = df_time_series_scaled['Code'].astype(str).str.zfill(6)\n\në¶ˆëŸ¬ì˜¨ ë°ì´í„°ì…‹ì€ 2016ë…„ ë¶€í„° 2021ë…„ê¹Œì§€ div-close ë°©ë²•ìœ¼ë¡œ í‘œì¤€í™”ëœ ì‹œê³„ì—´ ë°ì´í„°ì…ë‹ˆë‹¤.\n\ndata = df_time_series # Data Before Scaling\ndata_scaled = df_time_series_scaled # Data After Scaling\n\n# train, test dataset split\ntrain_data = data[(data['Date'] >= '2017-01-01') & (data['Date'] <= '2020-12-31')]\ntest_data = data[(data['Date'] >= '2021-01-01') & (data['Date'] <= '2021-12-31')]\n\n# train, test dataset split (scaled) \ntrain_data_scaled = data_scaled[(data_scaled['Date'] >= '2017-01-01') & (data_scaled['Date'] <= '2020-12-31')]\ntest_data_scaled = data_scaled[(data_scaled['Date'] >= '2021-01-01') & (data_scaled['Date'] <= '2021-12-31')]\n\nprint(train_data.shape, test_data.shape)\nprint(train_data_scaled.shape, test_data_scaled.shape)\n\n(828290, 483) (217159, 483)\n(828290, 483) (217159, 483)\n\n\n2017ë…„ ë¶€í„° 2020ë…„ê¹Œì§€ëŠ” í•™ìŠµ ë°ì´í„°ì…‹, 2021ë…„ì€ ì‹œí—˜ ë°ì´í„°ì…‹ìœ¼ë¡œ ì„¤ì •í•˜ì˜€ìŠµë‹ˆë‹¤.\n \n\n\n2. Trader ì •ì˜\nê·¸ ë‹¤ìŒ íŠ¸ë ˆì´ë”ë¥¼ ì •ì˜í•©ë‹ˆë‹¤. ë³¸ ê¸€ì—ì„œëŠ” ê¸€ì—ì„œ ì •ì˜í–ˆë˜ LightGBM, XGBoost, RandomForest, LSTM ëª¨ë¸ì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•˜ê² ìŠµë‹ˆë‹¤.\n\nlst_trader = [] \n\n\n\n1) LightGBM\n\nfrom lightgbm import LGBMClassifier\n\n# conditional_buyer: Object that determines acquisition based on data filtering conditions \nb1_lg = sai.ConditionalBuyer()\n\ndef sampling1(df): # Create a conditional function\n    condition1 = (-0.3 <= df.D0_Change) & (df.D0_Change <= 0.3) # Remove exceptions that exceed upper and lower limits\n    condition2 = (df.D0_Close * df.D0_Volume) >= 1000000000 # condition 1: Transaction amount of more than 1 billion won \n    condition3 = (-0.05 >= df.D0_Change) | (0.05 <= df.D0_Change) # condition 2: Today's stock price change rate is more than 5%\n    condition = condition1 & condition2 & condition3\n    return condition\n\nb1_lg.condition = sampling1  # Define the condition function directly (sampling1) and store it in the condition property \n\n\n# machinelearning_buyer: Object that determines acquisition by machine learning model\nb2_lg = sai.MachinelearningBuyer()\n\n# Save user-defined models to algorithm properties\nscale_pos_weight = round(72/28 , 2)\nparams = {  'random_state' : 42,\n            'scale_pos_weight' : scale_pos_weight,\n            'learning_rate' : 0.1, \n            'num_iterations' : 1000,\n            'max_depth' : 4,\n            'n_jobs' : 30,\n            'boost_from_average' : False,\n            'objective' : 'binary' }\n\nb2_lg.algorithm =  LGBMClassifier( **params )\n\n\n# SubSeller: Object that determines selling all of the following days\nsell_all = sai.SubSeller() \n\n\n# Trader Object   \nt1 = sai.Trader()\nt1.name = 'saiLightGBM' # Trader's name\nt1.label = 'class&0.02' # Set the Trader dependent variable (do not set if it is regression analysis) \nt1.buyer = sai.Buyer([b1_lg, b2_lg]) # [ conditional buyer, machinelearning buyer ] \nt1.seller = sai.Seller(sell_all)\n\nlst_trader.append(t1)\n\n\n\n\n2) XGBoost\n\nfrom xgboost import XGBClassifier\n\nb1_xgb = sai.ConditionalBuyer() \n\ndef sampling2(df): \n    condition1 = (-0.3 <= df.D0_Change) & (df.D0_Change <= 0.3) \n    condition2 = (df.D0_Close * df.D0_Volume) >= 1000000000 \n    condition3 = (-0.05 >= df.D0_Change) | (0.05 <= df.D0_Change) \n    condition = condition1 & condition2 & condition3\n    return condition\n\nb1_xgb.condition = sampling2\n\n\nb2_xgb = sai.MachinelearningBuyer()  \n\nscale_pos_weight = round(72/28 , 2)\nb2_xgb.algorithm = XGBClassifier(random_state = 42,\n                   n_jobs=30,\n                   scale_pos_weight=scale_pos_weight,\n                   learning_rate=0.1,\n                   max_depth=4,\n                   n_estimators=1000,\n                   )  \n\nsell_all = sai.SubSeller()\n\n\nt2 = sai.Trader()\nt2.name = 'saiXGboost' \nt2.label = 'class&0.02' \nt2.buyer = sai.Buyer([b1_xgb, b2_xgb])\nt2.seller = sai.Seller(sell_all) \n\nlst_trader.append(t2) \n\n\n\n\n3) RandomForest\n\nfrom sklearn.ensemble import RandomForestClassifier\n\nb1_rf = sai.ConditionalBuyer()\n\ndef sampling3(df):  \n    condition1 = (-0.3 <= df.D0_Change) & (df.D0_Change <= 0.3) \n    condition2 = (df.D0_Close * df.D0_Volume) >= 1000000000 \n    condition3 = (-0.05 >= df.D0_Change) | (0.05 <= df.D0_Change) \n    condition = condition1 & condition2 & condition3\n    return condition\n\nb1_rf.condition = sampling3\n\n\nb2_rf = sai.MachinelearningBuyer()  \n\nb2_rf.algorithm = RandomForestClassifier() \n\n\nsell_all = sai.SubSeller() \n\n\nt3 = sai.Trader()\nt3.name = 'saiRandomForest'  \nt3.label = 'class&0.02' \nt3.buyer = sai.Buyer([b1_rf, b2_rf]) \nt3.seller = sai.Seller(sell_all)\n\nlst_trader.append(t3) \n\n\n\n\n4) LSTM\n\nfrom tensorflow import keras\nimport numpy as np\n\nb1_ls = sai.ConditionalBuyer()\n\ndef sampling4(df): \n    condition1 = (-0.3 <= df.D0_Change) & (df.D0_Change <= 0.3) \n    condition2 = (df.D0_Close * df.D0_Volume) >= 1000000000 \n    condition3 = (-0.05 >= df.D0_Change) | (0.05 <= df.D0_Change) \n    condition = condition1 & condition2 & condition3\n    return condition\n\nb1_ls.condition = sampling4\n\n\nb2_ls = sai.MachinelearningBuyer()\n\n# â­ User-defined functions (users who want deep learning modeling)\ndef transform(data): # A function that converts into a two-dimensional structure / data: list (lst_time_series)\n    data_2d = []\n    n_col = int(len(data[0]) / 10) \n    for row in data:      \n        data_2d.append([])\n        for i in range(0, len(row), n_col):\n            data_2d[-1].append(row[i:i+n_col])\n    \n    return np.array(data_2d)\n    \n\n# Directly define a two-dimensional structure transformation function (transform) and store it in the data_transform property\nb2_ls.data_transform = transform \n\nmodel = keras.models.Sequential()\nmodel.add(keras.layers.InputLayer(input_shape=(10, 48)))\nmodel.add(keras.layers.LSTM(128, activation='selu', return_sequences=True))\nmodel.add(keras.layers.Dropout(0.2))\nmodel.add(keras.layers.LSTM(64, activation='selu', return_sequences=True))\nmodel.add(keras.layers.Dropout(0.2))\nmodel.add(keras.layers.LSTM(32, activation='selu', return_sequences=False))\nmodel.add(keras.layers.Dropout(0.2))\nmodel.add(keras.layers.Dense(1, activation='sigmoid'))\n    \nmodel.compile(optimizer=keras.optimizers.Adam(\n    learning_rate=keras.optimizers.schedules.ExponentialDecay(0.05,decay_steps=100000,decay_rate=0.96)), \n    loss=\"binary_crossentropy\",\n    metrics=['accuracy'])\n\nb2_ls.algorithm =  model\n\n\nsell_all = sai.SubSeller() \n\n\nt4 = sai.Trader()\nt4.name = 'saiLSTM' \nt4.label = 'class&0.02' \nt4.buyer = sai.Buyer([b1_ls, b2_ls]) \nt4.seller = sai.Seller(sell_all)\n\nlst_trader.append(t4)\n\n2023-04-02 10:53:25.764037: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2023-04-02 10:53:25.885853: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n2023-04-02 10:53:26.354422: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.6/lib64::/usr/local/cuda-11.5/lib64:/usr/local/cuda-11.5/targets/x86_64-linux/lib\n2023-04-02 10:53:26.354479: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.6/lib64::/usr/local/cuda-11.5/lib64:/usr/local/cuda-11.5/targets/x86_64-linux/lib\n2023-04-02 10:53:26.354484: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n2023-04-02 10:53:27.287623: E tensorflow/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n2023-04-02 10:53:27.287685: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (ahnbi2): /proc/driver/nvidia/version does not exist\n2023-04-02 10:53:27.288577: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n\n\në„¤ê°œì˜ ëª¨ë¸ì„ ì •ì˜í•˜ì˜€ê³  ê°ê°ì˜ ëª¨ë¸ì„ ëª¨ë‘ lst_trader ì•ˆì— ë„£ì—ˆìŠµë‹ˆë‹¤.\n \n\n\n\n3. Trader(Model) í•™ìŠ´ ë° í‰ê°€\n\n1) íŠ¸ë ˆì´ë”ì— ë°ì´í„°ì…‹ ì €ì¥í•˜ê¸°\níŠ¸ë ˆì´ë” ì•ˆì— ë°ì´í„°ì…‹ì„ ì €ì¥í•©ë‹ˆë‹¤.\n\nsai.save_dataset(lst_trader, train_data, test_data, train_data_scaled, test_data_scaled)\n\n== saiLightGBM ==\n== train_code_date: (828290, 2),  test_code_date: (217159, 2) ==\n== trainX: (828290, 480),  testX: (217159, 480) ==\n== trainX_scaled: (828290, 480),  testX_scaled: (217159, 480) ==\n== trainY: (828290,),  testY: (217159,) ==\n== trainY_classification: (828290,),  testY_classification: (217159,) ==\n\n== saiXGboost ==\n== train_code_date: (828290, 2),  test_code_date: (217159, 2) ==\n== trainX: (828290, 480),  testX: (217159, 480) ==\n== trainX_scaled: (828290, 480),  testX_scaled: (217159, 480) ==\n== trainY: (828290,),  testY: (217159,) ==\n== trainY_classification: (828290,),  testY_classification: (217159,) ==\n\n== saiRandomForest ==\n== train_code_date: (828290, 2),  test_code_date: (217159, 2) ==\n== trainX: (828290, 480),  testX: (217159, 480) ==\n== trainX_scaled: (828290, 480),  testX_scaled: (217159, 480) ==\n== trainY: (828290,),  testY: (217159,) ==\n== trainY_classification: (828290,),  testY_classification: (217159,) ==\n\n== saiLSTM ==\n== train_code_date: (828290, 2),  test_code_date: (217159, 2) ==\n== trainX: (828290, 480),  testX: (217159, 480) ==\n== trainX_scaled: (828290, 480),  testX_scaled: (217159, 480) ==\n== trainY: (828290,),  testY: (217159,) ==\n== trainY_classification: (828290,),  testY_classification: (217159,) ==\n\n\n\n\n\n\n2) ëª¨ë¸ í•™ìŠµ\nê° íŠ¸ë ˆì´ë”ì— ì •ì˜ë˜ì–´ìˆëŠ” ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ì„ í•™ìŠµí•©ë‹ˆë‹¤.\n\nsai.trader_train(lst_trader) \n\n== saiLightGBM Model Fitting Completed ==\n== saiXGboost Model Fitting Completed ==\n== saiRandomForest Model Fitting Completed ==\n1268/1268 [==============================] - 18s 13ms/step - loss: 257227161600.0000 - accuracy: 0.6267\n== saiLSTM Model Fitting Completed ==\n\n\n\n\n\n3) ëª¨ë¸ í‰ê°€ ë° ì„ê³„ê°’ ì„¤ì •\n\n\nëª¨ë¸ í‰ê°€\në„¤ê°œì˜ ëª¨ë¸ì— ëŒ€í•˜ì—¬ threshold ë³„ í‰ê°€ì§€í‘œë¥¼ ì‹œê°í™” í•˜ê³ , ë§¤ìˆ˜ë¥¼ ìœ„í•œ ì„ê³„ê°’ ì„¤ì •ì„ ê³ ë ¤í•©ë‹ˆë‹¤.\n\nsai.get_eval_by_threshold(lst_trader)\n\n380/380 [==============================] - 2s 4ms/step\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nì„ê³„ê°’ ì„¤ì •\nìœ„ì—ì„œ íŒë‹¨í•œ thresholdë¥¼ ìˆœì„œëŒ€ë¡œ lst_thresholdì— ë„£ì–´ì£¼ê³ , histogramì„ ê·¸ë ¤ ìˆ˜ìµì„± ê²€ì¦ì„ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì˜ˆì‹œë¡œ 0.8, 0.8, 0.6, 0.8 ë¡œ ì„¤ì •í•´ì£¼ì—ˆê³ , ë³€ê²½í•´ê°€ë©° ìˆ˜ìµì„± ê²€ì¦ ì‹¤í—˜ì„ ì§€ì†ì ìœ¼ë¡œ í•´ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\nsai.set_threshold(lst_trader, lst_threshold=[0.8, 0.8, 0.6, 0.8], histogram=True)\n\nError: local variable 'threshold' referenced before assignment\n380/380 [==============================] - 2s 4ms/step\n\n\n<Figure size 1152x720 with 0 Axes>\n\n\n<Figure size 1152x720 with 0 Axes>\n\n\n\n\n\n<Figure size 1152x720 with 0 Axes>\n\n\n<Figure size 1152x720 with 0 Axes>\n\n\n\n\n\n<Figure size 1152x720 with 0 Axes>\n\n\n<Figure size 1152x720 with 0 Axes>\n\n\n\n\n\n<Figure size 1152x720 with 0 Axes>\n\n\n<Figure size 1152x720 with 0 Axes>\n\n\n\n\n\n \n\n\n\n\n4. Back-Testing\n\n\n1) ë§¤ë§¤ì¼ì§€ ì‘ì„±\nê°ê°ì˜ íŠ¸ë ˆì´ë”ì—ì„œ ëª¨ë“  ë‚ ì§œì— ëŒ€í•˜ì—¬ ë§¤ìˆ˜ ë° ë§¤ë„ ê¸°ë¡ì„ ìƒì„±í•©ë‹ˆë‹¤.\n\ndf_signal_all = sai.decision(lst_trader, dtype='test')\ndf_signal_all\n\n217159it [00:06, 36183.33it/s]\n217159it [00:05, 36418.57it/s]\n\n\n== saiLightGBM completed ==\n\n\n217159it [00:06, 35330.36it/s]\n217159it [00:06, 35767.62it/s]\n\n\n== saiXGboost completed ==\n\n\n217159it [00:06, 35462.79it/s]\n217159it [00:06, 35646.76it/s]\n\n\n== saiRandomForest completed ==\n6787/6787 [==============================] - 30s 4ms/step\n\n\n217159it [00:05, 37334.97it/s]\n217159it [00:05, 37169.70it/s]\n\n\n== saiLSTM completed ==\n\n\n\n\n\n\n  \n    \n      \n      Trader_id\n      Date\n      Code\n      +(buy)/-(sell)\n      Amount\n      Close\n    \n  \n  \n    \n      0\n      saiLightGBM\n      2021-01-04\n      000020\n      +\n      0.0\n      19100.0\n    \n    \n      1\n      saiLightGBM\n      2021-01-05\n      000020\n      +\n      0.0\n      19400.0\n    \n    \n      2\n      saiLightGBM\n      2021-01-06\n      000020\n      +\n      0.0\n      19700.0\n    \n    \n      3\n      saiLightGBM\n      2021-01-07\n      000020\n      +\n      0.0\n      19700.0\n    \n    \n      4\n      saiLightGBM\n      2021-01-08\n      000020\n      +\n      0.0\n      19100.0\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      217154\n      saiLSTM\n      2021-12-24\n      009900\n      -\n      1.0\n      30600.0\n    \n    \n      217155\n      saiLSTM\n      2021-12-27\n      009900\n      -\n      1.0\n      29900.0\n    \n    \n      217156\n      saiLSTM\n      2021-12-28\n      009900\n      -\n      1.0\n      29400.0\n    \n    \n      217157\n      saiLSTM\n      2021-12-29\n      009900\n      -\n      1.0\n      29850.0\n    \n    \n      217158\n      saiLSTM\n      2021-12-30\n      009900\n      -\n      1.0\n      30100.0\n    \n  \n\n1737272 rows Ã— 6 columns\n\n\n\n\n\n\n2) ìˆ˜ìµë¥  ê³„ì‚° ì‹œë®¬ë ˆì´ì…˜\nìœ„ì˜ ë§¤ë§¤ì¼ì§€ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ê°ê°ì˜ íŠ¸ë ˆì´ë”ì˜ ëª¨ë“  ë‚ ì§œì— ëŒ€í•œ ìˆ˜ìµë¥ ì„ ê³„ì‚°í•©ë‹ˆë‹¤.\n\ndf_history_all = sai.simulation(df_signal_all, init_budget=10000000, init_stock={}, fee=0.01)\ndf_history_all\n\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 247/248 [00:07<00:00, 33.88it/s]\n\n\n== saiLSTM completed ==\n\n\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 247/248 [00:07<00:00, 33.80it/s]\n\n\n== saiLightGBM completed ==\n\n\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 247/248 [00:07<00:00, 33.88it/s]\n\n\n== saiRandomForest completed ==\n\n\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 247/248 [00:07<00:00, 33.95it/s]\n\n\n== saiXGboost completed ==\n\n\n\n\n\n\n\n\n\n  \n    \n      \n      Trader_id\n      Sell_date\n      Budget\n      Yield\n      Stock\n    \n  \n  \n    \n      0\n      saiLSTM\n      2021-01-04\n      10000000\n      0.000000\n      {}\n    \n    \n      1\n      saiLSTM\n      2021-01-05\n      10042066\n      0.420662\n      {'000100': 3, '000660': 2, '001120': 10, '0013...\n    \n    \n      2\n      saiLSTM\n      2021-01-06\n      9793093\n      -2.069066\n      {'000120': 1, '001250': 154, '001430': 27, '00...\n    \n    \n      3\n      saiLSTM\n      2021-01-07\n      9830300\n      -1.696992\n      {'000220': 59, '001200': 107, '001250': 205, '...\n    \n    \n      4\n      saiLSTM\n      2021-01-08\n      9808160\n      -1.918394\n      {'000540': 105, '001200': 67, '001380': 131, '...\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      243\n      saiXGboost\n      2021-12-24\n      6110500\n      -38.894995\n      {'025620': 315}\n    \n    \n      244\n      saiXGboost\n      2021-12-27\n      6110500\n      -38.894995\n      {}\n    \n    \n      245\n      saiXGboost\n      2021-12-28\n      6110500\n      -38.894995\n      {}\n    \n    \n      246\n      saiXGboost\n      2021-12-29\n      6110500\n      -38.894995\n      {}\n    \n    \n      247\n      saiXGboost\n      2021-12-30\n      6110500\n      -38.894995\n      {}\n    \n  \n\n992 rows Ã— 5 columns\n\n\n\n\n\n\n3) Leader Board\níŠ¸ë ˆì´ë”ì˜ ìµœì¢… ìˆ˜ìµë¥  ê²°ê³¼ë¥¼ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬í•˜ì—¬ ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ ë¦¬ë”ë³´ë“œë¥¼ ìƒì„±í•©ë‹ˆë‹¤.\n\nsai.leaderboard(df_history_all)\n\n\n\n\n\n  \n    \n      \n      Trader_id\n      Yield\n    \n  \n  \n    \n      0\n      saiLightGBM\n      44.423725\n    \n    \n      1\n      saiRandomForest\n      29.597223\n    \n    \n      2\n      saiXGboost\n      -38.894995\n    \n    \n      3\n      saiLSTM\n      -96.950032\n    \n  \n\n\n\n\në³¸ ì‹¤í—˜ì—ì„œëŠ” LightGBMì˜ ìˆ˜ìµë¥ ì´ ê°€ì¥ ë†’ê²Œ ë‚˜ì™”ìœ¼ë©°, LSTMì˜ ìˆ˜ìµë¥ ì´ ê°€ì¥ ë‚®ê²Œ ë‚˜ì™”ìŠµë‹ˆë‹¤.\n\n\n\n4) ìˆ˜ìµë¥  ê²°ê³¼ ì‹œê°í™”\níŠ¸ë ˆì´ë” ë³„ë¡œ ëª¨ë“  ë‚ ì§œì— ëŒ€í•œ ìˆ˜ìµë¥  ì¶”ì´ë¥¼ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\nsai.yield_plot(df_history_all)"
  },
  {
    "objectID": "posts/s03.different_scaling_method.html",
    "href": "posts/s03.different_scaling_method.html",
    "title": "[stockait] stockait ì‚¬ìš©í•˜ì—¬ ì£¼ê°€ ë°ì´í„° í‘œì¤€í™” ì‹¤í—˜í•˜ê¸°",
    "section": "",
    "text": "import pandas as pd\nimport stockait as sai\n\në³¸ ê¸€ì€ ê°™ì€ íŠ¸ë ˆì´ë”ì˜ ì¡°ê±´ì¼ ë•Œ ë„¤ê°œì˜ ë‹¤ë¥¸ í‘œì¤€í™” ë°©ë²•ìœ¼ë¡œ ì „ì²˜ë¦¬í•œ ë°ì´í„°ì…‹ ì¤‘ ì–´ë–¤ ë°ì´í„°ì…‹ì˜ ì„±ëŠ¥ì´ ê°€ì¥ ë†’ê²Œ ë‚˜ì˜¤ëŠ”ì§€ ë¹„êµ ì‹¤í—˜ì„ ìˆ˜í–‰í•˜ëŠ” ê¸€ì…ë‹ˆë‹¤.\n \n\n1. ë°ì´í„° ìˆ˜ì§‘\nì‹¤í—˜ì€ í•œêµ­ì˜ ì½”ìŠ¤í”¼ ì‹œì¥ì—ì„œ 500ê°œì˜ ì¢…ëª©ë§Œì„ ì‚¬ìš©í•´ì„œ ì£¼í–‰í•˜ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤.\n\nlst_tickers = sai.get_tickers(markets=['KOSPI'])\nprint(len(lst_tickers), lst_tickers[:5])\n\n920 ['095570', '006840', '282330', '027410', '138930']\n\n\n\nraw_data = sai.load_data(date=['2016-01-01', '2021-12-31'], tickers=lst_tickers[:600])\nprint(raw_data.shape)\nraw_data.head()\n\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 600/600 [00:38<00:00, 15.43it/s]\n\n\n(821266, 7)\n\n\n\n\n\n\n  \n    \n      \n      Code\n      Date\n      Open\n      High\n      Low\n      Close\n      Volume\n    \n  \n  \n    \n      0\n      000020\n      2016-01-04\n      8130\n      8150\n      7920\n      8140\n      281440\n    \n    \n      1\n      000020\n      2016-01-05\n      8040\n      8250\n      8000\n      8190\n      243179\n    \n    \n      2\n      000020\n      2016-01-06\n      8200\n      8590\n      8110\n      8550\n      609906\n    \n    \n      3\n      000020\n      2016-01-07\n      8470\n      8690\n      8190\n      8380\n      704752\n    \n    \n      4\n      000020\n      2016-01-08\n      8210\n      8900\n      8130\n      8770\n      802330\n    \n  \n\n\n\n\n\n\n\n2. ë°ì´í„° ì „ì²˜ë¦¬\n\n1) ë³´ì¡°ì§€í‘œ ì¶”ê°€\nì²«ë²ˆì§¸ë¡œ, ë³´ì¡°ì§€í‘œë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤.\n\ncheck_index = ['MA5', 'MA20', 'MA60','MA120', \n             'next_change','CMF','VPT','VMAP', \"ADI\",\n             'BHB','BLB','KCH','KCL','KCM','DCH','DCL','DCM','UI',\n             'SMA','EMA','WMA','MACD','VIneg','VIpos','TRIX','MI','CCI','DPO','KST','Ichimoku','ParabolicSAR','STC',\n             'RSI','SRSI','TSI','UO','SR','WR','AO','ROC','PPO','PVO']\n\ncheck_df = sai.add_index(data=raw_data, index_list=check_index)\ncheck_df\n\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 595/595 [09:39<00:00,  1.03it/s]\n\n\n\n\n\n\n  \n    \n      \n      Code\n      Date\n      Open\n      High\n      Low\n      Close\n      Volume\n      Change\n      MA5\n      MA20\n      ...\n      RSI\n      SRSI\n      TSI\n      UO\n      SR\n      WR\n      AO\n      ROC\n      PPO\n      PVO\n    \n  \n  \n    \n      0\n      000020\n      2016-06-29\n      9850\n      10100\n      9700\n      9750\n      352292\n      -0.001025\n      9544.0\n      10210.00\n      ...\n      45.763505\n      0.529126\n      -8.116135\n      0.0\n      41.176471\n      -58.823529\n      -827.794118\n      -3.940887\n      -1.408719\n      -11.019576\n    \n    \n      1\n      000020\n      2016-06-30\n      9850\n      10400\n      9760\n      10100\n      466248\n      0.035897\n      9618.0\n      10195.00\n      ...\n      51.232344\n      0.875363\n      -6.553028\n      0.0\n      54.901961\n      -45.098039\n      -765.088235\n      -2.415459\n      -1.124410\n      -9.119594\n    \n    \n      2\n      000020\n      2016-07-01\n      10200\n      10200\n      9960\n      9960\n      208228\n      -0.013861\n      9794.0\n      10148.00\n      ...\n      49.099659\n      0.767924\n      -5.911240\n      0.0\n      49.411765\n      -50.588235\n      -624.205882\n      -4.230769\n      -1.001426\n      -12.558989\n    \n    \n      3\n      000020\n      2016-07-04\n      10000\n      10400\n      9900\n      10400\n      275210\n      0.044177\n      9994.0\n      10135.50\n      ...\n      55.385557\n      1.000000\n      -3.312304\n      0.0\n      66.666667\n      -33.333333\n      -427.205882\n      -0.952381\n      -0.541344\n      -13.975141\n    \n    \n      4\n      000020\n      2016-07-05\n      10400\n      10450\n      10200\n      10350\n      156010\n      -0.004808\n      10112.0\n      10118.00\n      ...\n      54.560981\n      0.961700\n      -1.483696\n      0.0\n      64.705882\n      -35.294118\n      -266.529412\n      1.970443\n      -0.216142\n      -17.711174\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      750398\n      5307W1\n      2021-12-23\n      3420\n      3480\n      3370\n      3425\n      391929\n      0.007353\n      3440.0\n      3535.00\n      ...\n      40.390231\n      0.159714\n      -8.758815\n      0.0\n      20.000000\n      -80.000000\n      -126.044118\n      -5.647383\n      -0.810232\n      -11.033540\n    \n    \n      750399\n      5307W1\n      2021-12-24\n      3465\n      3520\n      3420\n      3500\n      313474\n      0.021898\n      3438.0\n      3529.50\n      ...\n      47.968313\n      0.589811\n      -8.179822\n      0.0\n      43.076923\n      -56.923077\n      -138.058824\n      -3.047091\n      -0.740535\n      -10.762865\n    \n    \n      750400\n      5307W1\n      2021-12-27\n      3515\n      3515\n      3465\n      3475\n      265569\n      -0.007143\n      3446.0\n      3527.25\n      ...\n      45.874782\n      0.494226\n      -8.285385\n      0.0\n      35.384615\n      -64.615385\n      -134.838235\n      -4.005525\n      -0.734518\n      -11.598254\n    \n    \n      750401\n      5307W1\n      2021-12-28\n      3500\n      3520\n      3455\n      3515\n      359215\n      0.011511\n      3463.0\n      3532.75\n      ...\n      49.660421\n      0.719680\n      -7.274530\n      0.0\n      52.542373\n      -47.457627\n      -114.720588\n      -2.361111\n      -0.630225\n      -9.883604\n    \n    \n      750402\n      5307W1\n      2021-12-29\n      3520\n      3650\n      3490\n      3615\n      436463\n      0.02845\n      3506.0\n      3539.75\n      ...\n      57.637528\n      1.000000\n      -3.768305\n      0.0\n      86.440678\n      -13.559322\n      -86.279412\n      -0.138122\n      -0.313929\n      -6.618431\n    \n  \n\n750403 rows Ã— 50 columns\n\n\n\n\n\n\n2) ë°ì´í„° í‘œì¤€í™”\në‹¤ìŒìœ¼ë¡œ stockaitì—ì„œ ì œê³µí•˜ëŠ” ë„¤ê°€ì§€ì˜ í‘œì¤€í™” ë°©ë²•ì„ ì‚¬ìš©í•˜ì—¬ ë°ì´í„°ë¥¼ í‘œì¤€í™” í•©ë‹ˆë‹¤.\n\n\nminmax\n\nscaled_minmax = sai.scaling(data=check_df, scaler_type=\"minmax\")\nscaled_minmax.head()\n\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 586/586 [00:58<00:00, 10.09it/s]\n\n\n\n\n\n\n  \n    \n      \n      Code\n      Date\n      Open\n      High\n      Low\n      Close\n      Volume\n      Change\n      MA5\n      MA20\n      ...\n      RSI\n      SRSI\n      TSI\n      UO\n      SR\n      WR\n      AO\n      ROC\n      PPO\n      PVO\n    \n  \n  \n    \n      0\n      000020\n      2016-06-29\n      0.194499\n      0.168374\n      0.201232\n      9750\n      352292\n      -0.001025\n      0.187465\n      0.222265\n      ...\n      45.763505\n      0.529126\n      -8.116135\n      0.0\n      41.176471\n      -58.823529\n      -827.794118\n      -3.940887\n      -1.408719\n      -11.019576\n    \n    \n      1\n      000020\n      2016-06-30\n      0.194499\n      0.17862\n      0.203696\n      10100\n      466248\n      0.035897\n      0.19065\n      0.221465\n      ...\n      51.232344\n      0.875363\n      -6.553028\n      0.0\n      54.901961\n      -45.098039\n      -765.088235\n      -2.415459\n      -1.124410\n      -9.119594\n    \n    \n      2\n      000020\n      2016-07-01\n      0.208251\n      0.17179\n      0.21191\n      9960\n      208228\n      -0.013861\n      0.198227\n      0.218958\n      ...\n      49.099659\n      0.767924\n      -5.911240\n      0.0\n      49.411765\n      -50.588235\n      -624.205882\n      -4.230769\n      -1.001426\n      -12.558989\n    \n    \n      3\n      000020\n      2016-07-04\n      0.200393\n      0.17862\n      0.209446\n      10400\n      275210\n      0.044177\n      0.206836\n      0.218291\n      ...\n      55.385557\n      1.000000\n      -3.312304\n      0.0\n      66.666667\n      -33.333333\n      -427.205882\n      -0.952381\n      -0.541344\n      -13.975141\n    \n    \n      4\n      000020\n      2016-07-05\n      0.21611\n      0.180328\n      0.221766\n      10350\n      156010\n      -0.004808\n      0.211915\n      0.217358\n      ...\n      54.560981\n      0.961700\n      -1.483696\n      0.0\n      64.705882\n      -35.294118\n      -266.529412\n      1.970443\n      -0.216142\n      -17.711174\n    \n  \n\n5 rows Ã— 50 columns\n\n\n\n\n\n\nstandard\n\nscaled_standard = sai.scaling(data=check_df, scaler_type=\"standard\")\nscaled_standard.head()\n\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 586/586 [00:26<00:00, 21.84it/s]\n\n\n\n\n\n\n  \n    \n      \n      Code\n      Date\n      Open\n      High\n      Low\n      Close\n      Volume\n      Change\n      MA5\n      MA20\n      ...\n      RSI\n      SRSI\n      TSI\n      UO\n      SR\n      WR\n      AO\n      ROC\n      PPO\n      PVO\n    \n  \n  \n    \n      0\n      000020\n      2016-06-29\n      -0.426667\n      -0.40741\n      -0.422378\n      9750\n      352292\n      -0.001025\n      -0.497811\n      -0.336067\n      ...\n      45.763505\n      0.529126\n      -8.116135\n      0.0\n      41.176471\n      -58.823529\n      -827.794118\n      -3.940887\n      -1.408719\n      -11.019576\n    \n    \n      1\n      000020\n      2016-06-30\n      -0.426667\n      -0.339931\n      -0.407372\n      10100\n      466248\n      0.035897\n      -0.479964\n      -0.339741\n      ...\n      51.232344\n      0.875363\n      -6.553028\n      0.0\n      54.901961\n      -45.098039\n      -765.088235\n      -2.415459\n      -1.124410\n      -9.119594\n    \n    \n      2\n      000020\n      2016-07-01\n      -0.343357\n      -0.384917\n      -0.357352\n      9960\n      208228\n      -0.013861\n      -0.437518\n      -0.351253\n      ...\n      49.099659\n      0.767924\n      -5.911240\n      0.0\n      49.411765\n      -50.588235\n      -624.205882\n      -4.230769\n      -1.001426\n      -12.558989\n    \n    \n      3\n      000020\n      2016-07-04\n      -0.390963\n      -0.339931\n      -0.372358\n      10400\n      275210\n      0.044177\n      -0.389284\n      -0.354314\n      ...\n      55.385557\n      1.000000\n      -3.312304\n      0.0\n      66.666667\n      -33.333333\n      -427.205882\n      -0.952381\n      -0.541344\n      -13.975141\n    \n    \n      4\n      000020\n      2016-07-05\n      -0.295751\n      -0.328684\n      -0.297327\n      10350\n      156010\n      -0.004808\n      -0.360826\n      -0.358600\n      ...\n      54.560981\n      0.961700\n      -1.483696\n      0.0\n      64.705882\n      -35.294118\n      -266.529412\n      1.970443\n      -0.216142\n      -17.711174\n    \n  \n\n5 rows Ã— 50 columns\n\n\n\n\n\n\nrobust\n\nscaled_robust = sai.scaling(data=check_df, scaler_type=\"robust\")\nscaled_robust.head()\n\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 586/586 [00:44<00:00, 13.26it/s]\n\n\n\n\n\n\n  \n    \n      \n      Code\n      Date\n      Open\n      High\n      Low\n      Close\n      Volume\n      Change\n      MA5\n      MA20\n      ...\n      RSI\n      SRSI\n      TSI\n      UO\n      SR\n      WR\n      AO\n      ROC\n      PPO\n      PVO\n    \n  \n  \n    \n      0\n      000020\n      2016-06-29\n      0.016653\n      0.060905\n      -0.009898\n      9750\n      352292\n      -0.001025\n      -0.037510\n      0.080375\n      ...\n      45.763505\n      0.529126\n      -8.116135\n      0.0\n      41.176471\n      -58.823529\n      -827.794118\n      -3.940887\n      -1.408719\n      -11.019576\n    \n    \n      1\n      000020\n      2016-06-30\n      0.016653\n      0.114006\n      0.000723\n      10100\n      466248\n      0.035897\n      -0.024412\n      0.077720\n      ...\n      51.232344\n      0.875363\n      -6.553028\n      0.0\n      54.901961\n      -45.098039\n      -765.088235\n      -2.415459\n      -1.124410\n      -9.119594\n    \n    \n      2\n      000020\n      2016-07-01\n      0.078605\n      0.078605\n      0.036124\n      9960\n      208228\n      -0.013861\n      0.006741\n      0.069401\n      ...\n      49.099659\n      0.767924\n      -5.911240\n      0.0\n      49.411765\n      -50.588235\n      -624.205882\n      -4.230769\n      -1.001426\n      -12.558989\n    \n    \n      3\n      000020\n      2016-07-04\n      0.043204\n      0.114006\n      0.025504\n      10400\n      275210\n      0.044177\n      0.042142\n      0.067188\n      ...\n      55.385557\n      1.000000\n      -3.312304\n      0.0\n      66.666667\n      -33.333333\n      -427.205882\n      -0.952381\n      -0.541344\n      -13.975141\n    \n    \n      4\n      000020\n      2016-07-05\n      0.114006\n      0.122857\n      0.078605\n      10350\n      156010\n      -0.004808\n      0.063029\n      0.064091\n      ...\n      54.560981\n      0.961700\n      -1.483696\n      0.0\n      64.705882\n      -35.294118\n      -266.529412\n      1.970443\n      -0.216142\n      -17.711174\n    \n  \n\n5 rows Ã— 50 columns\n\n\n\n\n\n\ndiv-close\n\nscaled_div_close = sai.scaling(data=check_df, scaler_type=\"div-close\")\nscaled_div_close.head()\n\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 586/586 [00:26<00:00, 21.94it/s]\n\n\n\n\n\n\n  \n    \n      \n      Code\n      Date\n      Open\n      High\n      Low\n      Close\n      Volume\n      Change\n      MA5\n      MA20\n      ...\n      RSI\n      SRSI\n      TSI\n      UO\n      SR\n      WR\n      AO\n      ROC\n      PPO\n      PVO\n    \n  \n  \n    \n      0\n      000020\n      2016-06-29\n      1.010256\n      1.035897\n      0.994872\n      9750\n      352292\n      -0.001025\n      0.978872\n      1.047179\n      ...\n      45.763505\n      0.529126\n      -8.116135\n      0.0\n      41.176471\n      -58.823529\n      -827.794118\n      -3.940887\n      -1.408719\n      -11.019576\n    \n    \n      1\n      000020\n      2016-06-30\n      1.010256\n      1.066667\n      1.001026\n      10100\n      466248\n      0.035897\n      0.986462\n      1.045641\n      ...\n      51.232344\n      0.875363\n      -6.553028\n      0.0\n      54.901961\n      -45.098039\n      -765.088235\n      -2.415459\n      -1.124410\n      -9.119594\n    \n    \n      2\n      000020\n      2016-07-01\n      1.009901\n      1.009901\n      0.986139\n      9960\n      208228\n      -0.013861\n      0.969703\n      1.004752\n      ...\n      49.099659\n      0.767924\n      -5.911240\n      0.0\n      49.411765\n      -50.588235\n      -624.205882\n      -4.230769\n      -1.001426\n      -12.558989\n    \n    \n      3\n      000020\n      2016-07-04\n      1.004016\n      1.044177\n      0.993976\n      10400\n      275210\n      0.044177\n      1.003414\n      1.017620\n      ...\n      55.385557\n      1.000000\n      -3.312304\n      0.0\n      66.666667\n      -33.333333\n      -427.205882\n      -0.952381\n      -0.541344\n      -13.975141\n    \n    \n      4\n      000020\n      2016-07-05\n      1.0\n      1.004808\n      0.980769\n      10350\n      156010\n      -0.004808\n      0.972308\n      0.972885\n      ...\n      54.560981\n      0.961700\n      -1.483696\n      0.0\n      64.705882\n      -35.294118\n      -266.529412\n      1.970443\n      -0.216142\n      -17.711174\n    \n  \n\n5 rows Ã— 50 columns\n\n\n\n\n\n\n\n3) ì‹œê³„ì—´ ë°ì´í„° ë³€í™˜\ní•˜ë‚˜ì˜ í–‰ì— 10ì¼ì¹˜(D-9 ~ D0)ì˜ ë°ì´í„°ê°€ ë‹´ê¸°ë„ë¡ ë°ì´í„°ì…‹ì„ ë³€í™˜í•©ë‹ˆë‹¤.\n\n# original dataset\ndf_time_series = sai.time_series(check_df, day=10)\n\n# scaled dataset \ndf_time_series_minmax = sai.time_series(scaled_minmax, day=10)\ndf_time_series_standard = sai.time_series(scaled_standard, day=10)\ndf_time_series_robust = sai.time_series(scaled_robust, day=10)\ndf_time_series_div_close = sai.time_series(scaled_div_close, day=10)\n\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 586/586 [02:19<00:00,  4.21it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 586/586 [02:25<00:00,  4.04it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 586/586 [02:18<00:00,  4.22it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 586/586 [02:18<00:00,  4.23it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 586/586 [02:18<00:00,  4.23it/s]\n\n\n \n\n\n\n3. íŠ¸ë ˆì´ë” ì •ì˜\n\nlst_trader = []\n\n\nfrom lightgbm import LGBMClassifier\n\n# conditional_buyer: Object that determines acquisition based on data filtering conditions \nb1_lg = sai.ConditionalBuyer()\n\ndef sampling1(df): # Create a conditional function\n    condition1 = (-0.3 <= df.D0_Change) & (df.D0_Change <= 0.3) # Remove exceptions that exceed upper and lower limits\n    condition2 = (df.D0_Close * df.D0_Volume) >= 1000000000 # condition 1: Transaction amount of more than 1 billion won \n    condition3 = (-0.05 >= df.D0_Change) | (0.05 <= df.D0_Change) # condition 2: Today's stock price change rate is more than 5%\n    condition = condition1 & condition2 & condition3\n    return condition\n\nb1_lg.condition = sampling1  # Define the condition function directly (sampling1) and store it in the condition property \n\n\n# machinelearning_buyer: Object that determines acquisition by machine learning model\nb2_lg = sai.MachinelearningBuyer()\n\n# Save user-defined models to algorithm properties\nscale_pos_weight = round(72/28 , 2)\nparams = {  'random_state' : 42,\n            'scale_pos_weight' : scale_pos_weight,\n            'learning_rate' : 0.1, \n            'num_iterations' : 1000,\n            'max_depth' : 4,\n            'n_jobs' : 30,\n            'boost_from_average' : False,\n            'objective' : 'binary' }\n\nb2_lg.algorithm =  LGBMClassifier( **params )\n\n\n# SubSeller: Object that determines selling all of the following days\nsell_all = sai.SubSeller() \n\níŠ¸ë ˆì´ë”ëŠ” ê°™ì€ ì¡°ê±´ìœ¼ë¡œ ìƒì„±ë˜ì–´ì•¼ í•˜ë¯€ë¡œ ì •ì˜ë¥¼ í•œ ë²ˆ í•´ì£¼ê³ , íŠ¸ë ˆì´ë”ë¥¼ êµ¬ë¶„í•˜ê¸° ìœ„í•œ ì´ë¦„ë§Œ ë‹¤ë¥´ê²Œ í•˜ì—¬ lst_traderì— ë„£ì–´ì¤ë‹ˆë‹¤.\n\ntrader_name = [\"trader_minmax\", \"trader_standard\", \"trader_robust\", \"trader_div-close\"]\nfor name in trader_name:    \n    trader = sai.Trader()\n    trader.name = name # Trader's name\n    trader.label = 'class&0.02' # Set the Trader dependent variable (do not set if it is regression analysis) \n    trader.buyer = sai.Buyer([b1_lg, b2_lg]) # [ conditional buyer, machinelearning buyer ] \n    trader.seller = sai.Seller(sell_all)\n    \n    lst_trader.append(trader)\n\n \n\n\n4. íŠ¸ë ˆì´ë” (ëª¨ë¸) í•™ìŠµ ë° í‰ê°€\n\n1) íŠ¸ë ˆì´ë” ê°ì²´ì— ë°ì´í„°ì…‹ ì €ì¥\n\ndf_time_series = df_time_series[~(df_time_series[\"Code\"].isin([\"33626K\", \"33637k\", '33637K']))]\ndf_time_series_minmax = df_time_series_minmax[~(df_time_series_minmax[\"Code\"].isin([\"33626K\", \"33637k\", '33637K']))]\ndf_time_series_standard = df_time_series_standard[~(df_time_series_standard[\"Code\"].isin([\"33626K\", \"33637k\", '33637K']))]\ndf_time_series_robust = df_time_series_robust[~(df_time_series_robust[\"Code\"].isin([\"33626K\", \"33637k\", '33637K']))]\ndf_time_series_div_close = df_time_series_div_close[~(df_time_series_div_close[\"Code\"].isin([\"33626K\", \"33637k\", '33637K']))]\n\n# train, test dataset split\ntrain_data = df_time_series[(df_time_series['Date'] >= '2017-01-01') & (df_time_series['Date'] <= '2020-12-31')]\ntest_data = df_time_series[(df_time_series['Date'] >= '2021-01-01') & (df_time_series['Date'] <= '2021-12-31')]\n\n# train, test dataset split (scaled) \ntrain_data_minmax = df_time_series_minmax[(df_time_series_minmax['Date'] >= '2017-01-01') & (df_time_series_minmax['Date'] <= '2020-12-31')]\ntest_data_minmax = df_time_series_minmax[(df_time_series_minmax['Date'] >= '2021-01-01') & (df_time_series_minmax['Date'] <= '2021-12-31')]\n\ntrain_data_standard = df_time_series_standard[(df_time_series_standard['Date'] >= '2017-01-01') & (df_time_series_standard['Date'] <= '2020-12-31')]\ntest_data_standard = df_time_series_standard[(df_time_series_standard['Date'] >= '2021-01-01') & (df_time_series_standard['Date'] <= '2021-12-31')]\n\ntrain_data_robust = df_time_series_robust[(df_time_series_robust['Date'] >= '2017-01-01') & (df_time_series_robust['Date'] <= '2020-12-31')]\ntest_data_robust = df_time_series_robust[(df_time_series_robust['Date'] >= '2021-01-01') & (df_time_series_robust['Date'] <= '2021-12-31')]\n\ntrain_data_div_close = df_time_series_div_close[(df_time_series_div_close['Date'] >= '2017-01-01') & (df_time_series_div_close['Date'] <= '2020-12-31')]\ntest_data_div_close = df_time_series_div_close[(df_time_series_div_close['Date'] >= '2021-01-01') & (df_time_series_div_close['Date'] <= '2021-12-31')]\n\n\n\nlst_dataset_scaled = [[train_data_minmax, test_data_minmax], \n                      [train_data_standard, test_data_standard],\n                     [train_data_robust, test_data_robust],\n                     [train_data_div_close, test_data_div_close]]\n\nfor i in range(4): \n    lst_trader_one = [lst_trader[i]]\n    sai.save_dataset(lst_trader_one, train_data, test_data, lst_dataset_scaled[i][0], lst_dataset_scaled[i][1])\n\n== trader_minmax ==\n== train_code_date: (542390, 2),  test_code_date: (139968, 2) ==\n== trainX: (542390, 470),  testX: (139968, 470) ==\n== trainX_scaled: (542390, 470),  testX_scaled: (139968, 470) ==\n== trainY: (542390,),  testY: (139968,) ==\n== trainY_classification: (542390,),  testY_classification: (139968,) ==\n\n== trader_standard ==\n== train_code_date: (542390, 2),  test_code_date: (139968, 2) ==\n== trainX: (542390, 470),  testX: (139968, 470) ==\n== trainX_scaled: (542390, 470),  testX_scaled: (139968, 470) ==\n== trainY: (542390,),  testY: (139968,) ==\n== trainY_classification: (542390,),  testY_classification: (139968,) ==\n\n== trader_robust ==\n== train_code_date: (542390, 2),  test_code_date: (139968, 2) ==\n== trainX: (542390, 470),  testX: (139968, 470) ==\n== trainX_scaled: (542390, 470),  testX_scaled: (139968, 470) ==\n== trainY: (542390,),  testY: (139968,) ==\n== trainY_classification: (542390,),  testY_classification: (139968,) ==\n\n== trader_div-close ==\n== train_code_date: (542390, 2),  test_code_date: (139968, 2) ==\n== trainX: (542390, 470),  testX: (139968, 470) ==\n== trainX_scaled: (542390, 470),  testX_scaled: (139968, 470) ==\n== trainY: (542390,),  testY: (139968,) ==\n== trainY_classification: (542390,),  testY_classification: (139968,) ==\n\n\n\n\n\n\n2) ëª¨ë¸ í•™ìŠµ\n\nsai.trader_train(lst_trader) \n\n== trader_minmax Model Fitting Completed ==\n== trader_standard Model Fitting Completed ==\n== trader_robust Model Fitting Completed ==\n== trader_div-close Model Fitting Completed ==\n\n\n\n\n\n3) ëª¨ë¸ í‰ê°€ ë° ì„ê³„ê°’ ì„¤ì •\n\nëª¨ë¸ í‰ê°€\n\nsai.get_eval_by_threshold(lst_trader)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nì„ê³„ê°’ ì„¤ì •\n\nsai.set_threshold(lst_trader, lst_threshold=[0.8, 0.8, 0.8, 0.8], histogram=True)\n\nError: local variable 'threshold' referenced before assignment\n\n\n<Figure size 1152x720 with 0 Axes>\n\n\n<Figure size 1152x720 with 0 Axes>\n\n\n\n\n\n<Figure size 1152x720 with 0 Axes>\n\n\n<Figure size 1152x720 with 0 Axes>\n\n\n\n\n\n<Figure size 1152x720 with 0 Axes>\n\n\n<Figure size 1152x720 with 0 Axes>\n\n\n\n\n\n<Figure size 1152x720 with 0 Axes>\n\n\n<Figure size 1152x720 with 0 Axes>\n\n\n\n\n\n \n\n\n\n\n4. BackTesting\n\n\n1) ë§¤ë§¤ì¼ì§€ ì‘ì„±\n\ndf_signal_all = sai.decision(lst_trader, dtype='test')\ndf_signal_all\n\n139968it [00:03, 36309.18it/s]\n139968it [00:03, 35560.93it/s]\n\n\n== trader_minmax completed ==\n\n\n139968it [00:03, 36491.79it/s]\n139968it [00:03, 36350.87it/s]\n\n\n== trader_standard completed ==\n\n\n139968it [00:03, 35696.38it/s]\n139968it [00:03, 36391.64it/s]\n\n\n== trader_robust completed ==\n\n\n139968it [00:03, 36208.76it/s]\n139968it [00:03, 35618.92it/s]\n\n\n== trader_div-close completed ==\n\n\n\n\n\n\n  \n    \n      \n      Trader_id\n      Date\n      Code\n      +(buy)/-(sell)\n      Amount\n      Close\n    \n  \n  \n    \n      0\n      trader_minmax\n      2021-01-04\n      000020\n      +\n      0.0\n      19100.0\n    \n    \n      1\n      trader_minmax\n      2021-01-05\n      000020\n      +\n      0.0\n      19400.0\n    \n    \n      2\n      trader_minmax\n      2021-01-06\n      000020\n      +\n      0.0\n      19700.0\n    \n    \n      3\n      trader_minmax\n      2021-01-07\n      000020\n      +\n      0.0\n      19700.0\n    \n    \n      4\n      trader_minmax\n      2021-01-08\n      000020\n      +\n      0.0\n      19100.0\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      139963\n      trader_div-close\n      2021-12-22\n      5307W1\n      -\n      1.0\n      3400.0\n    \n    \n      139964\n      trader_div-close\n      2021-12-23\n      5307W1\n      -\n      1.0\n      3425.0\n    \n    \n      139965\n      trader_div-close\n      2021-12-24\n      5307W1\n      -\n      1.0\n      3500.0\n    \n    \n      139966\n      trader_div-close\n      2021-12-27\n      5307W1\n      -\n      1.0\n      3475.0\n    \n    \n      139967\n      trader_div-close\n      2021-12-28\n      5307W1\n      -\n      1.0\n      3515.0\n    \n  \n\n1119744 rows Ã— 6 columns\n\n\n\n\n\n\n2) Simulation: ìˆ˜ìµë¥  ê³„ì‚°\n\ndf_history_all = sai.simulation(df_signal_all, init_budget=10000000, init_stock={}, fee=0.01)\ndf_history_all\n\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 245/246 [00:10<00:00, 24.31it/s]\n\n\n== trader_div-close completed ==\n\n\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 245/246 [00:10<00:00, 24.29it/s]\n\n\n== trader_minmax completed ==\n\n\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 245/246 [00:10<00:00, 24.32it/s]\n\n\n== trader_robust completed ==\n\n\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 245/246 [00:10<00:00, 24.29it/s]\n\n\n== trader_standard completed ==\n\n\n\n\n\n\n\n\n\n  \n    \n      \n      Trader_id\n      Sell_date\n      Budget\n      Yield\n      Stock\n    \n  \n  \n    \n      0\n      trader_div-close\n      2021-01-04\n      10000000\n      0.000000\n      {}\n    \n    \n      1\n      trader_div-close\n      2021-01-05\n      10860611\n      8.606110\n      {'006340': 7142}\n    \n    \n      2\n      trader_div-close\n      2021-01-06\n      10860611\n      8.606110\n      {}\n    \n    \n      3\n      trader_div-close\n      2021-01-07\n      10860611\n      8.606110\n      {}\n    \n    \n      4\n      trader_div-close\n      2021-01-08\n      10860611\n      8.606110\n      {}\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      241\n      trader_standard\n      2021-12-22\n      8866099\n      -11.339006\n      {}\n    \n    \n      242\n      trader_standard\n      2021-12-23\n      8866099\n      -11.339006\n      {}\n    \n    \n      243\n      trader_standard\n      2021-12-24\n      8866099\n      -11.339006\n      {}\n    \n    \n      244\n      trader_standard\n      2021-12-27\n      8866099\n      -11.339006\n      {}\n    \n    \n      245\n      trader_standard\n      2021-12-28\n      8866099\n      -11.339006\n      {}\n    \n  \n\n984 rows Ã— 5 columns\n\n\n\n\n\n\n3) ë¦¬ë”ë³´ë“œ\n\nsai.leaderboard(df_history_all)\n\n\n\n\n\n  \n    \n      \n      Trader_id\n      Yield\n    \n  \n  \n    \n      0\n      trader_div-close\n      69.888981\n    \n    \n      1\n      trader_standard\n      -11.339006\n    \n    \n      2\n      trader_robust\n      -22.930525\n    \n    \n      3\n      trader_minmax\n      -26.364585\n    \n  \n\n\n\n\n\n\n\n4) ì‹œê°í™” ê²°ê³¼\n\nsai.yield_plot(df_history_all)\n\n\n\n\ndiv-close í‘œì¤€í™” ë°©ë²•ì´ ë‹¤ë¥¸ ë°©ë²•ë“¤ë³´ë‹¤ ìˆ˜ìµë¥ ì´ ìƒë‹¹íˆ ë†’ê²Œ ë‚˜ì˜¨ ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
  },
  {
    "objectID": "posts/use_pil.html",
    "href": "posts/use_pil.html",
    "title": "PILì„ ì‚¬ìš©í•œ ì „í†µì ì¸ ì´ë¯¸ì§€ ì²˜ë¦¬ ë°©ë²•",
    "section": "",
    "text": "ì „í†µì ì¸ ì´ë¯¸ì§€ ì²˜ë¦¬ ë°©ë²•\në”¥ëŸ¬ë‹ì„ ì‚¬ìš©í•œ ì´ë¯¸ì§€ í•™ìŠµ ê¸°ë²• ì „ì—, ì—¬ëŸ¬ ìˆ˜ë§ì€ ì´ë¯¸ì§€ ì²˜ë¦¬ ê¸°ë²•ë“¤ì´ ë°œì „ë˜ì–´ ì™”ìŠµë‹ˆë‹¤. ì´ë“¤ì€ í˜„ì¬ëŠ” ë”¥ëŸ¬ë‹ ëª¨ë¸ í•™ìŠµì„ ìœ„í•œ ë°ì´í„° ê°€ê³µì— ì‚¬ìš©ë˜ê³  ìˆìŠµë‹ˆë‹¤.\nì „í†µì ì¸ ì´ë¯¸ì§€ ì²˜ë¦¬ ê¸°ë²•ì—ëŠ” ëŒ€í‘œì ìœ¼ë¡œ ë‹¤ìŒê³¼ ê°™ì´ 3ê°€ì§€ê°€ ì¡´ì¬í•©ë‹ˆë‹¤. - í˜•íƒœë³€í™˜\n\nìƒ‰ìƒë³€í™˜\ní•„í„° ë³€í™˜\n\në³¸ ê¸€ì—ì„œëŠ” ìœ„ì˜ ì„¸ê°€ì§€ ì „í†µì ì¸ ì´ë¯¸ì§€ ì²˜ë¦¬ ë°©ë²•ì— ëŒ€í•´ì„œ ì•Œì•„ë³´ê³ , ì§ì ‘ ì‹¤ìŠµí•´ë³´ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤.\n \n\n\nPIL (Pillow) ë€?\nPIL(Python Imaging Library)ì€ íŒŒì´ì¬ì—ì„œ openCVì™€ í•¨ê»˜ ìì£¼ ì‚¬ìš©ë˜ëŠ” ì´ë¯¸ì§€ ì²˜ë¦¬ ë¼ì´ë¸ŒëŸ¬ë¦¬ì…ë‹ˆë‹¤. ê¸°ì¡´ PIL ë¼ì´ë¸ŒëŸ¬ë¦¬ ê°œë°œì€ ì¤‘ë‹¨ë˜ì—ˆì§€ë§Œ, ì˜¤í”ˆì†ŒìŠ¤ë¥¼ ë³µì œí•œ Pillow ë¼ëŠ” ì´ë¦„ìœ¼ë¡œ í˜„ì¬ê¹Œì§€ ê°œë°œë˜ê³  ìˆìŠµë‹ˆë‹¤.\nì„¤ì¹˜\npip install Pillow\në¼ì´ë¸ŒëŸ¬ë¦¬ import\n\nfrom PIL import Image\n\n\n \n\n\n1. ì´ë¯¸ì§€ í™•ì¸í•˜ê¸°\n\nì´ë¯¸ì§€ ë¶ˆëŸ¬ì˜¤ê¸°\n\nimg = Image.open(\"pil_test.jfif\")\n\nimg ë³€ìˆ˜ì—ëŠ” PILì˜ Image í´ë˜ìŠ¤ ê°ì²´ê°€ ì €ì¥ë©ë‹ˆë‹¤. from PIL import Imageì—ì„œ ë¶ˆëŸ¬ì˜¨ Imageì™€, PILì˜ Image í´ë˜ìŠ¤ëŠ” ë‹¤ë¥¸ ì¡´ì¬ì…ë‹ˆë‹¤. ì§€ê¸ˆë¶€í„° ì‚¬ìš©í•˜ëŠ” ë©”ì„œë“œëŠ” ëª¨ë‘ PILì˜ Image í´ë˜ìŠ¤ì— êµ¬í˜„ëœ í•¨ìˆ˜ì„ì— ìœ ì˜í•˜ë„ë¡ í•©ë‹ˆë‹¤.\n\n\n\nì´ë¯¸ì§€ ì‹œê°í™”\n\nimport matplotlib.pyplot as plt \n\nplt.imshow(img)\n\n<matplotlib.image.AxesImage at 0x7f78cc261dc0>\n\n\n\n\n\nì´ë¯¸ì§€ ì‹œê°í™”ì—ëŠ” matplotlibì˜ imshow ë©”ì„œë“œë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. ì´ë¯¸ì§€ëŠ” ê·¼ìœ¡ì„ ìë‘í•˜ëŠ” ì§±êµ¬ë¡œ ì¤€ë¹„í•´ë³´ì•˜ìŠµë‹ˆë‹¤.\n\n\n\nì´ë¯¸ì§€ ì •ë³´ í™•ì¸\n\nprint(f\"Size: {img.size}, Mode: {img.mode}\")\n\nSize: (583, 587), Mode: RGB\n\n\nì´ë¯¸ì§€ì˜ ì‚¬ì´ì¦ˆëŠ” Image ê°ì²´ì•ˆì˜ size ì†ì„±ì—, ì»¬ëŸ¬ëŠ” mode ì†ì„±ì— ì €ì¥ë˜ì–´ìˆìŠµë‹ˆë‹¤. ì´ ì§±êµ¬ ì‚¬ì§„ì˜ í¬ê¸°ëŠ” ê°€ë¡œ 583, ì„¸ë¡œ 587 í”½ì…€ë¡œ ì´ë£¨ì–´ì ¸ ìˆê³ , ê° í”½ì…€ì€ RGB ì±„ë„ì„ ê°€ì§€ê³  ìˆìŒì„ í™•ì¸í•˜ì˜€ìŠµë‹ˆë‹¤.\n \n\n\n\n2. í˜•íƒœ ë³€í™˜\ní˜•íƒœ ë³€í™˜ì€ ë§ê·¸ëŒ€ë¡œ ì´ë¯¸ì§€ì˜ í˜•íƒœë¥¼ ë³€í™˜í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. ê·¸ ì˜ˆì‹œë¡œ, ì˜ë¼ë‚´ê¸° (crop), íšŒì „ (rotate), í¬ê¸° ë³€ê²½ (resize) ì´ ìˆìŠµë‹ˆë‹¤.\n\n2.1 ì´ë¯¸ì§€ ì˜ë¼ë‚´ê¸° (crop)\nì˜ë¼ë‚´ê¸° (crop)ì€ ì „ì²´ ì´ë¯¸ì§€ ì¤‘ íŠ¹ì • ë¶€ë¶„ë§Œ ì˜ë¼ë‚´ëŠ” ì‘ì—…ì…ë‹ˆë‹¤. crop ë©”ì„œë“œë¥¼ í†µí•´ êµ¬í˜„í•  ìˆ˜ ìˆìœ¼ë©°, ì´ë¯¸ì§€ ì¢Œí‘œê³„ì— ë”°ë¼, 4ê°œ ì¢Œí‘œë¥¼ íŠœí”Œ í˜•íƒœë¡œ ì…ë ¥í•˜ë©´ ë©ë‹ˆë‹¤.\në”°ë¼ì„œ ì´ë¯¸ì§€ì˜ ì¢Œí‘œê³„ì— ëŒ€í•˜ì—¬ ì´í•´í•˜ê³  ìˆì–´ì•¼ í•©ë‹ˆë‹¤. í•´ë‹¹ ì´ë¯¸ì§€ ì¢Œí‘œê³„ëŠ” ì›ì  (0, 0)ì´ ì¢Œì¸¡ ìµœìƒë‹¨ì— ì¡´ì¬í•©ë‹ˆë‹¤. ê·¸ë ‡ê¸° ë•Œë¬¸ì— xì¶•ì€ ì›ì ì„ ê¸°ì¤€ìœ¼ë¡œ ì˜¤ë¥¸ìª½ìœ¼ë¡œ, yì¶•ì€ ì›ì ì„ ê¸°ì¤€ìœ¼ë¡œ ì•„ë˜ìª½ìœ¼ë¡œ ê°’ì´ ì»¤ì§„ë‹¤ê³  ìƒê°í•˜ë©´ ë©ë‹ˆë‹¤.\ncrop ë©”ì„œë“œì— ë“¤ì–´ê°ˆ íŠœí”Œì€ (x1, y1, x2, y2) ì˜ í˜•íƒœë¡œ ë“¤ì–´ê°€ê²Œë˜ëŠ”ë°, í¬ë¡­ í•  ë„¤ëª¨ë°•ìŠ¤ ê¸°ì¤€ìœ¼ë¡œ ì¢Œì¸¡ ìƒë‹¨ ì¢Œí‘œ (x1, y1), ìš°ì¸¡ í•˜ë‹¨ ì¢Œí‘œ (x2, y2)ë¥¼ ì˜ë¯¸í•©ë‹ˆë‹¤.\n\nplt.imshow(img.crop((10, 50, 460, 350)))\n\n<matplotlib.image.AxesImage at 0x7f78b4f110d0>\n\n\n\n\n\nì§±êµ¬ì˜ ì–¼êµ´ë§Œ ì˜ë¼ë‚¸ ê²°ê³¼ì…ë‹ˆë‹¤.\n\n\n\n2.2 ì´ë¯¸ì§€ íšŒì „ (rotate)\nì´ë¯¸ì§€ íšŒì „ì€ ì´ë¯¸ì§€ë¥¼ ì‹œê³„ë°©í–¥ì´ë‚˜ ë°˜ì‹œê³„ ë°©í–¥ìœ¼ë¡œ ì¼ì • ê°ë„ë§Œí¼ ëŒë¦¬ëŠ” ê²ƒì…ë‹ˆë‹¤. rotate ë©”ì„œë“œì— íšŒì „í•˜ê³ ì í•˜ëŠ” ê°ë„ë¥¼ ë„£ìœ¼ë©´, í•´ë‹¹ ê°ë„ë§Œí¼ ë°˜ì‹œê³„ ë°©í–¥ìœ¼ë¡œ íšŒì „í•œ ê²°ê³¼ê°€ ë‚˜ì˜¤ê²Œë©ë‹ˆë‹¤.\n\nplt.imshow(img.rotate(30))\n\n<matplotlib.image.AxesImage at 0x7f78b4ebb130>\n\n\n\n\n\n\nplt.imshow(img.rotate(60, expand=True))\n\n<matplotlib.image.AxesImage at 0x7f78b52d7d00>\n\n\n\n\n\nì´ë¯¸ì§€ê°€ ì˜ë¦¬ëŠ” ê²ƒì„ ë§‰ê¸° ìœ„í•´ expand=Trueë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\nplt.imshow(img.rotate(270))\n\n<matplotlib.image.AxesImage at 0x7f78b51aecd0>\n\n\n\n\n\nì‹œê³„ë°©í–¥ìœ¼ë¡œ ëŒë¦¬ê³  ì‹¶ë‹¤ë©´, ëŒë¦¬ê³ ì‹¶ì€ ê°ë„ë§Œí¼ 360ì—ì„œ ìˆ˜ë¥¼ ë¹¼ì£¼ë©´ ë©ë‹ˆë‹¤.\n\n\n\n2.3 ì´ë¯¸ì§€ í¬ê¸° ë° ë¹„ìœ¨ ë³€í™˜\nì´ë¯¸ì§€ì˜ ê°€ë¡œ, ì„¸ë¡œ ê¸¸ì´ë¥¼ ë³€í™”ì‹œí‚¤ëŠ” ì‘ì—…ì…ë‹ˆë‹¤. resize ë©”ì„œë“œì— ë°”ê¾¸ê³ ì í•˜ëŠ” ê°€ë¡œ, ì„¸ë¡œ í”½ì…€ ê¸¸ì´ë¥¼ íŠœí”Œë¡œ ë„£ì–´ì£¼ë©´ ë©ë‹ˆë‹¤.\n\nimg_resized = img.resize((150, 300))\nplt.imshow(img_resized)\n\n<matplotlib.image.AxesImage at 0x7f78b49ac0a0>\n\n\n\n\n\n\n\n\n2.4 ì „ë‹¨ ë³€í™˜ (Shearing)\nì›ë˜ëŠ” ì‚¬ê°í˜•ì˜ í˜•íƒœì˜€ë˜ ì´ë¯¸ì§€ë¥¼ í‰í–‰ì‚¬ë³€í˜• ê¼´ë¡œ ë§Œë“œëŠ” ë³€í™˜ì…ë‹ˆë‹¤. ì—¬ê¸°ì—ëŠ” transform ë©”ì„œë“œë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.\nImage.transform(size, method, data=None, resample=Resampling.NEAREST, fill=1, fillcolor=None)\n\nsize: ì¶œë ¥ë  ì´ë¯¸ì§€ì˜ í¬ê¸°\nmethod: ë³€í™˜ì˜ ì¢…ë¥˜ë¥¼ ì§€ì • (ì „ë‹¨ ë³€í™˜ì—ëŠ” ì•„í•€ë³€í™˜[Affine Transform]ì„ ì‚¬ìš©í•´ì•¼ í•˜ë¯€ë¡œ Image.AFFINEì„ ì§€ì •í•©ë‹ˆë‹¤)\ndata: ì ì ˆí•œ ê°’ì„ ë„£ì–´ì£¼ì–´ì•¼ í•¨. (ì„ í˜•ëŒ€ìˆ˜í•™ì´ í•„ìš”í•¨)\n\nê³µì‹ë¬¸ì„œ\n\nplt.imshow(img.transform((int(img.size[0] * 1.5), img.size[1]), Image.AFFINE, (1, -0.5, 0, 0, 1, 0)))\n\n<matplotlib.image.AxesImage at 0x7f78b49d4fa0>\n\n\n\n\n\n\nplt.imshow(img.transform((int(img.size[0] * 1.2), img.size[1]), Image.AFFINE, (1, -0.2, 0, 0, 1, 0)))\n\n<matplotlib.image.AxesImage at 0x7f78b4b317f0>\n\n\n\n\n\në‘ ê°€ì§€ ë°©ë²•ì˜ ì°¨ì´ì ì€ ì²«ë²ˆì§¸ íŒŒë¼ë¯¸í„° (size) ì˜ ì²«ë²ˆì§¸ ê°’ì— ê³±í•´ì£¼ëŠ” ê°’ê³¼ ì„¸ë²ˆì§¸ íŒŒë¼ë¯¸í„° (data)ì˜ ì²«ë²ˆì§¸ ê°’ ì…ë‹ˆë‹¤. sizeì—ì„œ ê³±í•´ì£¼ëŠ” ê°’ì€ì¶œë ¥ í¬ê¸°ë¥¼ ë§ì¶°ì£¼ëŠ” ì¥ì¹˜ì¼ ë¿ì´ê³ , dataíŒŒë¼ë¯¸í„°ì˜ ë‘ë²ˆì§¸ ê°’ì´ ì „ë‹¨ ë³€í™˜ì˜ ì •ë„, ì¦‰ í‰í–‰ì‚¬ë³€í˜•ì˜ ê¸°ìš¸ê¸°ë¥¼ ì˜ë¯¸í•©ë‹ˆë‹¤.\n \n\n\n\n3. ìƒ‰ìƒ ë³€í™˜\nìƒ‰ìƒ ë³€í™˜ì—ëŠ” ëŒ€í‘œì ìœ¼ë¡œ ë°ê¸°(Brightness) ë³€í™”, ëŒ€ì¡°(Contrast)ë³€í™”, í‘ë°± (Grayscale)ë³€í™” ë“±ì´ ìˆìŠµë‹ˆë‹¤. ì—¬ê¸°ì—ëŠ” Pillowì˜ ImageEnhanceë¼ëŠ” ëª¨ë“ˆì„ ì‚¬ìš©í•©ë‹ˆë‹¤.\n\nfrom PIL import ImageEnhance \n\n\n\n3.1 ë°ê¸° ë³€í™”\n\nbright_enhancer = ImageEnhance.Brightness(img)\n\në¨¼ì €, ImageEnhanceëª¨ë“ˆì˜ Brightness í´ë˜ìŠ¤ë¥¼ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤. ê·¸ëŸ¬ë©´ img ê°ì²´ì— ë°ê¸° ì¡°ì ˆì„ í•  ìˆ˜ ìˆëŠ” Enhancer ê°ì²´ê°€ ë§Œë“¤ì–´ì§‘ë‹ˆë‹¤. ì´ ê°ì²´ì— enhanceë¼ëŠ”ì´ë¦„ì˜ ë©”ì„œë“œë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. ì–¼ë§ˆë‚˜ ë°ê¸° ì¡°ì ˆì„ í• ì§€, ë°°ìˆ˜ë¥¼ ë„£ì–´ì£¼ë©´ ë©ë‹ˆë‹¤.\n\nplt.imshow(bright_enhancer.enhance(2))\n\n<matplotlib.image.AxesImage at 0x7f78b503aac0>\n\n\n\n\n\në°ê¸°ë¥¼ 2ë°° ì˜¬ë ¤ì¤€ ì´ë¯¸ì§€ê°€ ë‚˜ì˜¨ ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\nplt.imshow(bright_enhancer.enhance(0.5))\n\n<matplotlib.image.AxesImage at 0x7f78b4a92b50>\n\n\n\n\n\nì´ë²ˆì—” 0.5ë°°ë¡œ ë°ê²Œ, ì¦‰, ë‘ë°° ì–´ë‘¡ê²Œ ë§Œë“¤ì–´ ì¤€ ì˜ˆì œì…ë‹ˆë‹¤.\n\n\n\n3.2 ëŒ€ì¡° ë³€í™”\nëŒ€ì¡° ë³€í™”ëŠ” ImageEnhance ëª¨ë“ˆì—ì„œ Contrast í´ë˜ìŠ¤ë¥¼ ì‚¬ìš©í•˜ë©°, ì‚¬ìš©ë²•ì€ ë°ê¸° ë³€í™”ì™€ ë™ì¼í•©ë‹ˆë‹¤.\n\ncontrast_enhancer = ImageEnhance.Contrast(img)\nplt.imshow(contrast_enhancer.enhance(2))\n\n<matplotlib.image.AxesImage at 0x7f78b4a8c9d0>\n\n\n\n\n\nëŒ€ì¡°ë¥¼ 2ë°° ê°•í•˜ê²Œ ë§Œë“¤ì—ˆìŠµë‹ˆë‹¤.\n\nplt.imshow(contrast_enhancer.enhance(0.5))\n\n<matplotlib.image.AxesImage at 0x7f78b48a5490>\n\n\n\n\n\nëŒ€ì¡°ë¥¼ 2ë°° ì•½í•˜ê²Œ ë§Œë“¤ì—ˆìŠµë‹ˆë‹¤.\n\n\n\n3.3 í‘ë°± ë³€í™”\nì´ë¯¸ì§€ ì»¬ëŸ¬ ì—¬ë¶€ë¥¼ mode ì†ì„±ì— RGB ì„ì´ ì €ì¥ë˜ì–´ìˆì—ˆëŠ”ë°, ì´ modeë¥¼ í‘ë°±ìœ¼ë¡œ ë³€ê²½í•´ì£¼ëŠ” ì‘ì—…ì…ë‹ˆë‹¤.\nê³µì‹ë¬¸ì„œ ì—ì„œ ëª¨ë“œë“¤ì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\nimg_gray = img.convert(\"L\")\nprint(\"í‘ë°±ì‚¬ì§„ ëª¨ë“œ: \", img_gray.mode)\nplt.imshow(img_gray, cmap=plt.get_cmap(\"gray\"))\n\ní‘ë°±ì‚¬ì§„ ëª¨ë“œ:  L\n\n\n<matplotlib.image.AxesImage at 0x7f78b4520fd0>\n\n\n\n\n\n \n\n\n\n4. í•„í„° ë³€í™˜\ní•„í„°ëŠ” í¬í† ìƒµì´ë‚˜ ìŠ¤ë§ˆíŠ¸í°ì—ì„œ ì‚¬ì§„ ë³´ì •ì„ ìœ„í•´ ì ìš©í•˜ëŠ” í•„í„°ì™€ ë¹„ìŠ·í•˜ë‹¤ê³  ìƒê°í•˜ë©´ ë©ë‹ˆë‹¤. ëŒ€í‘œì ìœ¼ë¡œ ìƒ¤í”„ë‹(Sharpening), ë¸”ëŸ¬ (Blur), ê²½ê³„ì„  íƒì§€ (Edge Detection) ì´ ìˆìŠµë‹ˆë‹¤. filter ë©”ì„œë“œì—ì„œ ì ìš©í•  í•„í„° ì¢…ë¥˜ë¥¼ íŒŒë¼ë¯¸í„°ë¡œ ë„£ì–´ì£¼ì–´ì•¼ í•©ë‹ˆë‹¤. ì´ëŠ” ImageFilterë¼ëŠ” ë³„ë„ ëª¨ë“ˆì— êµ¬í˜„ë˜ì–´ ìˆìŠµë‹ˆë‹¤.\nê³µì‹ë¬¸ì„œ ì—ì„œ í•„í„°ì˜ ì¢…ë¥˜ë¥¼ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\n4.1 ìƒ¤í”„ë‹ (Sharpening)\nì´ë¯¸ì§€ì˜ ì§ˆê°‘ì„ ë‚ ì¹´ë¡­ê²Œ ë§Œë“¤ì–´ì£¼ëŠ” ì‘ì—…ì…ë‹ˆë‹¤. ì›ë³¸ ì‚¬ì§„ì´ íë¦¿í•  ê²½ìš° ì–´ëŠ ì •ë„ì˜ í™”ì§ˆ ê°œì„ ì˜ íš¨ê³¼ë¥¼ ì–»ì„ ìˆ˜ ìˆì§€ë§Œ, ê³¼í•˜ê²Œ ì ìš©í•˜ë©´ ì´ë¯¸ì§€ì˜ ìê¸€ìê¸€í•œ ë…¸ì´ì¦ˆê°€ ë¶€ê°ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\nPillowì˜ ImageFilter ëª¨ë“ˆì˜ SHARPEN ì„ ë™íƒœ ìƒ¤í”„ë‹ì„ ìˆ˜í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\nfrom PIL import ImageFilter\n\nplt.imshow(img.filter(ImageFilter.SHARPEN))\n\n<matplotlib.image.AxesImage at 0x7f78b44836a0>\n\n\n\n\n\nìƒ¤í”„ë‹ ì „ê³¼ í° ì°¨ì´ê°€ ì—†ì–´ë³´ì´ë¯€ë¡œ ì´ëŸ´ ë•ŒëŠ” ì—¬ëŸ¬ ë²ˆ ì‚¬ìš©í•´ë³¼ ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤.\n\nimg_sharpen = img.filter(ImageFilter.SHARPEN)\nimg_sharpen = img_sharpen.filter(ImageFilter.SHARPEN)\nimg_sharpen = img_sharpen.filter(ImageFilter.SHARPEN)\nplt.imshow(img_sharpen)\n\n<matplotlib.image.AxesImage at 0x7f78b4a3feb0>\n\n\n\n\n\n\n\n\n4.2 ë¸”ëŸ¬ (Blur)\nìƒ¤í”„ë‹ê³¼ ë°˜ëŒ€ì˜ ê°œë…ìœ¼ë¡œ, ì´ë¯¸ì§€ë¥¼ íë¦¿í•˜ê²Œ ë§Œë“¤ì–´ì£¼ëŠ” ì‘ì—…ì…ë‹ˆë‹¤. ì˜ˆì‹œë¡œ, ë°°ê²½ì— ë¸”ëŸ¬ ì²˜ë¦¬ë¥¼ í•˜ì—¬ ì‚¬ì§„ì— ì°íŒ ëŒ€ìƒë¬¼ì„ ë¶€ê°ì‹œí‚¤ëŠ” ì‘ì—…ì—ì„œ í”íˆ ì‚¬ìš©í•©ë‹ˆë‹¤. ìŠ¤ë§ˆíŠ¸í° ì¹´ë©”ë¼ì˜ ì¸ë¬¼ëª¨ë“œë¡œ, ë°°ê²½ì— ìë™ìœ¼ë¡œ ë¸”ëŸ¬ë¥¼ ì…í˜€ì£¼ëŠ” ê¸°ëŠ¥ê³¼ ê°™ìŠµë‹ˆë‹¤.\në¸”ëŸ¬ëŠ” ImageFilterì˜ BLURë¥¼ ì‚¬ìš©í•˜ë©´ ë©ë‹ˆë‹¤. ì‚¬ìš©ë²•ì€ ìƒ¤í”„ë‹ê³¼ ë™ì¼í•˜ê²Œ ì§„í–‰í•˜ë©´ ë©ë‹ˆë‹¤.\n\nimg_blur = img.filter(ImageFilter.BLUR)\nimg_blur = img_blur.filter(ImageFilter.BLUR)\nimg_blur = img_blur.filter(ImageFilter.BLUR)\nplt.imshow(img_blur)\n\n<matplotlib.image.AxesImage at 0x7f78b4afad60>\n\n\n\n\n\n\n\n\n5.3 ê²½ê³„ì„  ê°ì§€\nê²½ê³„ì„  ê°ì§€ëŠ” ì´ë¯¸ì§€ì˜ ê²½ê³„ì„ ì„ ì°¾ì•„ì£¼ëŠ” ì‘ì—…ì…ë‹ˆë‹¤. ê²½ê³„ì„ ì€ ì´ë¯¸ì§€ ë‚´ì—ì„œ ìƒ‰ì˜ ë³€í™”ê°€ ê¸‰ê²©í•œ ì„  ì´ë¼ê³  í•´ì„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\nImageFilterì˜ FIND_EDGESë¥¼ í†µí•´ ê²½ê³„ì„ ì„ ì°¾ì•„ë‚´ë„ë¡ í•©ë‹ˆë‹¤.\n\nplt.imshow(img.filter(ImageFilter.FIND_EDGES))\n\n<matplotlib.image.AxesImage at 0x7f78b43c3eb0>\n\n\n\n\n\nê²€ì • ì„ ìœ¼ë¡œ ê·¸ë ¤ì§„ ìºë¦­í„°ë¼ ê·¸ëŸ°ì§€ ê²½ê³„ì„ ì´ ë” ì˜ ê°ì§€ë˜ëŠ” ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n \në³¸ ê¸€ì€ Elice ì´ë¯¸ì§€ ì²˜ë¦¬ - [ì´ë¡ ] PIL ì•Œì•„ë³´ê¸° ê°•ì˜ë¥¼ ì°¸ê³ í•˜ì—¬ ì •ë¦¬í•œ ê¸€ì…ë‹ˆë‹¤."
  },
  {
    "objectID": "posts/ìƒê¶Œì •ë³´ë¶„ì„.html",
    "href": "posts/ìƒê¶Œì •ë³´ë¶„ì„.html",
    "title": "ìƒê¶Œì •ë³´ ì‹œê°í™” ë¶„ì„",
    "section": "",
    "text": "ë°ì´í„° ì¶œì²˜ : ê³µê³µë°ì´í„°í¬í„¸: ì†Œìƒê³µì¸ì‹œì¥ì§„í¥ê³µë‹¨_ìƒê°€(ìƒê¶Œ)ì •ë³´\n\nì´ë²ˆ ë°ì´í„° ì‹œê°í™” ë¶„ì„ì—ì„œëŠ” ê³µê³µë°ì´í„°í¬í„¸ì˜ ìƒê¶Œì •ë³´ ë°ì´í„°ë¥¼ ì‚¬ìš©í•˜ì—¬ ì„œìš¸ì‹œì— ìˆëŠ” ìƒê¶Œ ì—…ì¢… ë¶„ë¥˜ ë³„ë¡œ ì‚´í´ë³´ë©° ì„œìš¸ì‹œ ìƒê¶Œì—…ì¢… í˜„í™©ì„ ë¶„ì„í•´ë³´ê³ , ê·¸ ì¤‘ì—ì„œë„ ë‘ ê°€ì§€ ë¶„ë¥˜(í•™ì›, ë² ì´ì»¤ë¦¬)ë¥¼ ì§‘ì¤‘ì ìœ¼ë¡œ ë¶„ì„í•´ë³´ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤.\n* ì „ê³µ ìˆ˜ì—…ì‹œê°„ì— ì§„í–‰í•œ ë‚´ìš©ì„ í† ëŒ€ë¡œ ê°€ì„¤ ì„¤ì •ê³¼ í•´ì„, ì‹œê°í™” ë””ìì¸ì„ ì¬êµ¬ì„±í•œ ê¸€ì…ë‹ˆë‹¤.\n\n\n\nimport sys\nprint('python', sys.version)\n\nimport numpy as np\nprint('numpy', np.__version__)\n\nimport pandas as pd\nprint('pandas', pd.__version__)\n\nimport matplotlib as mpl\nprint('matplotlib', mpl.__version__)\n\nimport matplotlib.pyplot as plt\nplt.rc(\"font\", family=\"Malgun Gothic\", size=15) \nplt.rc(\"axes\", unicode_minus=False) # x,yì¶• (-)ë¶€í˜¸ í‘œì‹œ\n\n# ë ˆí‹°ë‚˜ ë””ìŠ¤í”Œë ˆì´ë¡œ í°íŠ¸ê°€ ì„ ëª…í•˜ê²Œ í‘œì‹œë˜ë„ë¡ í•©ë‹ˆë‹¤.\nfrom IPython.display import set_matplotlib_formats\nset_matplotlib_formats(\"retina\")\n\nimport seaborn as sns\nprint('pandas', sns.__version__)\n\nfrom matplotlib.ticker import MaxNLocator\n\n# ê²°ê³¼ í™•ì¸ì„ ìš©ì´í•˜ê²Œ í•˜ê¸° ìœ„í•œ ì½”ë“œ\nfrom IPython.core.interactiveshell import InteractiveShell\nInteractiveShell.ast_node_interactivity = 'all'\n\n# dataframe 39 column ê¹Œì§€ í‘œì‹œ\npd.options.display.max_columns=39\n\npython 3.7.3 (default, Mar 27 2019, 17:13:21) [MSC v.1915 64 bit (AMD64)]\nnumpy 1.21.6\npandas 1.3.5\nmatplotlib 3.3.2\npandas 0.12.1\n\n\n\n\n\n\ndf = pd.read_csv(\"11w/data/ìƒê°€ì—…ì†Œì •ë³´_201912_01_small.csv\", sep='|')\n\ndf.head(3)\n\n\n\n\n\n  \n    \n      \n      ìƒê°€ì—…ì†Œë²ˆí˜¸\n      ìƒí˜¸ëª…\n      ì§€ì ëª…\n      ìƒê¶Œì—…ì¢…ëŒ€ë¶„ë¥˜ì½”ë“œ\n      ìƒê¶Œì—…ì¢…ëŒ€ë¶„ë¥˜ëª…\n      ìƒê¶Œì—…ì¢…ì¤‘ë¶„ë¥˜ì½”ë“œ\n      ìƒê¶Œì—…ì¢…ì¤‘ë¶„ë¥˜ëª…\n      ìƒê¶Œì—…ì¢…ì†Œë¶„ë¥˜ì½”ë“œ\n      ìƒê¶Œì—…ì¢…ì†Œë¶„ë¥˜ëª…\n      í‘œì¤€ì‚°ì—…ë¶„ë¥˜ì½”ë“œ\n      í‘œì¤€ì‚°ì—…ë¶„ë¥˜ëª…\n      ì‹œë„ì½”ë“œ\n      ì‹œë„ëª…\n      ì‹œêµ°êµ¬ì½”ë“œ\n      ì‹œêµ°êµ¬ëª…\n      í–‰ì •ë™ì½”ë“œ\n      í–‰ì •ë™ëª…\n      ë²•ì •ë™ì½”ë“œ\n      ë²•ì •ë™ëª…\n      ì§€ë²ˆì½”ë“œ\n      ëŒ€ì§€êµ¬ë¶„ì½”ë“œ\n      ëŒ€ì§€êµ¬ë¶„ëª…\n      ì§€ë²ˆë³¸ë²ˆì§€\n      ì§€ë²ˆë¶€ë²ˆì§€\n      ì§€ë²ˆì£¼ì†Œ\n      ë„ë¡œëª…ì½”ë“œ\n      ë„ë¡œëª…\n      ê±´ë¬¼ë³¸ë²ˆì§€\n      ê±´ë¬¼ë¶€ë²ˆì§€\n      ê±´ë¬¼ê´€ë¦¬ë²ˆí˜¸\n      ê±´ë¬¼ëª…\n      ë„ë¡œëª…ì£¼ì†Œ\n      êµ¬ìš°í¸ë²ˆí˜¸\n      ì‹ ìš°í¸ë²ˆí˜¸\n      ë™ì •ë³´\n      ì¸µì •ë³´\n      í˜¸ì •ë³´\n      ê²½ë„\n      ìœ„ë„\n    \n  \n  \n    \n      0\n      21817342\n      ê¸°ì•„ìë™ì°¨\n      ì¤‘ë‘ì§€ì \n      D\n      ì†Œë§¤\n      D23\n      ìë™ì°¨/ìë™ì°¨ìš©í’ˆ\n      D23A01\n      ìë™ì°¨íŒë§¤\n      G45110\n      ìë™ì°¨ ì‹ í’ˆ íŒë§¤ì—…\n      11\n      ì„œìš¸íŠ¹ë³„ì‹œ\n      11260\n      ì¤‘ë‘êµ¬\n      1126060000\n      ì¤‘í™”1ë™\n      1126010300\n      ì¤‘í™”ë™\n      1126010300202860025\n      1\n      ëŒ€ì§€\n      286\n      25.0\n      ì„œìš¸íŠ¹ë³„ì‹œ ì¤‘ë‘êµ¬ ì¤‘í™”ë™ 286-25\n      112603000001\n      ì„œìš¸íŠ¹ë³„ì‹œ ì¤‘ë‘êµ¬ ë™ì¼ë¡œ\n      802\n      NaN\n      1126010300102860025013386\n      ëŒ€ì‹ ë¹Œë”©\n      ì„œìš¸íŠ¹ë³„ì‹œ ì¤‘ë‘êµ¬ ë™ì¼ë¡œ 802\n      131120\n      2051.0\n      NaN\n      NaN\n      NaN\n      127.079691\n      37.602078\n    \n    \n      1\n      20614940\n      ë„ë¯¸ë…¸í”¼ì\n      ìš©ì‚°ì \n      Q\n      ìŒì‹\n      Q07\n      íŒ¨ìŠ¤íŠ¸í‘¸ë“œ\n      Q07A04\n      íŒ¨ìŠ¤íŠ¸í‘¸ë“œ\n      I56199\n      ê·¸ì™¸ ê¸°íƒ€ ìŒì‹ì ì—…\n      11\n      ì„œìš¸íŠ¹ë³„ì‹œ\n      11170\n      ìš©ì‚°êµ¬\n      1117056000\n      ì›íš¨ë¡œ1ë™\n      1117011300\n      ì›íš¨ë¡œ2ê°€\n      1117011300200030004\n      1\n      ëŒ€ì§€\n      3\n      4.0\n      ì„œìš¸íŠ¹ë³„ì‹œ ìš©ì‚°êµ¬ ì›íš¨ë¡œ2ê°€ 3-4\n      111703102007\n      ì„œìš¸íŠ¹ë³„ì‹œ ìš©ì‚°êµ¬ ì›íš¨ë¡œ\n      210\n      1.0\n      1117011300100030004018579\n      NaN\n      ì„œìš¸íŠ¹ë³„ì‹œ ìš©ì‚°êµ¬ ì›íš¨ë¡œ 210-1\n      140847\n      4368.0\n      NaN\n      1\n      NaN\n      126.965501\n      37.537384\n    \n    \n      2\n      16108153\n      ì¼€ì´ì•„ì´ì—í”„ì•¤ë¹„\n      NaN\n      D\n      ì†Œë§¤\n      D01\n      ìŒ/ì‹ë£Œí’ˆì†Œë§¤\n      D01A01\n      ì‹ë£Œí’ˆì \n      G47219\n      ê¸°íƒ€ ì‹ë£Œí’ˆ ì†Œë§¤ì—…\n      26\n      ë¶€ì‚°ê´‘ì—­ì‹œ\n      26110\n      ì¤‘êµ¬\n      2611056000\n      ë¶€í‰ë™\n      2611012300\n      ë¶€í‰ë™1ê°€\n      2611012300200290079\n      1\n      ëŒ€ì§€\n      29\n      79.0\n      ë¶€ì‚°ê´‘ì—­ì‹œ ì¤‘êµ¬ ë¶€í‰ë™1ê°€ 29-79\n      261104175232\n      ë¶€ì‚°ê´‘ì—­ì‹œ ì¤‘êµ¬ ì¤‘êµ¬ë¡œ29ë²ˆê¸¸\n      22\n      NaN\n      2611012300100290079003926\n      NaN\n      ë¶€ì‚°ê´‘ì—­ì‹œ ì¤‘êµ¬ ì¤‘êµ¬ë¡œ29ë²ˆê¸¸ 22\n      600804\n      48978.0\n      NaN\n      2\n      NaN\n      129.026690\n      35.100566\n    \n  \n\n\n\n\n \n\n\n\n\n\n\nprint(\"== í¬ê¸° í™•ì¸ ==\")\ndf.shape\n\nprint()\nprint(\"== ì»¬ëŸ¼ í™•ì¸ ==\")\ndf.columns\n\nprint()\nprint(\"== ì •ë³´ í™•ì¸ ==\")\ndf.info()\n\n== í¬ê¸° í™•ì¸ ==\n\n\n(150000, 39)\n\n\n\n== ì»¬ëŸ¼ í™•ì¸ ==\n\n\nIndex(['ìƒê°€ì—…ì†Œë²ˆí˜¸', 'ìƒí˜¸ëª…', 'ì§€ì ëª…', 'ìƒê¶Œì—…ì¢…ëŒ€ë¶„ë¥˜ì½”ë“œ', 'ìƒê¶Œì—…ì¢…ëŒ€ë¶„ë¥˜ëª…', 'ìƒê¶Œì—…ì¢…ì¤‘ë¶„ë¥˜ì½”ë“œ',\n       'ìƒê¶Œì—…ì¢…ì¤‘ë¶„ë¥˜ëª…', 'ìƒê¶Œì—…ì¢…ì†Œë¶„ë¥˜ì½”ë“œ', 'ìƒê¶Œì—…ì¢…ì†Œë¶„ë¥˜ëª…', 'í‘œì¤€ì‚°ì—…ë¶„ë¥˜ì½”ë“œ', 'í‘œì¤€ì‚°ì—…ë¶„ë¥˜ëª…', 'ì‹œë„ì½”ë“œ',\n       'ì‹œë„ëª…', 'ì‹œêµ°êµ¬ì½”ë“œ', 'ì‹œêµ°êµ¬ëª…', 'í–‰ì •ë™ì½”ë“œ', 'í–‰ì •ë™ëª…', 'ë²•ì •ë™ì½”ë“œ', 'ë²•ì •ë™ëª…', 'ì§€ë²ˆì½”ë“œ',\n       'ëŒ€ì§€êµ¬ë¶„ì½”ë“œ', 'ëŒ€ì§€êµ¬ë¶„ëª…', 'ì§€ë²ˆë³¸ë²ˆì§€', 'ì§€ë²ˆë¶€ë²ˆì§€', 'ì§€ë²ˆì£¼ì†Œ', 'ë„ë¡œëª…ì½”ë“œ', 'ë„ë¡œëª…', 'ê±´ë¬¼ë³¸ë²ˆì§€',\n       'ê±´ë¬¼ë¶€ë²ˆì§€', 'ê±´ë¬¼ê´€ë¦¬ë²ˆí˜¸', 'ê±´ë¬¼ëª…', 'ë„ë¡œëª…ì£¼ì†Œ', 'êµ¬ìš°í¸ë²ˆí˜¸', 'ì‹ ìš°í¸ë²ˆí˜¸', 'ë™ì •ë³´', 'ì¸µì •ë³´',\n       'í˜¸ì •ë³´', 'ê²½ë„', 'ìœ„ë„'],\n      dtype='object')\n\n\n\n== ì •ë³´ í™•ì¸ ==\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 150000 entries, 0 to 149999\nData columns (total 39 columns):\n #   Column     Non-Null Count   Dtype  \n---  ------     --------------   -----  \n 0   ìƒê°€ì—…ì†Œë²ˆí˜¸     150000 non-null  int64  \n 1   ìƒí˜¸ëª…        150000 non-null  object \n 2   ì§€ì ëª…        19939 non-null   object \n 3   ìƒê¶Œì—…ì¢…ëŒ€ë¶„ë¥˜ì½”ë“œ  150000 non-null  object \n 4   ìƒê¶Œì—…ì¢…ëŒ€ë¶„ë¥˜ëª…   150000 non-null  object \n 5   ìƒê¶Œì—…ì¢…ì¤‘ë¶„ë¥˜ì½”ë“œ  150000 non-null  object \n 6   ìƒê¶Œì—…ì¢…ì¤‘ë¶„ë¥˜ëª…   150000 non-null  object \n 7   ìƒê¶Œì—…ì¢…ì†Œë¶„ë¥˜ì½”ë“œ  150000 non-null  object \n 8   ìƒê¶Œì—…ì¢…ì†Œë¶„ë¥˜ëª…   150000 non-null  object \n 9   í‘œì¤€ì‚°ì—…ë¶„ë¥˜ì½”ë“œ   141065 non-null  object \n 10  í‘œì¤€ì‚°ì—…ë¶„ë¥˜ëª…    141065 non-null  object \n 11  ì‹œë„ì½”ë“œ       150000 non-null  int64  \n 12  ì‹œë„ëª…        150000 non-null  object \n 13  ì‹œêµ°êµ¬ì½”ë“œ      150000 non-null  int64  \n 14  ì‹œêµ°êµ¬ëª…       150000 non-null  object \n 15  í–‰ì •ë™ì½”ë“œ      150000 non-null  int64  \n 16  í–‰ì •ë™ëª…       150000 non-null  object \n 17  ë²•ì •ë™ì½”ë“œ      150000 non-null  int64  \n 18  ë²•ì •ë™ëª…       150000 non-null  object \n 19  ì§€ë²ˆì½”ë“œ       150000 non-null  int64  \n 20  ëŒ€ì§€êµ¬ë¶„ì½”ë“œ     150000 non-null  int64  \n 21  ëŒ€ì§€êµ¬ë¶„ëª…      150000 non-null  object \n 22  ì§€ë²ˆë³¸ë²ˆì§€      150000 non-null  int64  \n 23  ì§€ë²ˆë¶€ë²ˆì§€      124172 non-null  float64\n 24  ì§€ë²ˆì£¼ì†Œ       150000 non-null  object \n 25  ë„ë¡œëª…ì½”ë“œ      150000 non-null  int64  \n 26  ë„ë¡œëª…        150000 non-null  object \n 27  ê±´ë¬¼ë³¸ë²ˆì§€      150000 non-null  int64  \n 28  ê±´ë¬¼ë¶€ë²ˆì§€      18999 non-null   float64\n 29  ê±´ë¬¼ê´€ë¦¬ë²ˆí˜¸     150000 non-null  object \n 30  ê±´ë¬¼ëª…        69352 non-null   object \n 31  ë„ë¡œëª…ì£¼ì†Œ      150000 non-null  object \n 32  êµ¬ìš°í¸ë²ˆí˜¸      150000 non-null  int64  \n 33  ì‹ ìš°í¸ë²ˆí˜¸      149996 non-null  float64\n 34  ë™ì •ë³´        13346 non-null   object \n 35  ì¸µì •ë³´        90788 non-null   object \n 36  í˜¸ì •ë³´        22282 non-null   object \n 37  ê²½ë„         150000 non-null  float64\n 38  ìœ„ë„         150000 non-null  float64\ndtypes: float64(5), int64(11), object(23)\nmemory usage: 44.6+ MB\n\n\në°ì´í„°ë¥¼ í™•ì¸ í•´ë³´ë‹ˆ ê²°ì¸¡ê°’ ìˆëŠ” ì»¬ëŸ¼ì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\n\n\n\nìœ„ì—ì„œ í™•ì¸í•œ ê²°ì¸¡ê°’ì„ ë” ìì„¸íˆ ë³´ê¸° ìœ„í•´ missingo ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•˜ì—¬ ì‹œê°í™”í•´ë³´ê² ìŠµë‹ˆë‹¤.\në¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ conda install -c conda-forge missingno\n\nê¸°ë³¸\n\n\nimport missingno as msno\n\n_=msno.matrix(df)\n\n\n\n\n\n\n\n\n\ndf.describe()\n\n\n\n\n\n  \n    \n      \n      ìƒê°€ì—…ì†Œë²ˆí˜¸\n      ì‹œë„ì½”ë“œ\n      ì‹œêµ°êµ¬ì½”ë“œ\n      í–‰ì •ë™ì½”ë“œ\n      ë²•ì •ë™ì½”ë“œ\n      ì§€ë²ˆì½”ë“œ\n      ëŒ€ì§€êµ¬ë¶„ì½”ë“œ\n      ì§€ë²ˆë³¸ë²ˆì§€\n      ì§€ë²ˆë¶€ë²ˆì§€\n      ë„ë¡œëª…ì½”ë“œ\n      ê±´ë¬¼ë³¸ë²ˆì§€\n      ê±´ë¬¼ë¶€ë²ˆì§€\n      êµ¬ìš°í¸ë²ˆí˜¸\n      ì‹ ìš°í¸ë²ˆí˜¸\n      ê²½ë„\n      ìœ„ë„\n    \n  \n  \n    \n      count\n      1.500000e+05\n      150000.000000\n      150000.000000\n      1.500000e+05\n      1.500000e+05\n      1.500000e+05\n      150000.000000\n      150000.000000\n      124172.000000\n      1.500000e+05\n      150000.000000\n      18999.000000\n      150000.000000\n      149996.000000\n      150000.000000\n      150000.000000\n    \n    \n      mean\n      2.041990e+07\n      15.332800\n      15748.405900\n      1.574901e+09\n      1.574852e+09\n      1.574853e+18\n      1.001320\n      468.930780\n      33.179960\n      1.574876e+11\n      153.743900\n      7.015159\n      273384.199573\n      17495.024287\n      127.593626\n      36.856878\n    \n    \n      std\n      5.213909e+06\n      6.798467\n      6756.859055\n      6.756845e+08\n      6.756859e+08\n      6.756858e+17\n      0.036308\n      486.121473\n      105.183785\n      6.756853e+10\n      277.414013\n      9.877446\n      215380.462007\n      19396.705618\n      0.940031\n      1.080322\n    \n    \n      min\n      2.895874e+06\n      11.000000\n      11110.000000\n      1.111052e+09\n      1.111010e+09\n      1.111010e+18\n      1.000000\n      1.000000\n      1.000000\n      1.111020e+11\n      0.000000\n      1.000000\n      100011.000000\n      1000.000000\n      126.768169\n      35.010463\n    \n    \n      25%\n      1.606223e+07\n      11.000000\n      11320.000000\n      1.132069e+09\n      1.132011e+09\n      1.132011e+18\n      1.000000\n      108.000000\n      3.000000\n      1.132041e+11\n      20.000000\n      1.000000\n      134851.000000\n      4386.000000\n      126.966849\n      35.215700\n    \n    \n      50%\n      2.210410e+07\n      11.000000\n      11620.000000\n      1.162058e+09\n      1.162010e+09\n      1.162010e+18\n      1.000000\n      333.000000\n      10.000000\n      1.162030e+11\n      50.000000\n      3.000000\n      142874.000000\n      6522.000000\n      127.047053\n      37.511174\n    \n    \n      75%\n      2.477166e+07\n      26.000000\n      26200.000000\n      2.620065e+09\n      2.620012e+09\n      2.620012e+18\n      1.000000\n      679.000000\n      25.000000\n      2.620042e+11\n      171.000000\n      9.000000\n      604040.000000\n      46563.000000\n      128.986009\n      37.560275\n    \n    \n      max\n      2.852486e+07\n      26.000000\n      26710.000000\n      2.671033e+09\n      2.671033e+09\n      2.671033e+18\n      2.000000\n      9993.000000\n      3784.000000\n      2.671042e+11\n      3318.000000\n      198.000000\n      619963.000000\n      49527.000000\n      129.286869\n      37.688821\n    \n  \n\n\n\n\n \n\n\n\n\n\n\n\nprint(\"== ê²°ì¸¡ì¹˜ ==\")\nprint(df.isnull().sum())\n\nprint()\nprint(\"== ê²°ì¸¡ì¹˜ê°€ ë§ì€ ì»¬ëŸ¼ ==\")\nnot_use_col = df.isnull().sum().sort_values(ascending=False).head(9).index \nprint(not_use_col)\n\n== ê²°ì¸¡ì¹˜ ==\nìƒê°€ì—…ì†Œë²ˆí˜¸            0\nìƒí˜¸ëª…               0\nì§€ì ëª…          130061\nìƒê¶Œì—…ì¢…ëŒ€ë¶„ë¥˜ì½”ë“œ         0\nìƒê¶Œì—…ì¢…ëŒ€ë¶„ë¥˜ëª…          0\nìƒê¶Œì—…ì¢…ì¤‘ë¶„ë¥˜ì½”ë“œ         0\nìƒê¶Œì—…ì¢…ì¤‘ë¶„ë¥˜ëª…          0\nìƒê¶Œì—…ì¢…ì†Œë¶„ë¥˜ì½”ë“œ         0\nìƒê¶Œì—…ì¢…ì†Œë¶„ë¥˜ëª…          0\ní‘œì¤€ì‚°ì—…ë¶„ë¥˜ì½”ë“œ       8935\ní‘œì¤€ì‚°ì—…ë¶„ë¥˜ëª…        8935\nì‹œë„ì½”ë“œ              0\nì‹œë„ëª…               0\nì‹œêµ°êµ¬ì½”ë“œ             0\nì‹œêµ°êµ¬ëª…              0\ní–‰ì •ë™ì½”ë“œ             0\ní–‰ì •ë™ëª…              0\në²•ì •ë™ì½”ë“œ             0\në²•ì •ë™ëª…              0\nì§€ë²ˆì½”ë“œ              0\nëŒ€ì§€êµ¬ë¶„ì½”ë“œ            0\nëŒ€ì§€êµ¬ë¶„ëª…             0\nì§€ë²ˆë³¸ë²ˆì§€             0\nì§€ë²ˆë¶€ë²ˆì§€         25828\nì§€ë²ˆì£¼ì†Œ              0\në„ë¡œëª…ì½”ë“œ             0\në„ë¡œëª…               0\nê±´ë¬¼ë³¸ë²ˆì§€             0\nê±´ë¬¼ë¶€ë²ˆì§€        131001\nê±´ë¬¼ê´€ë¦¬ë²ˆí˜¸            0\nê±´ë¬¼ëª…           80648\në„ë¡œëª…ì£¼ì†Œ             0\nêµ¬ìš°í¸ë²ˆí˜¸             0\nì‹ ìš°í¸ë²ˆí˜¸             4\në™ì •ë³´          136654\nì¸µì •ë³´           59212\ní˜¸ì •ë³´          127718\nê²½ë„                0\nìœ„ë„                0\ndtype: int64\n\n== ê²°ì¸¡ì¹˜ê°€ ë§ì€ ì»¬ëŸ¼ ==\nIndex(['ë™ì •ë³´', 'ê±´ë¬¼ë¶€ë²ˆì§€', 'ì§€ì ëª…', 'í˜¸ì •ë³´', 'ê±´ë¬¼ëª…', 'ì¸µì •ë³´', 'ì§€ë²ˆë¶€ë²ˆì§€', 'í‘œì¤€ì‚°ì—…ë¶„ë¥˜ëª…',\n       'í‘œì¤€ì‚°ì—…ë¶„ë¥˜ì½”ë“œ'],\n      dtype='object')\n\n\n\ndf.shape\ndf = df.drop(columns=not_use_col)\ndf.shape\n\n(150000, 39)\n\n\n(150000, 30)\n\n\nê²°ì¸¡ì¹˜ê°€ ë„ˆë¬´ ë§ì€ ì»¬ëŸ¼ì€ ë¶„ì„ì— ì‚¬ìš©í•  ìˆ˜ ì—†ìœ¼ë¯€ë¡œ ê²°ì¸¡ê°’ ê°œìˆ˜ ìƒìœ„ 9ê°œ ì»¬ëŸ¼ì„ ì¶”ì¶œí•˜ì—¬ ì œê±°í•´ì£¼ì—ˆìŠµë‹ˆë‹¤.\n\n\n\n\ndf[\"ì‹œë„ëª…\"].unique()\n\narray(['ì„œìš¸íŠ¹ë³„ì‹œ', 'ë¶€ì‚°ê´‘ì—­ì‹œ'], dtype=object)\n\n\nì„œìš¸ì‹œì˜ ë°ì´í„°ë§Œ ë¶„ì„ì„ ì§„í–‰í•˜ê¸° ìœ„í•´ ì„œìš¸íŠ¹ë³„ì‹œë§Œ ë”°ë¡œ ì¶”ì¶œí•˜ì—¬ ì €ì¥í•˜ë„ë¡ í•©ë‹ˆë‹¤.\n\ndf = df[df[\"ì‹œë„ëª…\"] == \"ì„œìš¸íŠ¹ë³„ì‹œ\"]\n\n# í™•ì¸ \ndf[\"ì‹œë„ëª…\"].unique()\n\narray(['ì„œìš¸íŠ¹ë³„ì‹œ'], dtype=object)\n\n\n \n\n\n\n\n1 ) ì„œìš¸ì‹œ ìƒê¶Œì—…ì¢…ë¶„ë¥˜ í†µê³„\n2 ) ì„œìš¸ì‹œ ìŒì‹ì  ë¶„ì„\n3 ) ì„œìš¸ì‹œ í•™ì› ë¶„ì„\n4 ) ì„œìš¸ì‹œ ë² ì´ì»¤ë¦¬ ì…ì ë¶„ì„\n\n\n\nìƒê¶Œì—…ì¢…ëŒ€ë¶„ë¥˜ëª… ì»¬ëŸ¼ì„ ì‚¬ìš©í•˜ì—¬ ì„œìš¸ì‹œì— ì–´ë–¤ ì—…ì¢…ì´ ìˆê³ , ì–´ë–¤ ë¶„ë¥˜ì˜ ì—…ì¢…ì´ ë§ì´ ì¡´ì¬í•˜ëŠ”ì§€ í†µê³„ë¥¼ ë‚´ë³´ë„ë¡ í•©ë‹ˆë‹¤.\n\n\n\nsr_order = df['ìƒê¶Œì—…ì¢…ëŒ€ë¶„ë¥˜ëª…'].value_counts()\n\n_=plt.figure(figsize=(10, 5))\n\n_=plt.title(\"ì„œìš¸ì‹œ ìƒê¶Œ ì—…ì¢… í˜„í™©\", fontsize=20, pad=10)\nbars=sns.countplot(data=df, x=\"ìƒê¶Œì—…ì¢…ëŒ€ë¶„ë¥˜ëª…\", palette=sns.color_palette(\"viridis_r\"), order=sr_order.index)\n\nfor i, b in enumerate(bars.patches):\n    _=plt.text(b.get_x()+b.get_width()*(1/2),b.get_height()+500, \\\n            round(b.get_height()),ha='center',fontsize=15, color='black')\n\n_=plt.ylim(0, 55000)\n_=plt.xticks(rotation=45)\n\n\n\n\nìƒê¶Œì—…ì¢…ëŒ€ë¶„ë¥˜ëª…ìœ¼ë¡œ barplotì„ ê·¸ë ¤ë³´ì•˜ìŠµë‹ˆë‹¤. ì„œìš¸ì‹œì—ëŠ” ìŒì‹ ì—…ì¢…ì´ ê°€ì¥ ë§ê³ , ê·¸ ë‹¤ìŒìœ¼ë¡œëŠ” ë¹„ìŠ·í•˜ê²Œ ì†Œë§¤ì˜ ìˆ˜ê°€ ë§ìŠµë‹ˆë‹¤.\n\n\n\n\n\ndf[\"ì‹œêµ°êµ¬ëª…\"].value_counts().head(4)\n\nê°•ë‚¨êµ¬    12368\nì„œì´ˆêµ¬     6575\nê´‘ì§„êµ¬     5757\nì¤‘êµ¬      5538\nName: ì‹œêµ°êµ¬ëª…, dtype: int64\n\n\nì„œìš¸ì‹œ ìì¹˜êµ¬ ì¤‘ ìƒê¶Œì—…ì¢…ì´ ë§ì€ 4ê°€ì§€ ìì¹˜êµ¬(ê°•ë‚¨êµ¬, ì¤‘êµ¬, ì„œì´ˆêµ¬, ê°•ì„œêµ¬)ë§Œ ë½‘ì•„ ê° ì§€ì—­ë³„ë¡œ ì–´ë–¤ ì—…ì¢…ì´ ë§ì´ ì°¨ì§€í•˜ëŠ”ì§€ ì‹œê°í™” í•´ë³´ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤.\n\ndf_gn = df[df['ì‹œêµ°êµ¬ëª…']=='ê°•ë‚¨êµ¬']\ndf_j = df[df['ì‹œêµ°êµ¬ëª…']=='ì¤‘êµ¬']\ndf_s = df[df['ì‹œêµ°êµ¬ëª…']=='ì„œì´ˆêµ¬']\ndf_gs = df[df['ì‹œêµ°êµ¬ëª…']=='ê°•ì„œêµ¬']\n\nfig = plt.figure(figsize=(15, 10))\naxes = fig.subplots(2, 2).flatten() \n\n\nfor idx, df_r in enumerate([df_gn, df_j, df_s, df_gs]):\n    _=sns.countplot(data=df_r, x=\"ìƒê¶Œì—…ì¢…ëŒ€ë¶„ë¥˜ëª…\", \n                    palette=sns.color_palette(\"viridis_r\"), \n                    order=df_r[\"ìƒê¶Œì—…ì¢…ëŒ€ë¶„ë¥˜ëª…\"].value_counts().index, \n                    ax=axes[idx])\n\n    _=axes[idx].set_xticklabels(labels=df_r[\"ìƒê¶Œì—…ì¢…ëŒ€ë¶„ë¥˜ëª…\"].value_counts().index, rotation=45)\n\n    _=axes[idx].set_title(f\"{df_r['ì‹œêµ°êµ¬ëª…'].unique()[0]} ìƒê¶Œ ì—…ì¢… í˜„í™©\", fontsize=20, pad=10)\n\nfig.tight_layout()\n\n\n\n\nëŒ€ë¶€ë¶„ ë¹„ìŠ·í•œ í˜•íƒœë¥¼ ë„ëŠ”ë°, ì¤‘êµ¬ëŠ” ì¡°ê¸ˆ ë‹¤ë¥´ê²Œ ë‚˜íƒ€ë‚¬ìŠµë‹ˆë‹¤. ìŒì‹ë³´ë‹¤ ì†Œë§¤ ì—…ì¢…ì´ ì›”ë“±íˆ ë†’ì•˜ê³ , í•™ë¬¸/êµìœ¡ ì—…ì¢…ì€ ë‹¤ë¥¸ ì§€ì—­ì— ë¹„í•´ í˜„ì €íˆ ë‚®ì€ ê²ƒìœ¼ë¡œ ë³´ì—¬ì§‘ë‹ˆë‹¤.\n\n\n\n\n\nì„œìš¸ì‹œì˜ êµìœ¡ê´€ë ¨ ì—…ì¢… ë°ì´í„°ë§Œ ì¶”ì¶œí•˜ì—¬ ì–´ë–¤ ì§€ì—­ì— í•™ì›ì´ ë§ì´ ë“¤ì–´ì„œìˆëŠ”ì§€, ì–´ëŠ ê³³ì— í•™ì›ê°€ê°€ í™œë°œí•œì§€ ë¶„ì„í•©ë‹ˆë‹¤.\n\ndf_academy = df[df[\"ìƒê¶Œì—…ì¢…ëŒ€ë¶„ë¥˜ëª…\"] == \"í•™ë¬¸/êµìœ¡\"].copy()\n\n# í™•ì¸\ndf_academy[\"ìƒê¶Œì—…ì¢…ëŒ€ë¶„ë¥˜ëª…\"].unique()\n\narray(['í•™ë¬¸/êµìœ¡'], dtype=object)\n\n\në¨¼ì € ìƒê¶Œì—…ì¢…ëŒ€ë¶„ë¥˜ëª…ì´ í•™ë¬¸/êµìœ¡ì¸ ë°ì´í„°ë“¤ë§Œ ì¶”ì¶œí•˜ì—¬ df_academyì— ë„£ì–´ì£¼ì—ˆìŠµë‹ˆë‹¤.\n\n\n\nìƒí˜¸ëª… top 10ê°œ í™•ì¸í•˜ê¸°\n\n\ndf_academy[\"ìƒí˜¸ëª…\"].value_counts().head(10)\n\nì í”„ì…ˆêµì‹¤      442\ní•´ë²•ìˆ˜í•™        15\ní•´ë²•ì˜ì–´êµì‹¤       9\në¬´ì§€ê°œì–´ë¦°ì´ì§‘      8\në®¤ì— ì˜ì–´         7\nìœ¤ì„ ìƒì˜ì–´êµì‹¤      7\nìƒˆì‹¹ì–´ë¦°ì´ì§‘       6\nìˆ²ì†ì–´ë¦°ì´ì§‘       6\ní–‰ë³µí•œì–´ë¦°ì´ì§‘      6\nì‚¼ì„±ì˜ì–´         5\nName: ìƒí˜¸ëª…, dtype: int64\n\n\nì„œìš¸ì‹œì˜ ìƒí˜¸ëª…ì€ ì í”„ì…ˆêµì‹¤ì´ë¼ëŠ” ìƒí˜¸ëª…ì´ ì••ë„ì ìœ¼ë¡œ ë§ìŠµë‹ˆë‹¤.\n\nì„œìš¸ì‹œ ìì¹˜êµ¬ë³„ í•™ë¬¸/êµìœ¡ ì—…ì¢… ê°œìˆ˜ í™•ì¸í•˜ê¸°\n\n\nsr_order = df_academy['ì‹œêµ°êµ¬ëª…'].value_counts()\n\n_=plt.figure(figsize=(15, 7))\n\n_=plt.title(\"ì„œìš¸ì‹œ ìì¹˜êµ¬ë³„ í•™ë¬¸/êµìœ¡ ì—…ì¢… í˜„í™©\", fontsize=20, pad=10)\nbars=sns.countplot(data=df_academy, x=\"ì‹œêµ°êµ¬ëª…\", palette=sns.color_palette(\"twilight_shifted\"), order=sr_order.index)\n\nfor i, b in enumerate(bars.patches):\n    _=plt.text(b.get_x()+b.get_width()*(1/2),b.get_height()+10, \\\n            round(b.get_height()),ha='center',fontsize=15, color='black')\n\n_=plt.ylim(0, 1150)\n_=plt.xticks(rotation=45)\n\n\n\n\ní•™ë¬¸/êµìœ¡ ì—…ì¢…ì€ ê°•ë‚¨êµ¬ê°€ ê°€ì¥ ë§ì€ ë¹„ìœ¨ì„ ì°¨ì§€í•˜ê³  ìˆìŠµë‹ˆë‹¤. ê·¸ ë‹¤ìŒìœ¼ë¡œëŠ” ì„œì´ˆêµ¬, ê´‘ì§„êµ¬, ì†¡íŒŒêµ¬ ë“± ë¹„ë“±ë¹„ë“±í•œ ê°œìˆ˜ë¡œ ì ì°¨ ì¤„ì–´ë“­ë‹ˆë‹¤.\n\nìƒê¶Œì—…ì¢… ì¤‘ë¶„ë¥˜ ë³„ ê°œìˆ˜ í™•ì¸í•˜ê¸°\n\n\nsr_order = df_academy['ìƒê¶Œì—…ì¢…ì¤‘ë¶„ë¥˜ëª…'].value_counts()\n\n_=plt.figure(figsize=(11, 7))\n\n_=plt.title(\"ì„œìš¸ì‹œ ìƒê¶Œì—…ì¢…ì¤‘ë¶„ë¥˜ ë³„ ê°œìˆ˜\", fontsize=20, pad=10)\nbars=sns.countplot(data=df_academy, y=\"ìƒê¶Œì—…ì¢…ì¤‘ë¶„ë¥˜ëª…\", palette=sns.color_palette(\"twilight_shifted\"), order=sr_order.index)\n\nfor i, b in enumerate(bars.patches):\n    _=plt.text(b.get_width()+70,b.get_y()+b.get_height()*(1/2), \\\n            round(b.get_width()),ha='center',fontsize=15, color='black')\n\n_=plt.xlim(0, 2150)\n\n\n\n\nì¤‘ë¶„ë¥˜ ì—…ì¢…ìœ¼ë¡œ ë‚˜ëˆ ë´¤ì„ ë•ŒëŠ”, ë³´ìŠµêµìŠµì…ì‹œê°€ ê°€ì¥ ë§ì€ ê²ƒìœ¼ë¡œ ë‚˜íƒ€ë‚¬ìŠµë‹ˆë‹¤.\n\nìƒê¶Œì—…ì¢… ì†Œë¶„ë¥˜ëª… top 30ê°œ ê°œìˆ˜ í™•ì¸í•˜ê¸°\n\n\nsr_order = df_academy['ìƒê¶Œì—…ì¢…ì†Œë¶„ë¥˜ëª…'].value_counts().head(30)\n\n_=plt.figure(figsize=(20, 7))\n\n_=plt.title(\"ì„œìš¸ì‹œ ìƒê¶Œì—…ì¢… ì†Œë¶„ë¥˜ëª… (top 30)\", fontsize=20, pad=10)\nbars=sns.countplot(data=df_academy, x=\"ìƒê¶Œì—…ì¢…ì†Œë¶„ë¥˜ëª…\", palette=sns.color_palette(\"twilight_shifted\"), order=sr_order.index)\n\nfor i, b in enumerate(bars.patches):\n    _=plt.text(b.get_x()+b.get_width()*(1/2),b.get_height()+10, \\\n            round(b.get_height()),ha='center',fontsize=15, color='black')\n\n_=plt.ylim(0, 2100)\n_=plt.xticks(rotation=60)\n\n\n\n\nì†Œë¶„ë¥˜ëª…ë³„ ì‹œê°í™”ë¥¼ ì§„í–‰í•´ë³´ë‹ˆ, í•™ì›ì˜ ì…ì‹œì™€ ì¢…í•©ì´ ê°€ì¥ ë§ì•˜ìœ¼ë©°, ì–´ë¦°ì´ì§‘ë„ ë†’ì€ ë¹„ìœ¨ì„ ì°¨ì§€í•˜ê³ ìˆìŠµë‹ˆë‹¤.\n\nêµ¬ë³„, ì¤‘ë¶„ë¥˜ë³„ ìƒì ê°œìˆ˜ íŒŒì•…\n\n\nfig=plt.figure(figsize=(15,5),dpi=100)\n\n_=plt.title(\"êµ¬ë³„, ì¤‘ë¶„ë¥˜ë³„ ìƒì ê°œìˆ˜\", fontsize=20, pad=10)\n\n_=sns.countplot(data=df_academy, x='ì‹œêµ°êµ¬ëª…',hue='ìƒê¶Œì—…ì¢…ì¤‘ë¶„ë¥˜ëª…', palette=sns.color_palette(\"twilight_shifted\"))\n\n_=plt.xticks(rotation=60)\n_=plt.legend(bbox_to_anchor=(1.02, 1), loc=2)\n\n\n\n\nì„œìš¸ì‹œì˜ ìì¹˜êµ¬ ë³„ë¡œ ì¤‘ë¶„ë¥˜ë³„ ìƒì ê°œìˆ˜ ì‹œê°í™”ë¥¼ í•´ë³´ì•˜ìŠµë‹ˆë‹¤. íŠ¹íˆ ê°•ë‚¨êµ¬ì— í•™ì›-ë³´ìŠµêµìŠµì…ì‹œê°€ ëšœë ·í•˜ê²Œ ë†’ì´ ì†Ÿì•„ìˆëŠ” ê²ƒì„ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë˜, ëŒ€ì²´ì ìœ¼ë¡œ í•™ì›-ë³´ìŠµêµìŠµì…ì‹œì˜ ìƒì  ê°œìˆ˜ê°€ ê°€ì¥ ë§ì€ ê²ƒìœ¼ë¡œ ë³´ì…ë‹ˆë‹¤.\n\nìì¹˜êµ¬ë³„ ê°€ì¥ ë§ì€ ì†Œë¶„ë¥˜ëª… ìƒì ê³¼ ê·¸ ê°œìˆ˜ ì‹œê°í™”\n\n\n# ìì¹˜êµ¬ë³„ ìƒê¶Œì—…ì¢… ì†Œë¶„ë¥˜ëª…ì˜ ê°œìˆ˜ \nacademy_count_s=df_academy.groupby(['ì‹œêµ°êµ¬ëª…','ìƒê¶Œì—…ì¢…ì†Œë¶„ë¥˜ëª…'])['ìƒí˜¸ëª…'].count()\n\ndf2=academy_count_s.reset_index()\ndf2=df2.rename(columns={'ìƒí˜¸ëª…':'ìƒí˜¸ìˆ˜'})\n\nmax_num_index=df2.groupby(['ì‹œêµ°êµ¬ëª…'])['ìƒí˜¸ìˆ˜'].idxmax()\ndf3=df2.loc[max_num_index]\n\ndf3['ind']=df3['ì‹œêµ°êµ¬ëª…']+'('+df5['ìƒê¶Œì—…ì¢…ì†Œë¶„ë¥˜ëª…'] + ')'\ndf3.head()\ndf3=df3.set_index('ind')\ndf3=df3.drop(columns=['ì‹œêµ°êµ¬ëª…','ìƒê¶Œì—…ì¢…ì†Œë¶„ë¥˜ëª…'])\ndf3.head()\n\n\n\n\n\n  \n    \n      \n      ì‹œêµ°êµ¬ëª…\n      ìƒê¶Œì—…ì¢…ì†Œë¶„ë¥˜ëª…\n      ìƒí˜¸ìˆ˜\n      ind\n    \n  \n  \n    \n      46\n      ê°•ë‚¨êµ¬\n      í•™ì›-ì…ì‹œ\n      333\n      ê°•ë‚¨êµ¬(í•™ì›-ì…ì‹œ)\n    \n    \n      81\n      ê°•ë™êµ¬\n      í•™ì›-ì…ì‹œ\n      82\n      ê°•ë™êµ¬(í•™ì›-ì…ì‹œ)\n    \n    \n      99\n      ê°•ë¶êµ¬\n      í•™ì›-ì…ì‹œ\n      28\n      ê°•ë¶êµ¬(í•™ì›-ì…ì‹œ)\n    \n    \n      126\n      ê°•ì„œêµ¬\n      í•™ì›-ì…ì‹œ\n      106\n      ê°•ì„œêµ¬(í•™ì›-ì…ì‹œ)\n    \n    \n      149\n      ê´€ì•…êµ¬\n      í•™ì›-ì…ì‹œ\n      59\n      ê´€ì•…êµ¬(í•™ì›-ì…ì‹œ)\n    \n  \n\n\n\n\n\n\n\n\n  \n    \n      \n      ìƒí˜¸ìˆ˜\n    \n    \n      ind\n      \n    \n  \n  \n    \n      ê°•ë‚¨êµ¬(í•™ì›-ì…ì‹œ)\n      333\n    \n    \n      ê°•ë™êµ¬(í•™ì›-ì…ì‹œ)\n      82\n    \n    \n      ê°•ë¶êµ¬(í•™ì›-ì…ì‹œ)\n      28\n    \n    \n      ê°•ì„œêµ¬(í•™ì›-ì…ì‹œ)\n      106\n    \n    \n      ê´€ì•…êµ¬(í•™ì›-ì…ì‹œ)\n      59\n    \n  \n\n\n\n\n\n_=plt.figure(figsize=(12, 9))\n\n_=plt.title(\"ì„œìš¸ì‹œ ìƒê¶Œì—…ì¢…ì¤‘ë¶„ë¥˜ ë³„ ê°œìˆ˜\", fontsize=20, pad=10)\nbars=sns.barplot(data=df3.reset_index(), \n                   x=\"ìƒí˜¸ìˆ˜\",\n                   y='ind',\n                   palette=sns.color_palette(\"flare\"))\n\nfor i, b in enumerate(bars.patches):\n    _=plt.text(b.get_width()+10,b.get_y()+b.get_height()*(1/2), \\\n            round(b.get_width()),ha='center',fontsize=15, color='gray')\n\n_=plt.xlim(0, 360)\n\n\n\n\nëŒ€ì²´ì ìœ¼ë¡œ í•™ì›-ì…ì‹œ ì†Œë¶„ë¥˜ê°€ ê°€ì¥ ë§ìŠµë‹ˆë‹¤. ë‹¤ë¥¸ í•™ì› (ì¢…í•©, ì™¸êµ­ì–´/ì–´í•™)ë„ ìˆê³ , ì–´ë¦°ì´ì§‘ì´ ê°€ì¥ ë§ì€ ìì¹˜êµ¬ë„ ì¡´ì¬í•©ë‹ˆë‹¤.\n\n\n\n\n\ng = df_academy.groupby([\"ìƒê¶Œì—…ì¢…ì†Œë¶„ë¥˜ëª…\", \"ì‹œêµ°êµ¬ëª…\"])[\"ìƒí˜¸ëª…\"].count()\n\nsr_order = g.loc[\"í•™ì›-ì…ì‹œ\"].sort_values(ascending=False)\n\n_=plt.figure(figsize=(11, 7))\n\n_=plt.title(\"ì„œìš¸ì‹œ ìƒê¶Œì—…ì¢…ì¤‘ë¶„ë¥˜ ë³„ ê°œìˆ˜\", fontsize=20, pad=10)\nbars=sns.countplot(data=df_academy[df_academy[\"ìƒê¶Œì—…ì¢…ì†Œë¶„ë¥˜ëª…\"]==\"í•™ì›-ì…ì‹œ\"], \n                   y=\"ì‹œêµ°êµ¬ëª…\", \n                   palette=sns.color_palette(\"flare\"), \n                   order=sr_order.index)\n\nfor i, b in enumerate(bars.patches):\n    _=plt.text(b.get_width()+10,b.get_y()+b.get_height()*(1/2), \\\n            round(b.get_width()),ha='center',fontsize=15, color='gray')\n\n_=plt.xlim(0, 370)\n\n\n\n\ní•™ì›-ì…ì‹œ ìƒì  ê°œìˆ˜ë¥¼ ìì¹˜êµ¬ë³„ë¡œ í†µê³„ë‚¸ ì‹œê°í™”ì…ë‹ˆë‹¤. ê°•ë‚¨êµ¬ê°€ ë‹¤ë¥¸ ì§€ì—­ì˜ 2ë°° ì´ìƒì€ ë˜ëŠ” ì–‘ì…ë‹ˆë‹¤.\n\n\n\n\nìƒê¶Œì—…ì¢… ì†Œë¶„ë¥˜ëª…ë³„ ìƒì  ê°œìˆ˜ì—ì„œ 1, 3ìœ„ë¥¼ ì°¨ì§€í–ˆë˜ í•™ì›-ì…ì‹œì™€ ì–´ë¦°ì´ì§‘ì„ ì§€ë„ì— ê·¸ë ¤ë³´ê² ìŠµë‹ˆë‹¤.\n\nscatterplot\n\n\n# ì–´ë¦°ì´ì§‘ê³¼ í•™ì›-ì…ì‹œë¥¼ ë¹„êµí•´ ë´…ë‹ˆë‹¤.\n_=plt.figure(figsize=(10, 7))\n\n_=plt.title(\"í•™ì›-ì…ì‹œ vs ì–´ë¦°ì´ì§‘\", fontsize=20, pad=10)\n\n_=sns.scatterplot(data=df_academy.loc[df_academy[\"ìƒê¶Œì—…ì¢…ì†Œë¶„ë¥˜ëª…\"].isin([\"ì–´ë¦°ì´ì§‘\", \"í•™ì›-ì…ì‹œ\"])],\n                x=\"ê²½ë„\", y=\"ìœ„ë„\", hue=\"ìƒê¶Œì—…ì¢…ì†Œë¶„ë¥˜ëª…\")\n\n\n\n\në¨¼ì €, ì „ì²´ ì„œìš¸ì‹œ ì§€ì—­ì— ëŒ€í•´ì„œ í•™ì›/ì…ì‹œ ì™€ ì–´ë¦°ì´ì§‘ì„ scatter plotìœ¼ë¡œ ê·¸ë ¤ë³´ì•˜ìŠµë‹ˆë‹¤. ì‚°ì ë„ì— ìœ„ë„ì™€ ê²½ë„ë¥¼ ê·¸ë ¤ë´„ìœ¼ë¡œì¨ ëŒ€ëµì ì¸ ë¶„í¬ë„ë¥¼ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\nFolium\n\n\ndf_academy = df.loc[df[\"ì‹œêµ°êµ¬ëª…\"].isin([\"ë„ë´‰êµ¬\",\"ì„±ë™êµ¬\"]) & (df[\"ìƒê¶Œì—…ì¢…ëŒ€ë¶„ë¥˜ëª…\"] == \"í•™ë¬¸/êµìœ¡\")].copy()\ndf_m = df_academy.loc[df_academy[\"ìƒê¶Œì—…ì¢…ì†Œë¶„ë¥˜ëª…\"].isin([\"ì–´ë¦°ì´ì§‘\", \"í•™ì›-ì…ì‹œ\"])]\n\nlat = df_m[\"ìœ„ë„\"].mean()\nlong = df_m[\"ê²½ë„\"].mean()\nm = folium.Map(location=[lat, long], zoom_start=12)\n\ndf_m_ex=df_m.loc[df_m['ìƒê¶Œì—…ì¢…ì†Œë¶„ë¥˜ëª…']=='í•™ì›-ì…ì‹œ']\nfor i in df_m_ex.index:\n    t1 = df_m.loc[i, \"ìƒí˜¸ëª…\"] +\"(\"+ df_m.loc[i, \"ë„ë¡œëª…ì£¼ì†Œ\"]+\")\"\n    lat = df_m.loc[i, \"ìœ„ë„\"]\n    long = df_m.loc[i, \"ê²½ë„\"]\n    \n    _=folium.Marker([lat, long], tooltip=t1, radius=2, icon=folium.Icon(color=\"green\")).add_to(m)\n    \ndf_m_ch=df_m.loc[df_m['ìƒê¶Œì—…ì¢…ì†Œë¶„ë¥˜ëª…']=='ì–´ë¦°ì´ì§‘']\nfor i in df_m_ch.index:\n    t1 = df_m.loc[i, \"ìƒí˜¸ëª…\"] +\"(\"+ df_m.loc[i, \"ë„ë¡œëª…ì£¼ì†Œ\"]+\")\"\n    lat = df_m.loc[i, \"ìœ„ë„\"]\n    long = df_m.loc[i, \"ê²½ë„\"]\n    \n    _=folium.Marker([lat, long], tooltip=t1, radius=2, icon=folium.Icon(color=\"red\")).add_to(m)\n\nm\n\nMake this Notebook Trusted to load map: File -> Trust Notebook\n\n\nfolium ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì´ìš©í•˜ì—¬, ë„ë´‰êµ¬ì™€ ì„±ë™êµ¬ì— ëŒ€í•´ì„œë§Œ ì–´ë¦°ì´ì§‘ê³¼ í•™ì›-ì…ì‹œ ìƒì ì„ ì‹œê°í™” í•´ë³´ì•˜ìŠµë‹ˆë‹¤. ì´ˆë¡ìƒ‰ì´ í•™ì›-ì…ì‹œ, ë¹¨ê°„ìƒ‰ì´ ì–´ë¦°ì´ì§‘ì…ë‹ˆë‹¤. ë‘ê°€ì§€ ìƒì ì˜ ë¶„í¬ë„ íŒŒì•…ì´ë‚˜, ì‹¤ì œ ìœ„ì¹˜ì •ë³´ê¹Œì§€ ì•Œ ìˆ˜ ìˆëŠ” ìœ ìš©í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ì…ë‹ˆë‹¤.\n\nì—¬ê¸°ê¹Œì§€ ê³µê³µë°ì´í„°í¬í„¸ì˜ ì†Œìƒê³µì¸ì‹œì¥ì§„í¥ê³µë‹¨_ìƒê°€(ìƒê¶Œ)ì •ë³´ ë°ì´í„°ë¥¼ ì‚¬ìš©í•˜ì—¬ ì‹œê°í™”í•´ë³´ì•˜ìŠµë‹ˆë‹¤. ì‹œê°í™” ì—°ìŠµìš©ìœ¼ë¡œ ë°ì´í„°ì˜ ê°œìˆ˜ë¥¼ ì¤„ì—¬ì„œ ì§„í–‰í•˜ì˜€ëŠ”ë°, ì „ì²´ ë°ì´í„°ë¥¼ ê°€ì§€ê³  ì—¬ëŸ¬ ì£¼ì œë¡œ ë¶„ì„ì´ ê°€ëŠ¥í•´ ë³´ì…ë‹ˆë‹¤. ì´ ê¸€ì—ì„œëŠ” êµìœ¡ ì—…ì¢… í˜„í™©ì„ íŒŒì•…í•´ë³´ë©´ì„œ, ì–´ëŠ ì§€ì—­ì— ì…ì‹œ í•™ì›ì´ ë§ì€ì§€ì— ëŒ€í•œ ì •ë³´ë¥¼ ì–»ì„ ìˆ˜ ìˆì—ˆìŠµë‹ˆë‹¤. ë˜ ë‹¤ë¥¸ íŠ¹ì • ìƒê¶Œì—…ì¢…ë¶„ë¥˜ ì—…ì¢…ì„ ì‚¬ìš©í•˜ì—¬, ì…ì ë¶„ì„ì´ë‚˜ í•´ë‹¹ ì—…ì¢…ì˜ í˜„í™© íŒŒì•…, ì—…ì¢…ì´ ë°œë‹¬í•œ ì§€ì—­ ë“±ì„ ë¶„ì„í•´ ë³´ëŠ” ê²ƒë„ í¥ë¯¸ë¡œìš¸ ê²ƒ ê°™ìŠµë‹ˆë‹¤."
  },
  {
    "objectID": "posts/ì„œìš¸ì‹œ_1ì¸ê°€êµ¬_ì‹œê°í™”.html",
    "href": "posts/ì„œìš¸ì‹œ_1ì¸ê°€êµ¬_ì‹œê°í™”.html",
    "title": "ì„œìš¸ì‹œ 1ì¸ê°€êµ¬ ì‹œê°í™” ë¶„ì„",
    "section": "",
    "text": "ë³¸ ê¸€ì€ ë°ì´í„°ë¶„ì„ ê¸°ë°˜ ì›¹ ì„œë¹„ìŠ¤ ê°œë°œ í”„ë¡œì íŠ¸ì˜ ë°ì´í„°ë¶„ì„ ë‚´ìš©ì…ë‹ˆë‹¤. ì„œìš¸ì‹œ 1ì¸ê°€êµ¬ ë°ì´í„°ë¥¼ í™œìš©í•˜ì—¬ ì¸ì‚¬ì´íŠ¸ë¥¼ ë„ì¶œí•´ë‚´ì–´, 1ì¸ê°€êµ¬ì˜ ë¬¸ì œì ì„ íŒŒì•…í•˜ê³ , í•´ê²°ë°©ì•ˆì„ ì œì‹œí•˜ëŠ” ê³¼ì •ìœ¼ë¡œ ì›¹ì„œë¹„ìŠ¤ë¥¼ ê¸°íší•©ë‹ˆë‹¤."
  },
  {
    "objectID": "posts/ì„œìš¸ì‹œ_1ì¸ê°€êµ¬_ì‹œê°í™”.html#section",
    "href": "posts/ì„œìš¸ì‹œ_1ì¸ê°€êµ¬_ì‹œê°í™”.html#section",
    "title": "ì„œìš¸ì‹œ 1ì¸ê°€êµ¬ ì‹œê°í™” ë¶„ì„",
    "section": "",
    "text": "* ì»¤ë®¤ë‹ˆì¼€ì´ì…˜ì´ ì ì€ ì§‘ë‹¨ ì¸¡ì • ê¸°ì¤€: ì „í™”/ë¬¸ì ìˆ˜ ë°œì‹  ëŒ€ìƒì ìˆ˜, ì „í™”/ë¬¸ì ìˆ˜ ë°œì‹  ê±´ ìˆ˜, SNS ì‚¬ìš©ëŸ‰ ê¸°ì¤€\n* ì™¸ì¶œì´ ë§¤ìš° ë§ì€ì§‘ë‹¨ ì¸¡ì • ê¸°ì¤€: 1ì¸ê°€êµ¬ ëŒ€ìƒ ê·¼ë¡œì†Œë“ì´ 3ì²œë§Œì› ì´ˆê³¼ì´ê³ , íœ´ì¼ì˜ ì´ë™ê±´ìˆ˜ì™€ ì´ë™ê±°ë¦¬ê°€ í¬ê³ , íœ´ì¼ì˜ ì¶”ì •ê±°ì£¼ì§€ ì²´ë¥˜ì‹œê°„ì´ ì ì€ ì‚¬ëŒ ê¸°ì¤€ (íœ´ì¼ ì´ë™ê²½í–¥ì´ ë†’ì€ ëŒ€ìƒìë¥¼ êµ¬ë¶„í•˜ê¸° ìœ„í•¨)"
  },
  {
    "objectID": "posts/ì „êµ­_ì•„íŒŒíŠ¸_ë¶„ì–‘ê°€_ë¶„ì„.html",
    "href": "posts/ì „êµ­_ì•„íŒŒíŠ¸_ë¶„ì–‘ê°€_ë¶„ì„.html",
    "title": "ì „êµ­ ì•„íŒŒíŠ¸ ë¶„ì–‘ê°€ê²© ì‹œê°í™” ë¶„ì„",
    "section": "",
    "text": "ë¶„ì–‘ê°€ë€ ì²˜ìŒ ë¶„ì–‘ì„ ì‹œì‘í•  ë•Œì˜ ê°€ê²©ì„ ë§í•©ë‹ˆë‹¤. ì´ë²ˆ ë°ì´í„° ë¶„ì„ì—ì„œëŠ” 2013ë…„ë¶€í„° 2019ë…„ë„ê¹Œì§€ì˜ ì „êµ­ ì‹ ê·œ ë¯¼ê°„ ì•„íŒŒíŠ¸ ë¶„ì–‘ê°€ê²© ë°ì´í„°ë¥¼ ì‚¬ìš©í•˜ì—¬ ì•„íŒŒíŠ¸ ë¶„ì–‘ê°€ë¥¼ ì—°ë„, ì§€ì—­ ë³„ë¡œ ë¶„ì„í•´ë³´ë©´ì„œ ë¶€ë™ì‚° ê°€ê²© ë³€ë™ ì¶”ì„¸ë¥¼ ì•Œì•„ë´…ë‹ˆë‹¤.\n* ì¸í”„ëŸ°, ì „ê³µ ìˆ˜ì—…ì‹œê°„ì— ì§„í–‰í•œ ë‚´ìš©ì„ í† ëŒ€ë¡œ ê°€ì„¤ ì„¤ì •ê³¼ í•´ì„, ì‹œê°í™” ë””ìì¸ì„ ì¬êµ¬ì„±í•œ ê¸€ì…ë‹ˆë‹¤.\n\n\n\nimport sys\nprint('python', sys.version)\n\nimport numpy as np\nprint('numpy', np.__version__)\n\nimport pandas as pd\nprint('pandas', pd.__version__)\n\nimport matplotlib as mpl\nprint('matplotlib', mpl.__version__)\n\nimport matplotlib.pyplot as plt\n\nimport seaborn as sns\nprint('pandas', sns.__version__)\n\nimport matplotlib.pyplot as plt\nplt.rc(\"font\", family=\"Malgun Gothic\", size=15) \n\n# ê²°ê³¼ í™•ì¸ì„ ìš©ì´í•˜ê²Œ í•˜ê¸° ìœ„í•œ ì½”ë“œ\nfrom IPython.core.interactiveshell import InteractiveShell\nInteractiveShell.ast_node_interactivity = 'all'\n\n# ê²½ê³  ë©”ì‹œì§€ ë¬´ì‹œ.\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\npython 3.7.3 (default, Mar 27 2019, 17:13:21) [MSC v.1915 64 bit (AMD64)]\nnumpy 1.21.6\npandas 1.2.0\nmatplotlib 3.3.2\npandas 0.11.1\n\n\n \n\n\n\në°ì´í„°: [ê³µê³µë°ì´í„° í¬í„¸] ì£¼íƒë„ì‹œë³´ì¦ê³µì‚¬_ì „êµ­ í‰ê·  ë¶„ì–‘ê°€ê²©\n\n# 1) 2015.10 ~ 2019.12 \ndf_last = pd.read_csv(\"3w/data/ì „êµ­í‰ê·  ë¶„ì–‘ê°€ê²© (2015ë…„10ì›”~2019ë…„12ì›”).csv\", encoding=\"cp949\") # í•œê¸€ì´ ê¹¨ì§€ì§€ ì•Šê²Œ í•˜ë ¤ë©´ ì¸ì½”ë”©ì„ í•´ì¤˜ì•¼ í•œë‹¤.\n\n# 2) 2013.12 ~ 2015.8 \ndf_first = pd.read_csv(\"3w/data/ì „êµ­í‰ê·  ë¶„ì–‘ê°€ê²© (2013ë…„12ì›”~2015ë…„8ì›”).csv\", encoding=\"cp949\") # í•œê¸€ì´ ê¹¨ì§€ì§€ ì•Šê²Œ í•˜ë ¤ë©´ ì¸ì½”ë”©ì„ í•´ì¤˜ì•¼ í•œë‹¤.\n\n\nprint(\"== shape ==\")\nprint(\"df_last\", df_last.shape)\nprint(\"df_first\", df_first.shape)\n\nprint()\nprint(\"== head ==\")\ndf_last.head() \ndf_first.head()\n\n== shape ==\ndf_last (4335, 5)\ndf_first (17, 22)\n\n== head ==\n\n\n\n\n\n\n  \n    \n      \n      ì§€ì—­ëª…\n      ê·œëª¨êµ¬ë¶„\n      ì—°ë„\n      ì›”\n      ë¶„ì–‘ê°€ê²©(ã¡)\n    \n  \n  \n    \n      0\n      ì„œìš¸\n      ì „ì²´\n      2015\n      10\n      5841\n    \n    \n      1\n      ì„œìš¸\n      ì „ìš©ë©´ì  60ã¡ì´í•˜\n      2015\n      10\n      5652\n    \n    \n      2\n      ì„œìš¸\n      ì „ìš©ë©´ì  60ã¡ì´ˆê³¼ 85ã¡ì´í•˜\n      2015\n      10\n      5882\n    \n    \n      3\n      ì„œìš¸\n      ì „ìš©ë©´ì  85ã¡ì´ˆê³¼ 102ã¡ì´í•˜\n      2015\n      10\n      5721\n    \n    \n      4\n      ì„œìš¸\n      ì „ìš©ë©´ì  102ã¡ì´ˆê³¼\n      2015\n      10\n      5879\n    \n  \n\n\n\n\n\n\n\n\n  \n    \n      \n      ì§€ì—­\n      2013ë…„12ì›”\n      2014ë…„1ì›”\n      2014ë…„2ì›”\n      2014ë…„3ì›”\n      2014ë…„4ì›”\n      2014ë…„5ì›”\n      2014ë…„6ì›”\n      2014ë…„7ì›”\n      2014ë…„8ì›”\n      ...\n      2014ë…„11ì›”\n      2014ë…„12ì›”\n      2015ë…„1ì›”\n      2015ë…„2ì›”\n      2015ë…„3ì›”\n      2015ë…„4ì›”\n      2015ë…„5ì›”\n      2015ë…„6ì›”\n      2015ë…„7ì›”\n      2015ë…„8ì›”\n    \n  \n  \n    \n      0\n      ì„œìš¸\n      18189\n      17925\n      17925\n      18016\n      18098\n      19446\n      18867\n      18742\n      19274\n      ...\n      20242\n      20269\n      20670\n      20670\n      19415\n      18842\n      18367\n      18374\n      18152\n      18443\n    \n    \n      1\n      ë¶€ì‚°\n      8111\n      8111\n      9078\n      8965\n      9402\n      9501\n      9453\n      9457\n      9411\n      ...\n      9208\n      9208\n      9204\n      9235\n      9279\n      9327\n      9345\n      9515\n      9559\n      9581\n    \n    \n      2\n      ëŒ€êµ¬\n      8080\n      8080\n      8077\n      8101\n      8267\n      8274\n      8360\n      8360\n      8370\n      ...\n      8439\n      8253\n      8327\n      8416\n      8441\n      8446\n      8568\n      8542\n      8542\n      8795\n    \n    \n      3\n      ì¸ì²œ\n      10204\n      10204\n      10408\n      10408\n      10000\n      9844\n      10058\n      9974\n      9973\n      ...\n      10020\n      10020\n      10017\n      9876\n      9876\n      9938\n      10551\n      10443\n      10443\n      10449\n    \n    \n      4\n      ê´‘ì£¼\n      6098\n      7326\n      7611\n      7346\n      7346\n      7523\n      7659\n      7612\n      7622\n      ...\n      7752\n      7748\n      7752\n      7756\n      7861\n      7914\n      7877\n      7881\n      8089\n      8231\n    \n  \n\n5 rows Ã— 22 columns\n\n\n\n2013 ~ 2015ë…„ ë°ì´í„°ì™€ 2015 ~ 2019ë…„ ë°ì´í„°ê°€ ì„œë¡œ ë‹¤ë¥¸ í˜•íƒœë¡œ ë˜ì–´ìˆëŠ” ê²ƒì„ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë¨¼ì €, ê°™ì€ í˜•íƒœë¡œ ì „ì²˜ë¦¬ í•œ í›„ì— ë°ì´í„°ì…‹ì„ ë³‘í•©í•˜ì—¬ ë¶„ì„ì„ ì§„í–‰í•˜ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤.\n \n\n\n\n\n\nê·œëª¨êµ¬ë¶„ ì»¬ëŸ¼ì— ì „ìš©ë©´ì ì´ë¼ëŠ” ë¬¸êµ¬ê°€ ê³µí†µì ìœ¼ë¡œ ë“¤ì–´ê°€ê³ , ê·œëª¨êµ¬ë¶„ ë³´ë‹¤ ì „ìš© ë©´ì ì´ ë” ì§ê´€ì ì´ë¯€ë¡œ ìƒˆë¡œìš´ â€˜ì „ìš©ë©´ì â€™ ì´ë¼ëŠ” ì»¬ëŸ¼ì„ ìƒì„±í•©ë‹ˆë‹¤. ë˜, ê¸°ì¡´ ê·œëª¨êµ¬ë¶„ì˜ ë°ì´í„°ì—ì„œ ì „ìš©ë©´ì , ì´ˆê³¼, ì´í•˜ ì™€ ê°™ì€ ë¬¸êµ¬ë¥¼ ë¹¼ê³  ê°„ê²°í•˜ê²Œ ë§Œë“­ë‹ˆë‹¤.\n\ndf_last[\"ì „ìš©ë©´ì \"] = df_last[\"ê·œëª¨êµ¬ë¶„\"].str.replace(\"ì „ìš©ë©´ì \", \"\")\ndf_last[\"ì „ìš©ë©´ì \"] = df_last[\"ì „ìš©ë©´ì \"].str.replace(\"ì´ˆê³¼\", \"~\")\ndf_last[\"ì „ìš©ë©´ì \"] = df_last[\"ì „ìš©ë©´ì \"].str.replace(\"ì´í•˜\", \"\")\ndf_last[\"ì „ìš©ë©´ì \"] = df_last[\"ì „ìš©ë©´ì \"].str.replace(\" \", \"\")\ndf_last.head()\n\n\n\n\n\n  \n    \n      \n      ì§€ì—­ëª…\n      ê·œëª¨êµ¬ë¶„\n      ì—°ë„\n      ì›”\n      ë¶„ì–‘ê°€ê²©(ã¡)\n      ë¶„ì–‘ê°€ê²©\n      í‰ë‹¹ë¶„ì–‘ê°€ê²©\n      ì „ìš©ë©´ì \n    \n  \n  \n    \n      0\n      ì„œìš¸\n      ì „ì²´\n      2015\n      10\n      5841\n      5841.0\n      19275.3\n      ì „ì²´\n    \n    \n      1\n      ì„œìš¸\n      ì „ìš©ë©´ì  60ã¡ì´í•˜\n      2015\n      10\n      5652\n      5652.0\n      18651.6\n      60ã¡\n    \n    \n      2\n      ì„œìš¸\n      ì „ìš©ë©´ì  60ã¡ì´ˆê³¼ 85ã¡ì´í•˜\n      2015\n      10\n      5882\n      5882.0\n      19410.6\n      60ã¡~85ã¡\n    \n    \n      3\n      ì„œìš¸\n      ì „ìš©ë©´ì  85ã¡ì´ˆê³¼ 102ã¡ì´í•˜\n      2015\n      10\n      5721\n      5721.0\n      18879.3\n      85ã¡~102ã¡\n    \n    \n      4\n      ì„œìš¸\n      ì „ìš©ë©´ì  102ã¡ì´ˆê³¼\n      2015\n      10\n      5879\n      5879.0\n      19400.7\n      102ã¡~\n    \n  \n\n\n\n\n\n\n\n\n\n\ndf_last ë°ì´í„°ì—ì„œ ë¶„ì–‘ê°€ê²©ì´ object ë°ì´í„° íƒ€ì…ìœ¼ë¡œ ë˜ì–´ìˆìŠµë‹ˆë‹¤. ê³„ì‚°ì„ ìœ„í•´ì„œëŠ” floatí˜•ìœ¼ë¡œ ë°”ê¿”ì£¼ì–´ì•¼ í•©ë‹ˆë‹¤.\n\ndef to_float(x): \n    if x!=x: # ê²°ì¸¡ê°’ ê²€ì‚¬\n        return np.nan \n    \n    x = x.replace(\",\", \"\")\n    if x.isdigit(): \n        return float(x) \n    \n    return np.nan \n\ndf_last[\"ë¶„ì–‘ê°€ê²©\"] = df_last['ë¶„ì–‘ê°€ê²©(ã¡)'].map(to_float)\ndf_last[\"ë¶„ì–‘ê°€ê²©\"].dtypes\n\ndtype('float64')\n\n\n\n\n\n\në¨¼ì €, df_first ë°ì´í„°ëŠ” í‰ë‹¹ ë¶„ì–‘ê°€ê²© ê¸°ì¤€ìœ¼ë¡œ ì´ë£¨ì–´ì ¸ìˆëŠ”ë°, df_last ë°ì´í„°ëŠ” ã¡ë‹¹ ë¶„ì–‘ê°€ê²©ìœ¼ë¡œ ë“¤ì–´ê°€ìˆê¸° ë•Œë¬¸ì— ë¶„ì–‘ ê°€ê²©ì„ í‰ë‹¹ ê¸°ì¤€ìœ¼ë¡œ ë³´ê¸° ìœ„í•´ 3.3ì„ ê³±í•´ì„œ df_last ë°ì´í„°ì— í‰ë‹¹ë¶„ì–‘ê°€ê²© ì»¬ëŸ¼ì„ ìƒì„±í•˜ë„ë¡ í•©ë‹ˆë‹¤.\n\ndf_last['í‰ë‹¹ë¶„ì–‘ê°€ê²©'] = df_last['ë¶„ì–‘ê°€ê²©'] * 3.3 \ndf_last.head()\n\n\n\n\n\n  \n    \n      \n      ì§€ì—­ëª…\n      ê·œëª¨êµ¬ë¶„\n      ì—°ë„\n      ì›”\n      ë¶„ì–‘ê°€ê²©(ã¡)\n      ë¶„ì–‘ê°€ê²©\n      í‰ë‹¹ë¶„ì–‘ê°€ê²©\n      ì „ìš©ë©´ì \n    \n  \n  \n    \n      0\n      ì„œìš¸\n      ì „ì²´\n      2015\n      10\n      5841\n      5841.0\n      19275.3\n      ì „ì²´\n    \n    \n      1\n      ì„œìš¸\n      ì „ìš©ë©´ì  60ã¡ì´í•˜\n      2015\n      10\n      5652\n      5652.0\n      18651.6\n      60ã¡\n    \n    \n      2\n      ì„œìš¸\n      ì „ìš©ë©´ì  60ã¡ì´ˆê³¼ 85ã¡ì´í•˜\n      2015\n      10\n      5882\n      5882.0\n      19410.6\n      60ã¡~85ã¡\n    \n    \n      3\n      ì„œìš¸\n      ì „ìš©ë©´ì  85ã¡ì´ˆê³¼ 102ã¡ì´í•˜\n      2015\n      10\n      5721\n      5721.0\n      18879.3\n      85ã¡~102ã¡\n    \n    \n      4\n      ì„œìš¸\n      ì „ìš©ë©´ì  102ã¡ì´ˆê³¼\n      2015\n      10\n      5879\n      5879.0\n      19400.7\n      102ã¡~\n    \n  \n\n\n\n\n\n\n\n\ndf_lastì™€ ê°™ì€ í˜•íƒœê°€ ë˜ë„ë¡ meltë¥¼ ì‚¬ìš©í•˜ì—¬ ë³€ê²½í•´ì¤ë‹ˆë‹¤. ì»¬ëŸ¼ì˜ ì´ë¦„ë„ ê°™ê²Œ ë§Œë“¤ì–´ì£¼ë„ë¡ í•©ë‹ˆë‹¤.\n\ndf_first_melt = df_first.melt(id_vars='ì§€ì—­', var_name=\"ë‚ ì§œ\", value_name='í‰ë‹¹ë¶„ì–‘ê°€ê²©').rename(columns={\"ì§€ì—­\": \"ì§€ì—­ëª…\"})\ndf_first_melt.head()\n\n\n\n\n\n  \n    \n      \n      ì§€ì—­ëª…\n      ë‚ ì§œ\n      í‰ë‹¹ë¶„ì–‘ê°€ê²©\n    \n  \n  \n    \n      0\n      ì„œìš¸\n      2013ë…„12ì›”\n      18189\n    \n    \n      1\n      ë¶€ì‚°\n      2013ë…„12ì›”\n      8111\n    \n    \n      2\n      ëŒ€êµ¬\n      2013ë…„12ì›”\n      8080\n    \n    \n      3\n      ì¸ì²œ\n      2013ë…„12ì›”\n      10204\n    \n    \n      4\n      ê´‘ì£¼\n      2013ë…„12ì›”\n      6098\n    \n  \n\n\n\n\n\n\n\n\n\ndef parse_year(x):\n    year = x.split(\"ë…„\")[0]\n    return int(year) \n\ndef parse_month(x):\n    year = x.split(\"ë…„\")[1].replace(\"ì›”\", \"\")\n    return int(year) \n\ndf_first_melt['ì—°ë„'] = df_first_melt['ë‚ ì§œ'].map(parse_year)\ndf_first_melt['ì›”'] = df_first_melt['ë‚ ì§œ'].map(parse_month)\ndf_first_melt.head()\n\n\n\n\n\n  \n    \n      \n      ì§€ì—­ëª…\n      ë‚ ì§œ\n      í‰ë‹¹ë¶„ì–‘ê°€ê²©\n      ì—°ë„\n      ì›”\n    \n  \n  \n    \n      0\n      ì„œìš¸\n      2013ë…„12ì›”\n      18189\n      2013\n      12\n    \n    \n      1\n      ë¶€ì‚°\n      2013ë…„12ì›”\n      8111\n      2013\n      12\n    \n    \n      2\n      ëŒ€êµ¬\n      2013ë…„12ì›”\n      8080\n      2013\n      12\n    \n    \n      3\n      ì¸ì²œ\n      2013ë…„12ì›”\n      10204\n      2013\n      12\n    \n    \n      4\n      ê´‘ì£¼\n      2013ë…„12ì›”\n      6098\n      2013\n      12\n    \n  \n\n\n\n\n\n\n\n\ndf_lastì™€ df_fist_melt ì— ê³µí†µì ìœ¼ë¡œ ìˆëŠ” column ì¶”ì¶œí•˜ê¸°\n\ncols = df_last.columns.intersection(df_first_melt.columns) \ncols\n\nIndex(['ì§€ì—­ëª…', 'ì—°ë„', 'ì›”', 'í‰ë‹¹ë¶„ì–‘ê°€ê²©'], dtype='object')\n\n\nê³µí†µì ìœ¼ë¡œ ìˆëŠ” ì»¬ëŸ¼ë§Œ ë½‘ì•„ df_first_prepare, df_last_prepare ìƒì„±\n\ndf_first_prepare = df_first_melt[cols]\ndf_first.head() \n\ndf_last_prepare = df_last.loc[df_last['ì „ìš©ë©´ì ']=='ì „ì²´', cols].copy() \ndf_last_prepare.head() \n\n\n\n\n\n  \n    \n      \n      ì§€ì—­\n      2013ë…„12ì›”\n      2014ë…„1ì›”\n      2014ë…„2ì›”\n      2014ë…„3ì›”\n      2014ë…„4ì›”\n      2014ë…„5ì›”\n      2014ë…„6ì›”\n      2014ë…„7ì›”\n      2014ë…„8ì›”\n      ...\n      2014ë…„11ì›”\n      2014ë…„12ì›”\n      2015ë…„1ì›”\n      2015ë…„2ì›”\n      2015ë…„3ì›”\n      2015ë…„4ì›”\n      2015ë…„5ì›”\n      2015ë…„6ì›”\n      2015ë…„7ì›”\n      2015ë…„8ì›”\n    \n  \n  \n    \n      0\n      ì„œìš¸\n      18189\n      17925\n      17925\n      18016\n      18098\n      19446\n      18867\n      18742\n      19274\n      ...\n      20242\n      20269\n      20670\n      20670\n      19415\n      18842\n      18367\n      18374\n      18152\n      18443\n    \n    \n      1\n      ë¶€ì‚°\n      8111\n      8111\n      9078\n      8965\n      9402\n      9501\n      9453\n      9457\n      9411\n      ...\n      9208\n      9208\n      9204\n      9235\n      9279\n      9327\n      9345\n      9515\n      9559\n      9581\n    \n    \n      2\n      ëŒ€êµ¬\n      8080\n      8080\n      8077\n      8101\n      8267\n      8274\n      8360\n      8360\n      8370\n      ...\n      8439\n      8253\n      8327\n      8416\n      8441\n      8446\n      8568\n      8542\n      8542\n      8795\n    \n    \n      3\n      ì¸ì²œ\n      10204\n      10204\n      10408\n      10408\n      10000\n      9844\n      10058\n      9974\n      9973\n      ...\n      10020\n      10020\n      10017\n      9876\n      9876\n      9938\n      10551\n      10443\n      10443\n      10449\n    \n    \n      4\n      ê´‘ì£¼\n      6098\n      7326\n      7611\n      7346\n      7346\n      7523\n      7659\n      7612\n      7622\n      ...\n      7752\n      7748\n      7752\n      7756\n      7861\n      7914\n      7877\n      7881\n      8089\n      8231\n    \n  \n\n5 rows Ã— 22 columns\n\n\n\n\n\n\n\n  \n    \n      \n      ì§€ì—­ëª…\n      ì—°ë„\n      ì›”\n      í‰ë‹¹ë¶„ì–‘ê°€ê²©\n    \n  \n  \n    \n      0\n      ì„œìš¸\n      2015\n      10\n      19275.3\n    \n    \n      5\n      ì¸ì²œ\n      2015\n      10\n      10437.9\n    \n    \n      10\n      ê²½ê¸°\n      2015\n      10\n      10355.4\n    \n    \n      15\n      ë¶€ì‚°\n      2015\n      10\n      10269.6\n    \n    \n      20\n      ëŒ€êµ¬\n      2015\n      10\n      8850.6\n    \n  \n\n\n\n\ndf_first_prepare + df_last_prepare í•©ì¹˜ê¸°\n\ndf = pd.concat([df_first_prepare, df_last_prepare])\ndf.head()\n\n\n\n\n\n  \n    \n      \n      ì§€ì—­ëª…\n      ì—°ë„\n      ì›”\n      í‰ë‹¹ë¶„ì–‘ê°€ê²©\n    \n  \n  \n    \n      0\n      ì„œìš¸\n      2013\n      12\n      18189.0\n    \n    \n      1\n      ë¶€ì‚°\n      2013\n      12\n      8111.0\n    \n    \n      2\n      ëŒ€êµ¬\n      2013\n      12\n      8080.0\n    \n    \n      3\n      ì¸ì²œ\n      2013\n      12\n      10204.0\n    \n    \n      4\n      ê´‘ì£¼\n      2013\n      12\n      6098.0\n    \n  \n\n\n\n\n2013ë…„ë„ë¶€í„° 2019ë…„ë„ê¹Œì§€ì˜ ë°ì´í„°ë¥¼ ê°™ì€ í˜•íƒœë¡œ ëª¨ë‘ í•©ì³¤ìŠµë‹ˆë‹¤. ì´ì œ ë³¸ê²©ì ìœ¼ë¡œ ì „êµ­ì˜ ì•„íŒŒíŠ¸ ë¶„ì–‘ê°€ ì‹œê°í™” ë¶„ì„ì„ ì§„í–‰í•´ë³´ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤.\n \n\n\n\n\n\n\nprint(\"df.shape: \", df.shape)\nprint()\nprint(\"df.info(): \", df.info())\nprint()\nprint(\"== ê²°ì¸¡ê°’ ==\")\nprint(df.isnull().sum())\n\ndf.shape:  (1224, 4)\n\n<class 'pandas.core.frame.DataFrame'>\nInt64Index: 1224 entries, 0 to 4330\nData columns (total 4 columns):\n #   Column  Non-Null Count  Dtype  \n---  ------  --------------  -----  \n 0   ì§€ì—­ëª…     1224 non-null   object \n 1   ì—°ë„      1224 non-null   int64  \n 2   ì›”       1224 non-null   int64  \n 3   í‰ë‹¹ë¶„ì–‘ê°€ê²©  1215 non-null   float64\ndtypes: float64(1), int64(2), object(1)\nmemory usage: 47.8+ KB\ndf.info():  None\n\n== ê²°ì¸¡ê°’ ==\nì§€ì—­ëª…       0\nì—°ë„        0\nì›”         0\ní‰ë‹¹ë¶„ì–‘ê°€ê²©    9\ndtype: int64\n\n\ní‰ë‹¹ë¶„ì–‘ê°€ê²©ì— ê²°ì¸¡ê°’ì´ 9ê°œ ìˆëŠ” ê²ƒì„ í™•ì¸í•˜ì˜€ìŠµë‹ˆë‹¤. ì‹œê°í™” ì •ë³´ì „ë‹¬ì˜ ëª©ì ì´ë¯€ë¡œ ê²°ì¸¡ê°’ ì²˜ë¦¬ëŠ” ì§„í–‰í•˜ì§€ ì•Šê³ , ì–´ë–¤ ë°ì´í„°ì— ê²°ì¸¡ê°’ì´ ìˆëŠ”ì§€ ì•Œì•„ë³´ê² ìŠµë‹ˆë‹¤.\n\ndf[df['í‰ë‹¹ë¶„ì–‘ê°€ê²©'].isnull()]\n\n\n\n\n\n  \n    \n      \n      ì§€ì—­ëª…\n      ì—°ë„\n      ì›”\n      í‰ë‹¹ë¶„ì–‘ê°€ê²©\n    \n  \n  \n    \n      3265\n      ìš¸ì‚°\n      2018\n      12\n      NaN\n    \n    \n      3350\n      ìš¸ì‚°\n      2019\n      1\n      NaN\n    \n    \n      3435\n      ìš¸ì‚°\n      2019\n      2\n      NaN\n    \n    \n      3520\n      ìš¸ì‚°\n      2019\n      3\n      NaN\n    \n    \n      3605\n      ìš¸ì‚°\n      2019\n      4\n      NaN\n    \n    \n      3690\n      ìš¸ì‚°\n      2019\n      5\n      NaN\n    \n    \n      3775\n      ìš¸ì‚°\n      2019\n      6\n      NaN\n    \n    \n      3860\n      ìš¸ì‚°\n      2019\n      7\n      NaN\n    \n    \n      3945\n      ìš¸ì‚°\n      2019\n      8\n      NaN\n    \n  \n\n\n\n\nìš¸ì‚° ì§€ì—­ì—ì„œ 2018ë…„ë„ë¶€í„° ë°ì´í„°ê°€ ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ê²ƒìœ¼ë¡œ ë³´ì…ë‹ˆë‹¤. ì´ ë°ì´í„°ì…‹(2013~2019)ì—ì„œ ìš¸ì‚° ì§€ì—­ì˜ ìµœì‹  ë°ì´í„°ëŠ” ì—†ëŠ” ê²ƒì„ ê°ì•ˆí•˜ê³  ë¶„ì„ì„ ì§„í–‰í•˜ë„ë¡ í•©ë‹ˆë‹¤.\n \n\n\n\n1 ) ì—°ë„ë³„ ë¶„ì–‘ê°€ê²© ë™í–¥ - ì§€ì†ì ìœ¼ë¡œ ë¶„ì–‘ê°€ê²©ì´ ì¦ê°€ í•  ê²ƒì´ë‹¤.\n2 ) ì§€ì—­ë³„ ë¶„ì–‘ê°€ê²© - ìˆ˜ë„ê¶Œ ê·¼ë°©ì˜ ë¶„ì–‘ê°€ê²©ì´ ë†’ì„ ê²ƒì´ë‹¤.\n3 ) ì§€ì—­ë³„ ì—°ë„ë³„ ë¶„ì–‘ê°€ê²© - íŠ¹ì • ì§€ì—­ì˜ ì—°ë„ë³„ ë¶„ì–‘ê°€ê²©ì´ ì¦ê°€í•  ê²ƒì´ë‹¤.\në¨¼ì € ì—°ë„ë³„ë¡œ ë¶„ì–‘ê°€ê²©ì„ ë¶„ì„í•´ë³´ê³  ë¶„ì–‘ê°€ê²©ì˜ ì¶”ì´ë¥¼ ì‚´í´ë´…ë‹ˆë‹¤. ê·¸ ë‹¤ìŒ ì§€ì—­ ë³„ë¡œ ë¶„ì–‘ê°€ê²©ì„ ë¶„ì„í•˜ì—¬ ê° ì§€ì—­ì˜ í‰ê·  ë¶„ì–‘ê°€ê²©ì„ íŒŒì•…í•˜ê³ , ì–´ëŠ ì§€ì—­ì˜ ë¶„ì–‘ê°€ê²©ì´ ë†’ê³  ë‚®ì€ì§€ í™•ì¸í•©ë‹ˆë‹¤. ë§ˆì§€ë§‰ìœ¼ë¡œ ì§€ì—­ë³„ ì—°ë„ë³„ë¡œ ì‹œê°í™”ë¥¼ ì§„í–‰í•˜ê³ , íŠ¹ì • ì§€ì—­ì˜ ì–´ëŠ ì—°ë„ì—ì„œ ì¦ê°€ ì¶”ì„¸ë¥¼ ë„ëŠ”ì§€, ë™í–¥ì´ ì–´ë–»ê²Œ ë˜ëŠ”ì§€ ë¶„ì„í•©ë‹ˆë‹¤.\n \n\n\n\n\n\n\n\nì—°ë„ ë³„ í‰ë‹¹ë¶„ì–‘ê°€ê²©ì˜ í‰ê· ì„ ê·¸ë ¤ë´…ë‹ˆë‹¤.\n\nbarplot + pointplot\n\n\n_=plt.figure(figsize=(8, 5))\n\n_=plt.title(\"ì—°ë„ë³„ í‰ë‹¹ ë¶„ì–‘ê°€ê²© í‰ê·  (ë‹¨ìœ„: ì²œì›)\", fontsize=20, pad=10)\nbars=sns.barplot(data=df, x='ì—°ë„', y='í‰ë‹¹ë¶„ì–‘ê°€ê²©', ci=None, palette=sns.color_palette(\"Pastel1\"))\n_=sns.pointplot(data=df, x=\"ì—°ë„\", y=\"í‰ë‹¹ë¶„ì–‘ê°€ê²©\", ci=None, color='#bfaeae')\n\nfor i, b in enumerate(bars.patches):\n    _=plt.text(b.get_x()+b.get_width()*(1/2),b.get_height()+500, \\\n            round(b.get_height()),ha='center',fontsize=15, color='grey')\n\n_=plt.ylim(0, 13500)\n\n\n\n\n2013ë…„ë„ë¶€í„° 2019ë…„ë„ê¹Œì§€ í‰ë‹¹ ë¶„ì–‘ê°€ê²©ì˜ í‰ê· ì„ ì‹œê°í™”í•´ë³´ì•˜ë”ë‹ˆ ì§€ì†ì ìœ¼ë¡œ ì¦ê°€í•˜ëŠ” ì¶”ì„¸ë¥¼ ë³´ì˜€ìŠµë‹ˆë‹¤.\n\nboxplot\n\n\n_=plt.figure(figsize=(8, 5))\n\n_=plt.title(\"ì—°ë„ë³„ í‰ë‹¹ ë¶„ì–‘ê°€ê²© (ë‹¨ìœ„: ì²œì›)\", fontsize=20, pad=10)\n_=sns.boxplot(data=df, x='ì—°ë„', y='í‰ë‹¹ë¶„ì–‘ê°€ê²©', palette=sns.color_palette(\"Pastel1\"))\n\n\n\n\nboxplotìœ¼ë¡œëŠ” ì „ì²´ ê°’ì˜ ë¶„í¬ì™€ ì´ìƒì¹˜ë¥¼ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì—°ë„ë³„ë¡œ IQR ë°©ì‹ì˜ ì´ìƒì¹˜ë¡œ íŒë‹¨ë˜ëŠ” ê°’ì´ ì¡°ê¸ˆì”© ìˆëŠ” ê²ƒìœ¼ë¡œ ë³´ì—¬ì§‘ë‹ˆë‹¤. ê·¸ë¦¬ê³  ì¤‘ì•™ê°’ì´ë‚˜ ì‚¬ë¶„ìœ„ìˆ˜ ë“± ëª¨ë‘ ì—°ë„ë³„ë¡œ ì¦ê°€í•˜ëŠ” ê²ƒì„ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\nviolinplot\n\n\n_=plt.figure(figsize=(8, 5))\n\n_=plt.title(\"ì—°ë„ë³„ í‰ë‹¹ ë¶„ì–‘ê°€ê²© (ë‹¨ìœ„: ì²œì›)\", fontsize=20, pad=10)\n_=sns.violinplot(data=df, x='ì—°ë„', y='í‰ë‹¹ë¶„ì–‘ê°€ê²©', palette=sns.color_palette(\"Pastel1\"))\n\n\n\n\nviolinplotì„ ì‚¬ìš©í•˜ì—¬ ì—°ë„ë³„ ë¶„ì–‘ê°€ê²©ì˜ ë¹ˆë„ë¥¼ íŒŒì•…í•  ìˆ˜ ìˆê³ , ë§ˆì°¬ê°€ì§€ë¡œ ì—°ë„ë³„ë¡œ ì¦ê°€í•˜ëŠ” ì¶”ì„¸ë¥¼ ë³´ì´ê³  ìˆìŒì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\n\n\n\nì´ë²ˆì—” ì—°ë„ë³„ ì›”ë³„ í‰ë‹¹ë¶„ì–‘ê°€ê²©ì˜ í‰ê· ì„ ì‹œê°í™”í•˜ì—¬ ë‹¬ë§ˆë‹¤ ì–´ë–¤ ì¶”ì„¸ê°€ ìˆëŠ”ì§€ í™•ì¸í•´ë³´ê² ìŠµë‹ˆë‹¤.\n\n_=plt.figure(figsize=(15, 7))\n\n_=plt.title(\"ì—°ë„ë³„ ì›”ë³„ í‰ë‹¹ ë¶„ì–‘ê°€ê²© í‰ê· \", fontsize=20, pad=10)\n_=sns.barplot(data=df, x='ì—°ë„', y='í‰ë‹¹ë¶„ì–‘ê°€ê²©', hue='ì›”', ci=None, palette=sns.color_palette(\"Accent\"))\n\n_=plt.legend(loc=\"upper left\")\n\n_=plt.ylim(0, 13000)\n\n\n\n\nì—°ë„ë³„ ì›”ë³„ë¡œ í‰ë‹¹ ë¶„ì–‘ê°€ê²©ì˜ í‰ê·  ì‹œê°í™”ë¥¼ ê·¸ë ¤ë³´ì•˜ì„ ë•Œ, ëŒ€ì²´ì ìœ¼ë¡œ ì—°ì´ˆë³´ë‹¤ëŠ” ì—°ë§ì— ë¶„ì–‘ê°€ê²©ì´ ë†’ì€ ê²ƒìœ¼ë¡œ ë³´ì…ë‹ˆë‹¤.\n\n\n\n\n\n\n\n\nbaplot\n\n\n_=plt.figure(figsize=(17, 6))\n\n_=plt.title(\"ì§€ì—­ë³„ í‰ë‹¹ ë¶„ì–‘ê°€ê²© í‰ê·  (ë‹¨ìœ„: ì²œì›)\", fontsize=20, pad=10)\nbars=sns.barplot(data=df, x='ì§€ì—­ëª…', y='í‰ë‹¹ë¶„ì–‘ê°€ê²©', ci=None, palette=sns.color_palette(\"turbo\"))\n\nfor i, b in enumerate(bars.patches):\n    _=plt.text(b.get_x()+b.get_width()*(1/2),b.get_height()+500, \\\n            round(b.get_height()),ha='center',fontsize=15, color='#706363')\n\n_=plt.ylim(0, 25000)\n\n\n\n\nì„œìš¸ì˜ ë¶„ì–‘ê°€ê°€ ì••ë„ì ìœ¼ë¡œ ë†’ì€ ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë” ì§ê´€ì ìœ¼ë¡œ ì§€ì—­ë³„ ì§‘ê°’ì„ í™•ì¸í•´ë³´ê¸° ìœ„í•´ ë‚´ë¦¼ì°¨ìˆœìœ¼ë¡œ sorting í•˜ì—¬ ê·¸ë ¤ë³´ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤.\n\n_=plt.figure(figsize=(17, 6))\n\n# orderì— ì‚¬ìš© í•  ì‹œë¦¬ì¦ˆ ìƒì„± \nsr1_order = df.groupby(\"ì§€ì—­ëª…\")[\"í‰ë‹¹ë¶„ì–‘ê°€ê²©\"].mean().sort_values(ascending=False)\n\n_=plt.title(\"ì§€ì—­ë³„ í‰ë‹¹ ë¶„ì–‘ê°€ê²© í‰ê·  (ë‹¨ìœ„: ì²œì›)\", fontsize=20, pad=10)\nbars=sns.barplot(data=df, x='ì§€ì—­ëª…', y='í‰ë‹¹ë¶„ì–‘ê°€ê²©', ci=None, palette=sns.color_palette(\"turbo\"), order=sr1_order.index)\n\nfor i, b in enumerate(bars.patches):\n    _=plt.text(b.get_x()+b.get_width()*(1/2),b.get_height()+500, \\\n            round(b.get_height()),ha='center',fontsize=15, color='#706363')\n\n_=plt.ylim(0, 25000)\n\n\n\n\nì„œìš¸-ê²½ê¸°-ë¶€ì‚°-ì¸ì²œ ë“±ì˜ ìˆœìœ¼ë¡œ ë¶„ì–‘ê°€ê°€ ë†’ê³ , ì „ë‚¨-ì „ë¶-ì¶©ë¶ ì§€ì—­ì˜ ë¶„ì–‘ê°€ê°€ ê°€ì¥ ë‚®ì€ ê²ƒìœ¼ë¡œ ë‚˜íƒ€ë‚©ë‹ˆë‹¤.\n\nboxplot & violinplot\n\n\nfig = plt.figure(figsize=(17, 15))\nax1, ax2 = fig.subplots(2, 1)\n\n_=plt.suptitle(\"ì§€ì—­ë³„ í‰ë‹¹ ë¶„ì–‘ê°€ê²© (ë‹¨ìœ„: ì²œì›)\", fontsize=25)\n\n_=sns.boxplot(data=df, x='ì§€ì—­ëª…', y='í‰ë‹¹ë¶„ì–‘ê°€ê²©', palette=sns.color_palette(\"turbo\"), ax=ax1)\n_=sns.violinplot(data=df, x='ì§€ì—­ëª…', y='í‰ë‹¹ë¶„ì–‘ê°€ê²©', palette=sns.color_palette(\"turbo\"), ax=ax2)\n\n_=ax1.set_title(\"boxplot\", pad=10, fontsize=20)\n_=ax2.set_title(\"violinplot\", pad=10, fontsize=20)\n\nfig.tight_layout()\n\n\n\n\nì§€ì—­ë³„ë¡œ ì‹œê°í™”ë¥¼ í•´ë³´ë‹ˆ, ì´ìƒì¹˜ëŠ” ì–¼ë§ˆ ë‚˜íƒ€ë‚˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì—°ë„ë³„ ì‹œê°í™”ì—ì„œ ë‚˜ì˜¨ ì´ìƒì¹˜ëŠ” ì„œìš¸ì˜ ë¶„ì–‘ê°€ì˜€ë˜ ê²ƒìœ¼ë¡œ í•´ì„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë’¤ì—ì„œ ì—°ë„ë³„ ì§€ì—­ë³„ë¡œ ê·¸ë ¤ë³´ê³  ì§ì ‘ í™•ì¸í•´ë³´ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤.\n\n\n\n\n\n\n\n\nheatmap\n\n\n# ì§€ì—­ë³„ ì—°ë„ë³„ pivot table ìƒì„± \ndf_region_year = df.pivot_table(index=\"ì—°ë„\", columns='ì§€ì—­ëª…', values='í‰ë‹¹ë¶„ì–‘ê°€ê²©').round().astype(int)\n\nfig = plt.figure(figsize=(20, 17), dpi=100)\nax1, ax2 = fig.subplots(2, 1)\n\n_=sns.heatmap(df_region_year, annot=True, fmt=\".0f\", cmap=\"Purples\", ax=ax1)\n_=sns.heatmap(df_region_year.T, annot=True, fmt=\".0f\", cmap=\"Purples\", ax=ax2)\n\n_=ax1.set_title(\"ì§€ì—­ë³„ ì—°ë„ë³„ í‰ë‹¹ ë¶„ì–‘ê°€ê²© í‰ê· \", fontsize=20, pad=10)\n_=ax2.set_title(\"ì—°ë„ë³„ ì§€ì—­ë³„ í‰ë‹¹ ë¶„ì–‘ê°€ê²© í‰ê· \", fontsize=20, pad=10)\n\nfig.tight_layout()\n\n\n\n\nìœ„ì—ì„œ ì—°ë„ ë³„ boxplotì„ ê·¸ë ¤ë´¤ì„ ë•Œ ì´ìƒì¹˜ë¡œ ë‚˜ì˜¨ ê°’ë“¤ì´ ì„œìš¸ ë¶„ì–‘ê°€ì¸ ê²ƒìœ¼ë¡œ ë³´ì…ë‹ˆë‹¤. ì„œìš¸ì´ ëŒ€ì²´ì ìœ¼ë¡œ ë‹¤ë¥¸ ì§€ì—­ì˜ ë‘ë°° ì´ìƒì˜ ê°’ì¸ ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\nlineplot\n\n\n_=plt.figure(figsize=(13, 8))\n\n_=sns.lineplot(data=df, x=\"ì—°ë„\", y=\"í‰ë‹¹ë¶„ì–‘ê°€ê²©\", hue=\"ì§€ì—­ëª…\", ci=None, marker='o')\n_=plt.legend(bbox_to_anchor=(1.02, 1), loc=2)\n\n_=plt.title(\"ì—°ë„ë³„ ì§€ì—­ë³„ í‰ë‹¹ ë¶„ì–‘ê°€ê²© (ë‹¨ìœ„: ì²œì›)\", pad=10, fontsize=20)\n\n\n\n\nlineplotìœ¼ë¡œ í™•ì¸í•´ ë´ë„ ì„œìš¸ì˜ ë¶„ì–‘ê°€ê²©ì´ ì›”ë“±íˆ ë†’ì€ ê²ƒì„ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë˜, ì¦ê°€ í­ë„ ë†’ì€ ê²ƒìœ¼ë¡œ í™•ì¸ë©ë‹ˆë‹¤. ë”°ë¼ì„œ ë‹¤ìŒìœ¼ë¡œëŠ” ì„œìš¸ ì§€ì—­ë§Œ ë”°ë¡œ ë½‘ì•„ ì‹œê°í™” í•´ë³´ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤.\n\n\n\n\n\ndf_seoul = df[df['ì§€ì—­ëª…']=='ì„œìš¸'].copy()\ndf_seoul.shape\n\n(72, 4)\n\n\nì„œìš¸ ì§€ì—­ë§Œ ë”°ë¡œ ë½‘ì•„ df_seoul ë³€ìˆ˜ì— ë„£ì–´ì£¼ì—ˆìŠµë‹ˆë‹¤.\n\nbarplot\n\n\n_=plt.figure(figsize=(10, 5))\n\n_=plt.title(\"ì—°ë„ë³„ í‰ë‹¹ë¶„ì–‘ê°€ê²© í‰ê·  (ì„œìš¸)\", fontsize=20, pad=10)\n\nbars=sns.barplot(data=df_seoul, x=\"ì—°ë„\", y=\"í‰ë‹¹ë¶„ì–‘ê°€ê²©\", ci=None, palette=sns.color_palette(\"magma_r\"))\n\nfor i, b in enumerate(bars.patches):\n    _=plt.text(b.get_x()+b.get_width()*(1/2),b.get_height()+500, \\\n            round(b.get_height()),ha='center',fontsize=15, color='#706363')\n\n_=plt.ylim(0, 29900)\n\n\n\n\nì—°ë„ ë³„ë¡œ ì¦ê°€í•˜ëŠ” ì¶”ì„¸ë¥¼ ë³´ì…ë‹ˆë‹¤.\n\nboxplot\n\n\n_=plt.figure(figsize=(10, 5))\n\n_=plt.title(\"ì—°ë„ë³„ í‰ë‹¹ë¶„ì–‘ê°€ê²© (ì„œìš¸)\", fontsize=20, pad=10)\n\nbars=sns.boxplot(data=df_seoul, x=\"ì—°ë„\", y=\"í‰ë‹¹ë¶„ì–‘ê°€ê²©\", palette=sns.color_palette(\"magma_r\"))\n\n\n\n\nì„œìš¸ì§€ì—­ì˜ ì—°ë„ë³„ boxplotì„ ê·¸ë ¤ë³´ì•˜ì„ ë•Œ, ì´ìƒì¹˜ëŠ” ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ë˜, ê°’ì„ ì‚´í´ë³´ë©´, ìœ„ì—ì„œ ì „ì²´ ì§€ì—­ì˜ ì—°ë„ë³„ í‰ë‹¹ë¶„ì–‘ê°€ê²©ì˜ boxplotì—ì„œ ì´ìƒì¹˜ë¡œ ë‚˜ì™”ë˜ ê°’ë“¤ì´ ì„œìš¸ ì§€ì—­ì˜ ê°’ë“¤ì¸ ê²ƒì„ ì§ì ‘ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n \n\n\n\n\n\n4ë²ˆì—ì„œ ì„¤ì •í–ˆë˜ ê°€ì„¤ì„ í† ëŒ€ë¡œ ì‹œê°í™” ë¶„ì„ ê²°ê³¼ë¥¼ ìš”ì•½í•©ë‹ˆë‹¤.\n1 ) ì—°ë„ë³„ ë¶„ì–‘ê°€ê²© ë™í–¥ - ì§€ì†ì ìœ¼ë¡œ ë¶„ì–‘ê°€ê²©ì´ ì¦ê°€í•œë‹¤.\n2 ) ì§€ì—­ë³„ ë¶„ì–‘ê°€ê²© - ìˆ˜ë„ê¶Œ ê·¼ë°©, (ì„œìš¸, ê²½ê¸°ë„, ..) ê´‘ì—­ì‹œ (ëŒ€êµ¬, ..)ì˜ ë¶„ì–‘ê°€ê²©ì´ ë†’ë‹¤. íŠ¹íˆ ì„œìš¸ì€ ë‹¤ë¥¸ ì§€ì—­ì˜ 2ë°° ì •ë„ì˜ ë¶„ì–‘ê°€ì„ì„ í™•ì¸í•˜ì˜€ë‹¤.\n3 ) ì§€ì—­ë³„ ì—°ë„ë³„ ë¶„ì–‘ê°€ê²© - ëŒ€ì²´ì ìœ¼ë¡œ ëª¨ë“  ì§€ì—­ì—ì„œ ì—°ë„ë§ˆë‹¤ ì§€ì†ì ìœ¼ë¡œ ë¶„ì–‘ê°€ê°€ ì¦ê°€í•´ì™”ë‹¤.\n\nì „êµ­ì˜ ë¶„ì–‘ê°€ê²© ë°ì´í„°ë¡œ ì‹œê°í™” ë¶„ì„ì„ ì§„í–‰í•˜ì˜€ìŠµë‹ˆë‹¤. ë¶„ì–‘ê°€ê²©ì˜ ë™í–¥ì„ ì—°ë„ë³„, ì§€ì—­ë³„ë¡œ í™•ì¸í•˜ì˜€ê³ , ì–´ëŠ ì§€ì—­ì˜ ë¶„ì–‘ê°€ê°€ ë†’ì€ì§€ ì •ë³´ë¥¼ ì–»ì„ ìˆ˜ ìˆì—ˆìŠµë‹ˆë‹¤. ì‹œê°í™” ë¶„ì„ ê³¼ì •ì—ì„œ ì „ì²´ ì§€ì—­ì—ì„œëŠ” ì´ìƒì¹˜ë¡œ ë‚˜ì™”ë˜ ê°’ë“¤ì´ ì„œìš¸ ì§€ì—­ì˜ ë³´í¸ì ì¸ ê°’ì´ë¼ëŠ” ì ì„ í™•ì¸í•˜ì˜€ê³ , ì´ë¥¼ í†µí•´ ì´ìƒì¹˜ë¥¼ ì„£ë¶ˆë¦¬ ì‚­ì œí•´ì„œëŠ” ì•ˆ ë˜ê³ , ê°’ì„ ë¶„ì„í•´ ë³¸ í›„ì— ì²˜ë¦¬í•´ì•¼ í•œë‹¤ëŠ” ê²ƒì„ ë°°ì› ìŠµë‹ˆë‹¤."
  }
]